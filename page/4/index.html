<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.1.2"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/leaf.jpg?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/leaf.ico?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/leaf1.ico?v=7.1.2">


  <link rel="mask-icon" href="/images/leaf.jpg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Heroinlin&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/4/index.html">
<meta property="og:site_name" content="Heroinlin&#39;s Blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Heroinlin&#39;s Blog">





  
  
  <link rel="canonical" href="http://yoursite.com/page/4/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Heroinlin's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Heroinlin's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Navigationsleiste an/ausschalten">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>Startseite</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>Schlagwörter</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>Kategorien</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>Archiv</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">

    
    
    
      
    

    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br/>Zeitplan</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Python_Numpy/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Python_Numpy/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：Python Numpy教程</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-03-26 08:53:06" itemprop="dateModified" datetime="2018-03-26T08:53:06+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：Python-Numpy教程"><a href="#CS231n课程笔记翻译：Python-Numpy教程" class="headerlink" title="CS231n课程笔记翻译：Python Numpy教程"></a>CS231n课程笔记翻译：Python Numpy教程</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>这篇教程由<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/jcjohns/" target="_blank" rel="noopener"><strong>Justin Johnson</strong></a>创作。</p>
<p>我们将使用Python编程语言来完成本课程的所有作业。Python是一门伟大的通用编程语言，在一些常用库（numpy, scipy, matplotlib）的帮助下，它又会变成一个强大的科学计算环境。</p>
<p>我们期望你们中大多数人对于Python语言和Numpy库比较熟悉，而对于没有Python经验的同学，这篇教程可以帮助你们快速了解Python编程环境和如何使用Python作为科学计算工具。</p>
<p>一部分同学对于Matlab有一定经验。对于这部分同学，我们推荐阅读 <a href="http://link.zhihu.com/?target=http%3A//wiki.scipy.org/NumPy_for_Matlab_Users" target="_blank" rel="noopener"><strong>numpy for Matlab users</strong></a>页面。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//wiki.scipy.org/NumPy_for_Matlab_Users" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//wiki.scipy.org/NumPy_for_Matlab_Users</a></p>
</blockquote>
<p>你们还可以查看<a href="http://link.zhihu.com/?target=https%3A//github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb" target="_blank" rel="noopener"><strong>本教程的IPython notebook版</strong></a>。该教程是由<a href="http://link.zhihu.com/?target=http%3A//web.stanford.edu/%257Ekuleshov/" target="_blank" rel="noopener"><strong>Volodymyr Kuleshov</strong></a>和<a href="http://link.zhihu.com/?target=https%3A//symsys.stanford.edu/viewing/symsysaffiliate/21335" target="_blank" rel="noopener"><strong>Isaac Caswell</strong></a>为课程<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/%257Eermon/cs228/index.html" target="_blank" rel="noopener"><strong>CS 228</strong></a>创建的。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=https%3A//github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb</a></p>
<p><a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/%257Eermon/cs228/index.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//cs.stanford.edu/%257Eermon/cs228/index.html</a></p>
</blockquote>
<p>内容列表：</p>
<ul>
<li><p>Python</p>
</li>
<li><ul>
<li><p>基本数据类型</p>
</li>
<li><p>容器</p>
<ul>
<li>列表</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code>- 字典
- 集合
- 元组
</code></pre><ul>
<li><p>函数</p>
</li>
<li><p>类</p>
</li>
</ul>
<ul>
<li><p>Numpy</p>
</li>
<li><ul>
<li>数组</li>
<li>访问数组</li>
<li>数据类型</li>
<li>数组计算</li>
<li>广播</li>
</ul>
</li>
<li><p>SciPy</p>
</li>
<li><ul>
<li>图像操作</li>
<li>MATLAB文件</li>
<li>点之间的距离</li>
</ul>
</li>
<li><p>Matplotlib</p>
</li>
<li><ul>
<li>绘制图形</li>
<li>绘制多个图形</li>
<li>图像</li>
</ul>
</li>
</ul>
<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p>Python是一种高级的，动态类型的多范型编程语言。很多时候，大家会说Python看起来简直和伪代码一样，这是因为你能够通过很少行数的代码表达出很有力的思想。举个例子，下面是用Python实现的经典的quicksort算法例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quicksort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(arr) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> arr</span><br><span class="line">    pivot = arr[len(arr) / <span class="number">2</span>]</span><br><span class="line">    left = [x <span class="keyword">for</span> x <span class="keyword">in</span> arr <span class="keyword">if</span> x &lt; pivot]</span><br><span class="line">    middle = [x <span class="keyword">for</span> x <span class="keyword">in</span> arr <span class="keyword">if</span> x == pivot]</span><br><span class="line">    right = [x <span class="keyword">for</span> x <span class="keyword">in</span> arr <span class="keyword">if</span> x &gt; pivot]</span><br><span class="line">    <span class="keyword">return</span> quicksort(left) + middle + quicksort(right)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> quicksort([<span class="number">3</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">10</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line"><span class="comment"># Prints "[1, 1, 2, 3, 6, 8, 10]"</span></span><br></pre></td></tr></table></figure>
<h3 id="Python版本"><a href="#Python版本" class="headerlink" title="Python版本"></a>Python版本</h3><p>Python有两个支持的版本，分别是2.7和3.4。这有点让人迷惑，3.0向语言中引入了很多不向后兼容的变化，2.7下的代码有时候在3.4下是行不通的。<strong>在这个课程中，我们使用的是2.7版本。</strong></p>
<p>如何查看版本呢？使用<strong>python –version</strong>命令。</p>
<h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><p>和大多数编程语言一样，Python拥有一系列的基本数据类型，比如整型、浮点型、布尔型和字符串等。这些类型的使用方式和在其他语言中的使用方式是类似的。</p>
<p><strong>数字</strong>：整型和浮点型的使用与其他语言类似。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">3</span></span><br><span class="line"><span class="keyword">print</span> type(x) <span class="comment"># Prints "&lt;type 'int'&gt;"</span></span><br><span class="line"><span class="keyword">print</span> x       <span class="comment"># Prints "3"</span></span><br><span class="line"><span class="keyword">print</span> x + <span class="number">1</span>   <span class="comment"># Addition; prints "4"</span></span><br><span class="line"><span class="keyword">print</span> x - <span class="number">1</span>   <span class="comment"># Subtraction; prints "2"</span></span><br><span class="line"><span class="keyword">print</span> x * <span class="number">2</span>   <span class="comment"># Multiplication; prints "6"</span></span><br><span class="line"><span class="keyword">print</span> x ** <span class="number">2</span>  <span class="comment"># Exponentiation; prints "9"</span></span><br><span class="line">x += <span class="number">1</span></span><br><span class="line"><span class="keyword">print</span> x  <span class="comment"># Prints "4"</span></span><br><span class="line">x *= <span class="number">2</span></span><br><span class="line"><span class="keyword">print</span> x  <span class="comment"># Prints "8"</span></span><br><span class="line">y = <span class="number">2.5</span></span><br><span class="line"><span class="keyword">print</span> type(y) <span class="comment"># Prints "&lt;type 'float'&gt;"</span></span><br><span class="line"><span class="keyword">print</span> y, y + <span class="number">1</span>, y * <span class="number">2</span>, y ** <span class="number">2</span> <span class="comment"># Prints "2.5 3.5 5.0 6.25"</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是，Python中没有 x++ 和 x– 的操作符。</p>
<p>Python也有内置的长整型和复杂数字类型，具体细节可以查看<a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23numeric-types-int-float-long-complex" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p>文档地址：<a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23numeric-types-int-float-long-complex" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23numeric-types-int-float-long-complex</a></p>
</blockquote>
<p><strong>布尔型</strong>：Python实现了所有的布尔逻辑，但用的是英语，而不是我们习惯的操作符（比如&amp;&amp;和||等）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">t = <span class="keyword">True</span></span><br><span class="line">f = <span class="keyword">False</span></span><br><span class="line"><span class="keyword">print</span> type(t) <span class="comment"># Prints "&lt;type 'bool'&gt;"</span></span><br><span class="line"><span class="keyword">print</span> t <span class="keyword">and</span> f <span class="comment"># Logical AND; prints "False"</span></span><br><span class="line"><span class="keyword">print</span> t <span class="keyword">or</span> f  <span class="comment"># Logical OR; prints "True"</span></span><br><span class="line"><span class="keyword">print</span> <span class="keyword">not</span> t   <span class="comment"># Logical NOT; prints "False"</span></span><br><span class="line"><span class="keyword">print</span> t != f  <span class="comment"># Logical XOR; prints "True"</span></span><br></pre></td></tr></table></figure>
<p><strong>字符串</strong>：Python对字符串的支持非常棒。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hello = <span class="string">'hello'</span>   <span class="comment"># String literals can use single quotes</span></span><br><span class="line">world = <span class="string">"world"</span>   <span class="comment"># or double quotes; it does not matter.</span></span><br><span class="line"><span class="keyword">print</span> hello       <span class="comment"># Prints "hello"</span></span><br><span class="line"><span class="keyword">print</span> len(hello)  <span class="comment"># String length; prints "5"</span></span><br><span class="line">hw = hello + <span class="string">' '</span> + world  <span class="comment"># String concatenation</span></span><br><span class="line"><span class="keyword">print</span> hw  <span class="comment"># prints "hello world"</span></span><br><span class="line">hw12 = <span class="string">'%s %s %d'</span> % (hello, world, <span class="number">12</span>)  <span class="comment"># sprintf style string formatting</span></span><br><span class="line"><span class="keyword">print</span> hw12  <span class="comment"># prints "hello world 12"</span></span><br></pre></td></tr></table></figure>
<p>字符串对象有一系列有用的方法，比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">"hello"</span></span><br><span class="line"><span class="keyword">print</span> s.capitalize()  <span class="comment"># Capitalize a string; prints "Hello"</span></span><br><span class="line"><span class="keyword">print</span> s.upper()       <span class="comment"># Convert a string to uppercase; prints "HELLO"</span></span><br><span class="line"><span class="keyword">print</span> s.rjust(<span class="number">7</span>)      <span class="comment"># Right-justify a string, padding with spaces; prints "  hello"</span></span><br><span class="line"><span class="keyword">print</span> s.center(<span class="number">7</span>)     <span class="comment"># Center a string, padding with spaces; prints " hello "</span></span><br><span class="line"><span class="keyword">print</span> s.replace(<span class="string">'l'</span>, <span class="string">'(ell)'</span>)  <span class="comment"># Replace all instances of one substring with another;</span></span><br><span class="line">                               <span class="comment"># prints "he(ell)(ell)o"</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'  world '</span>.strip()  <span class="comment"># Strip leading and trailing whitespace; prints "world"</span></span><br></pre></td></tr></table></figure>
<p>如果想详细查看字符串方法，请看<a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23string-methods" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23string-methods" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23string-methods</a></p>
</blockquote>
<h3 id="容器Containers"><a href="#容器Containers" class="headerlink" title="容器Containers"></a>容器Containers</h3><p><strong>译者注</strong>：有知友建议container翻译为复合数据类型，供读者参考。</p>
<p>Python有以下几种容器类型：列表（lists）、字典（dictionaries）、集合（sets）和元组（tuples）。</p>
<h3 id="列表Lists"><a href="#列表Lists" class="headerlink" title="列表Lists"></a>列表Lists</h3><p>列表就是Python中的数组，但是列表长度可变，且能包含不同类型元素。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">xs = [<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>]   <span class="comment"># Create a list</span></span><br><span class="line"><span class="keyword">print</span> xs, xs[<span class="number">2</span>]  <span class="comment"># Prints "[3, 1, 2] 2"</span></span><br><span class="line"><span class="keyword">print</span> xs[<span class="number">-1</span>]     <span class="comment"># Negative indices count from the end of the list; prints "2"</span></span><br><span class="line">xs[<span class="number">2</span>] = <span class="string">'foo'</span>    <span class="comment"># Lists can contain elements of different types</span></span><br><span class="line"><span class="keyword">print</span> xs         <span class="comment"># Prints "[3, 1, 'foo']"</span></span><br><span class="line">xs.append(<span class="string">'bar'</span>) <span class="comment"># Add a new element to the end of the list</span></span><br><span class="line"><span class="keyword">print</span> xs         <span class="comment"># Prints </span></span><br><span class="line">x = xs.pop()     <span class="comment"># Remove and return the last element of the list</span></span><br><span class="line"><span class="keyword">print</span> x, xs      <span class="comment"># Prints "bar [3, 1, 'foo']"</span></span><br></pre></td></tr></table></figure>
<p>列表的细节，同样可以查阅<a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/datastructures.html%23more-on-lists" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/datastructures.html%23more-on-lists" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/datastructures.html%23more-on-lists</a></p>
</blockquote>
<p><strong>切片Slicing</strong>：为了一次性地获取列表中的元素，Python提供了一种简洁的语法，这就是切片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">nums = range(<span class="number">5</span>)    <span class="comment"># range is a built-in function that creates a list of integers</span></span><br><span class="line"><span class="keyword">print</span> nums         <span class="comment"># Prints "[0, 1, 2, 3, 4]"</span></span><br><span class="line"><span class="keyword">print</span> nums[<span class="number">2</span>:<span class="number">4</span>]    <span class="comment"># Get a slice from index 2 to 4 (exclusive); prints "[2, 3]"</span></span><br><span class="line"><span class="keyword">print</span> nums[<span class="number">2</span>:]     <span class="comment"># Get a slice from index 2 to the end; prints "[2, 3, 4]"</span></span><br><span class="line"><span class="keyword">print</span> nums[:<span class="number">2</span>]     <span class="comment"># Get a slice from the start to index 2 (exclusive); prints "[0, 1]"</span></span><br><span class="line"><span class="keyword">print</span> nums[:]      <span class="comment"># Get a slice of the whole list; prints ["0, 1, 2, 3, 4]"</span></span><br><span class="line"><span class="keyword">print</span> nums[:<span class="number">-1</span>]    <span class="comment"># Slice indices can be negative; prints ["0, 1, 2, 3]"</span></span><br><span class="line">nums[<span class="number">2</span>:<span class="number">4</span>] = [<span class="number">8</span>, <span class="number">9</span>] <span class="comment"># Assign a new sublist to a slice</span></span><br><span class="line"><span class="keyword">print</span> nums         <span class="comment"># Prints "[0, 1, 8, 8, 4]"</span></span><br></pre></td></tr></table></figure>
<p>在Numpy数组的内容中，我们会再次看到切片语法。</p>
<p><strong>循环Loops</strong>：我们可以这样遍历列表中的每一个元素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">animals = [<span class="string">'cat'</span>, <span class="string">'dog'</span>, <span class="string">'monkey'</span>]</span><br><span class="line"><span class="keyword">for</span> animal <span class="keyword">in</span> animals:</span><br><span class="line">    <span class="keyword">print</span> animal</span><br><span class="line"><span class="comment"># Prints "cat", "dog", "monkey", each on its own line.</span></span><br></pre></td></tr></table></figure>
<p>如果想要在循环体内访问每个元素的指针，可以使用内置的<strong>enumerate</strong>函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">animals = [<span class="string">'cat'</span>, <span class="string">'dog'</span>, <span class="string">'monkey'</span>]</span><br><span class="line"><span class="keyword">for</span> idx, animal <span class="keyword">in</span> enumerate(animals):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'#%d: %s'</span> % (idx + <span class="number">1</span>, animal)</span><br><span class="line"><span class="comment"># Prints "#1: cat", "#2: dog", "#3: monkey", each on its own line</span></span><br></pre></td></tr></table></figure>
<p><strong>列表推导List comprehensions</strong>：在编程的时候，我们常常想要将一种数据类型转换为另一种。下面是一个简单例子，将列表中的每个元素变成它的平方。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">squares = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> nums:</span><br><span class="line">    squares.append(x ** <span class="number">2</span>)</span><br><span class="line"><span class="keyword">print</span> squares   <span class="comment"># Prints [0, 1, 4, 9, 16]</span></span><br></pre></td></tr></table></figure>
<p>使用列表推导，你就可以让代码简化很多：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">squares = [x ** <span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> nums]</span><br><span class="line"><span class="keyword">print</span> squares   <span class="comment"># Prints [0, 1, 4, 9, 16]</span></span><br></pre></td></tr></table></figure>
<p>列表推导还可以包含条件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">even_squares = [x ** <span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> nums <span class="keyword">if</span> x % <span class="number">2</span> == <span class="number">0</span>]</span><br><span class="line"><span class="keyword">print</span> even_squares  <span class="comment"># Prints "[0, 4, 16]"</span></span><br></pre></td></tr></table></figure>
<h3 id="字典Dictionaries"><a href="#字典Dictionaries" class="headerlink" title="字典Dictionaries"></a>字典Dictionaries</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'cat'</span>: <span class="string">'cute'</span>, <span class="string">'dog'</span>: <span class="string">'furry'</span>&#125;  <span class="comment"># Create a new dictionary with some data</span></span><br><span class="line"><span class="keyword">print</span> d[<span class="string">'cat'</span>]       <span class="comment"># Get an entry from a dictionary; prints "cute"</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'cat'</span> <span class="keyword">in</span> d     <span class="comment"># Check if a dictionary has a given key; prints "True"</span></span><br><span class="line">d[<span class="string">'fish'</span>] = <span class="string">'wet'</span>    <span class="comment"># Set an entry in a dictionary</span></span><br><span class="line"><span class="keyword">print</span> d[<span class="string">'fish'</span>]      <span class="comment"># Prints "wet"</span></span><br><span class="line"><span class="comment"># print d['monkey']  # KeyError: 'monkey' not a key of d</span></span><br><span class="line"><span class="keyword">print</span> d.get(<span class="string">'monkey'</span>, <span class="string">'N/A'</span>)  <span class="comment"># Get an element with a default; prints "N/A"</span></span><br><span class="line"><span class="keyword">print</span> d.get(<span class="string">'fish'</span>, <span class="string">'N/A'</span>)    <span class="comment"># Get an element with a default; prints "wet"</span></span><br><span class="line"><span class="keyword">del</span> d[<span class="string">'fish'</span>]        <span class="comment"># Remove an element from a dictionary</span></span><br><span class="line"><span class="keyword">print</span> d.get(<span class="string">'fish'</span>, <span class="string">'N/A'</span>) <span class="comment"># "fish" is no longer a key; prints "N/A"</span></span><br></pre></td></tr></table></figure>
<p>想要知道字典的其他特性，请查阅<a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23dict" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23dict" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/stdtypes.html%23dict</a></p>
</blockquote>
<p><strong>循环Loops</strong>：在字典中，用键来迭代更加容易。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'person'</span>: <span class="number">2</span>, <span class="string">'cat'</span>: <span class="number">4</span>, <span class="string">'spider'</span>: <span class="number">8</span>&#125;</span><br><span class="line"><span class="keyword">for</span> animal <span class="keyword">in</span> d:</span><br><span class="line">    legs = d[animal]</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'A %s has %d legs'</span> % (animal, legs)</span><br><span class="line"><span class="comment"># Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs"</span></span><br></pre></td></tr></table></figure>
<p>如果你想要访问键和对应的值，那就使用<strong>iteritems</strong>方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'person'</span>: <span class="number">2</span>, <span class="string">'cat'</span>: <span class="number">4</span>, <span class="string">'spider'</span>: <span class="number">8</span>&#125;</span><br><span class="line"><span class="keyword">for</span> animal, legs <span class="keyword">in</span> d.iteritems():</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'A %s has %d legs'</span> % (animal, legs)</span><br><span class="line"><span class="comment"># Prints "A person has 2 legs", "A spider has 8 legs", "A cat has 4 legs"</span></span><br></pre></td></tr></table></figure>
<p><strong>字典推导Dictionary comprehensions</strong>：和列表推导类似，但是允许你方便地构建字典。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nums = [0, 1, 2, 3, 4]</span><br><span class="line">even_num_to_square = &#123;x: x ** 2 for x in nums if x % 2 == 0&#125;</span><br><span class="line">print even_num_to_square  # Prints &quot;&#123;0: 0, 2: 4, 4: 16&#125;&quot;</span><br></pre></td></tr></table></figure>
<h3 id="集合Sets"><a href="#集合Sets" class="headerlink" title="集合Sets"></a>集合Sets</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">animals = &#123;<span class="string">'cat'</span>, <span class="string">'dog'</span>&#125;</span><br><span class="line"><span class="keyword">print</span> <span class="string">'cat'</span> <span class="keyword">in</span> animals   <span class="comment"># Check if an element is in a set; prints "True"</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'fish'</span> <span class="keyword">in</span> animals  <span class="comment"># prints "False"</span></span><br><span class="line">animals.add(<span class="string">'fish'</span>)      <span class="comment"># Add an element to a set</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'fish'</span> <span class="keyword">in</span> animals  <span class="comment"># Prints "True"</span></span><br><span class="line"><span class="keyword">print</span> len(animals)       <span class="comment"># Number of elements in a set; prints "3"</span></span><br><span class="line">animals.add(<span class="string">'cat'</span>)       <span class="comment"># Adding an element that is already in the set does nothing</span></span><br><span class="line"><span class="keyword">print</span> len(animals)       <span class="comment"># Prints "3"</span></span><br><span class="line">animals.remove(<span class="string">'cat'</span>)    <span class="comment"># Remove an element from a set</span></span><br><span class="line"><span class="keyword">print</span> len(animals)       <span class="comment"># Prints "2"</span></span><br></pre></td></tr></table></figure>
<p>和前面一样，要知道更详细的，查看<a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/sets.html%23set-objects" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/sets.html%23set-objects" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//docs.python.org/2/library/sets.html%23set-objects</a></p>
</blockquote>
<p><strong>循环Loops</strong>：在集合中循环的语法和在列表中一样，但是集合是无序的，所以你在访问集合的元素的时候，不能做关于顺序的假设。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">animals = &#123;<span class="string">'cat'</span>, <span class="string">'dog'</span>, <span class="string">'fish'</span>&#125;</span><br><span class="line"><span class="keyword">for</span> idx, animal <span class="keyword">in</span> enumerate(animals):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'#%d: %s'</span> % (idx + <span class="number">1</span>, animal)</span><br><span class="line"><span class="comment"># Prints "#1: fish", "#2: dog", "#3: cat"</span></span><br></pre></td></tr></table></figure>
<p><strong>集合推导**</strong>Set comprehensions**：和字典推导一样，可以很方便地构建集合：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> sqrt</span><br><span class="line">nums = &#123;int(sqrt(x)) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">30</span>)&#125;</span><br><span class="line"><span class="keyword">print</span> nums  <span class="comment"># Prints "set([0, 1, 2, 3, 4, 5])"</span></span><br></pre></td></tr></table></figure>
<h3 id="元组Tuples"><a href="#元组Tuples" class="headerlink" title="元组Tuples"></a>元组Tuples</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;(x, x + <span class="number">1</span>): x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>)&#125;  <span class="comment"># Create a dictionary with tuple keys</span></span><br><span class="line"><span class="keyword">print</span> d</span><br><span class="line">t = (<span class="number">5</span>, <span class="number">6</span>)       <span class="comment"># Create a tuple</span></span><br><span class="line"><span class="keyword">print</span> type(t)    <span class="comment"># Prints "&lt;type 'tuple'&gt;"</span></span><br><span class="line"><span class="keyword">print</span> d[t]       <span class="comment"># Prints "5"</span></span><br><span class="line"><span class="keyword">print</span> d[(<span class="number">1</span>, <span class="number">2</span>)]  <span class="comment"># Prints "1"</span></span><br></pre></td></tr></table></figure>
<p><a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/datastructures.html%23tuples-and-sequences" target="_blank" rel="noopener"><strong>文档</strong></a>有更多元组的信息。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/datastructures.html%23tuples-and-sequences" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/datastructures.html%23tuples-and-sequences</a></p>
</blockquote>
<h3 id="函数Functions"><a href="#函数Functions" class="headerlink" title="函数Functions"></a>函数Functions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sign</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'positive'</span></span><br><span class="line">    <span class="keyword">elif</span> x &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'negative'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'zero'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">-1</span>, <span class="number">0</span>, <span class="number">1</span>]:</span><br><span class="line">    <span class="keyword">print</span> sign(x)</span><br><span class="line"><span class="comment"># Prints "negative", "zero", "positive"</span></span><br></pre></td></tr></table></figure>
<p>我们常常使用可选参数来定义函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span><span class="params">(name, loud=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> loud:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'HELLO, %s'</span> % name.upper()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'Hello, %s!'</span> % name</span><br><span class="line"></span><br><span class="line">hello(<span class="string">'Bob'</span>) <span class="comment"># Prints "Hello, Bob"</span></span><br><span class="line">hello(<span class="string">'Fred'</span>, loud=<span class="keyword">True</span>)  <span class="comment"># Prints "HELLO, FRED!"</span></span><br></pre></td></tr></table></figure>
<p>函数还有很多内容，可以查看<a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/controlflow.html%23defining-functions" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/controlflow.html%23defining-functions" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/controlflow.html%23defining-functions</a></p>
</blockquote>
<h3 id="类Classes"><a href="#类Classes" class="headerlink" title="类Classes"></a>类Classes</h3><p>Python对于类的定义是简单直接的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Greeter</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Constructor</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self.name = name  <span class="comment"># Create an instance variable</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Instance method</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">greet</span><span class="params">(self, loud=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> loud:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'HELLO, %s!'</span> % self.name.upper()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'Hello, %s'</span> % self.name</span><br><span class="line"></span><br><span class="line">g = Greeter(<span class="string">'Fred'</span>)  <span class="comment"># Construct an instance of the Greeter class</span></span><br><span class="line">g.greet()            <span class="comment"># Call an instance method; prints "Hello, Fred"</span></span><br><span class="line">g.greet(loud=<span class="keyword">True</span>)   <span class="comment"># Call an instance method; prints "HELLO, FRED!"</span></span><br></pre></td></tr></table></figure>
<p>更多类的信息请查阅<a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/classes.html" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/classes.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=https%3A//docs.python.org/2/tutorial/classes.html</a></p>
</blockquote>
<h2 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h2><p>Numpy是Python中用于科学计算的核心库。它提供了高性能的多维数组对象，以及相关工具。</p>
<h3 id="数组Arrays"><a href="#数组Arrays" class="headerlink" title="数组Arrays"></a>数组Arrays</h3><p>一个numpy数组是一个由不同数值组成的网格。网格中的数据都是同一种数据类型，可以通过非负整型数的元组来访问。维度的数量被称为数组的阶，数组的大小是一个由整型数构成的元组，可以描述数组不同维度上的大小。</p>
<p>我们可以从列表创建数组，然后利用方括号访问其中的元素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])  <span class="comment"># Create a rank 1 array</span></span><br><span class="line"><span class="keyword">print</span> type(a)            <span class="comment"># Prints "&lt;type 'numpy.ndarray'&gt;"</span></span><br><span class="line"><span class="keyword">print</span> a.shape            <span class="comment"># Prints "(3,)"</span></span><br><span class="line"><span class="keyword">print</span> a[<span class="number">0</span>], a[<span class="number">1</span>], a[<span class="number">2</span>]   <span class="comment"># Prints "1 2 3"</span></span><br><span class="line">a[<span class="number">0</span>] = <span class="number">5</span>                 <span class="comment"># Change an element of the array</span></span><br><span class="line"><span class="keyword">print</span> a                  <span class="comment"># Prints "[5, 2, 3]"</span></span><br><span class="line"></span><br><span class="line">b = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])   <span class="comment"># Create a rank 2 array</span></span><br><span class="line"><span class="keyword">print</span> b                           <span class="comment"># 显示一下矩阵b</span></span><br><span class="line"><span class="keyword">print</span> b.shape                     <span class="comment"># Prints "(2, 3)"</span></span><br><span class="line"><span class="keyword">print</span> b[<span class="number">0</span>, <span class="number">0</span>], b[<span class="number">0</span>, <span class="number">1</span>], b[<span class="number">1</span>, <span class="number">0</span>]   <span class="comment"># Prints "1 2 4"</span></span><br></pre></td></tr></table></figure>
<p>Numpy还提供了很多其他创建数组的方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.zeros((<span class="number">2</span>,<span class="number">2</span>))  <span class="comment"># Create an array of all zeros</span></span><br><span class="line"><span class="keyword">print</span> a              <span class="comment"># Prints "[[ 0.  0.]</span></span><br><span class="line">                     <span class="comment">#          [ 0.  0.]]"</span></span><br><span class="line"></span><br><span class="line">b = np.ones((<span class="number">1</span>,<span class="number">2</span>))   <span class="comment"># Create an array of all ones</span></span><br><span class="line"><span class="keyword">print</span> b              <span class="comment"># Prints "[[ 1.  1.]]"</span></span><br><span class="line"></span><br><span class="line">c = np.full((<span class="number">2</span>,<span class="number">2</span>), <span class="number">7</span>) <span class="comment"># Create a constant array</span></span><br><span class="line"><span class="keyword">print</span> c               <span class="comment"># Prints "[[ 7.  7.]</span></span><br><span class="line">                      <span class="comment">#          [ 7.  7.]]"</span></span><br><span class="line"></span><br><span class="line">d = np.eye(<span class="number">2</span>)        <span class="comment"># Create a 2x2 identity matrix</span></span><br><span class="line"><span class="keyword">print</span> d              <span class="comment"># Prints "[[ 1.  0.]</span></span><br><span class="line">                     <span class="comment">#          [ 0.  1.]]"</span></span><br><span class="line"></span><br><span class="line">e = np.random.random((<span class="number">2</span>,<span class="number">2</span>)) <span class="comment"># Create an array filled with random values</span></span><br><span class="line"><span class="keyword">print</span> e                     <span class="comment"># Might print "[[ 0.91940167  0.08143941]</span></span><br><span class="line">                            <span class="comment">#               [ 0.68744134  0.87236687]]"</span></span><br></pre></td></tr></table></figure>
<p>其他数组相关方法，请查看<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/user/basics.creation.html%23arrays-creation" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/user/basics.creation.html%23arrays-creation" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/user/basics.creation.html%23arrays-creation</a></p>
</blockquote>
<h3 id="访问数组"><a href="#访问数组" class="headerlink" title="访问数组"></a>访问数组</h3><p>Numpy提供了多种访问数组的方法。</p>
<p><strong>切片</strong>：和Python列表类似，numpy数组可以使用切片语法。因为数组可以是多维的，所以你<strong>必须</strong>为每个维度指定好切片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the following rank 2 array with shape (3, 4)</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use slicing to pull out the subarray consisting of the first 2 rows</span></span><br><span class="line"><span class="comment"># and columns 1 and 2; b is the following array of shape (2, 2):</span></span><br><span class="line"><span class="comment"># [[2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]]</span></span><br><span class="line">b = a[:<span class="number">2</span>, <span class="number">1</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># A slice of an array is a view into the same data, so modifying it</span></span><br><span class="line"><span class="comment"># will modify the original array.</span></span><br><span class="line"><span class="keyword">print</span> a[<span class="number">0</span>, <span class="number">1</span>]   <span class="comment"># Prints "2"</span></span><br><span class="line">b[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">77</span>    <span class="comment"># b[0, 0] is the same piece of data as a[0, 1]</span></span><br><span class="line"><span class="keyword">print</span> a[<span class="number">0</span>, <span class="number">1</span>]   <span class="comment"># Prints "77"</span></span><br></pre></td></tr></table></figure>
<p>你可以同时使用整型和切片语法来访问数组。但是，这样做会产生一个比原数组低阶的新数组。需要注意的是，这里和MATLAB中的情况是不同的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the following rank 2 array with shape (3, 4)</span></span><br><span class="line"><span class="comment"># [[ 1  2  3  4]</span></span><br><span class="line"><span class="comment">#  [ 5  6  7  8]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11 12]]</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], [<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>], [<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Two ways of accessing the data in the middle row of the array.</span></span><br><span class="line"><span class="comment"># Mixing integer indexing with slices yields an array of lower rank,</span></span><br><span class="line"><span class="comment"># while using only slices yields an array of the same rank as the</span></span><br><span class="line"><span class="comment"># original array:</span></span><br><span class="line">row_r1 = a[<span class="number">1</span>, :]    <span class="comment"># Rank 1 view of the second row of a  </span></span><br><span class="line">row_r2 = a[<span class="number">1</span>:<span class="number">2</span>, :]  <span class="comment"># Rank 2 view of the second row of a</span></span><br><span class="line"><span class="keyword">print</span> row_r1, row_r1.shape  <span class="comment"># Prints "[5 6 7 8] (4,)"</span></span><br><span class="line"><span class="keyword">print</span> row_r2, row_r2.shape  <span class="comment"># Prints "[[5 6 7 8]] (1, 4)"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We can make the same distinction when accessing columns of an array:</span></span><br><span class="line">col_r1 = a[:, <span class="number">1</span>]</span><br><span class="line">col_r2 = a[:, <span class="number">1</span>:<span class="number">2</span>]</span><br><span class="line"><span class="keyword">print</span> col_r1, col_r1.shape  <span class="comment"># Prints "[ 2  6 10] (3,)"</span></span><br><span class="line"><span class="keyword">print</span> col_r2, col_r2.shape  <span class="comment"># Prints "[[ 2]</span></span><br><span class="line">                            <span class="comment">#          [ 6]</span></span><br><span class="line">                            <span class="comment">#          [10]] (3, 1)"</span></span><br></pre></td></tr></table></figure>
<p><strong>整型数组访问</strong>：当我们使用切片语法访问数组时，得到的总是原数组的一个子集。整型数组访问允许我们利用其它数组的数据构建一个新的数组：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># An example of integer array indexing.</span></span><br><span class="line"><span class="comment"># The returned array will have shape (3,) and </span></span><br><span class="line"><span class="keyword">print</span> a[[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]]  <span class="comment"># Prints "[1 4 5]"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The above example of integer array indexing is equivalent to this:</span></span><br><span class="line"><span class="keyword">print</span> np.array([a[<span class="number">0</span>, <span class="number">0</span>], a[<span class="number">1</span>, <span class="number">1</span>], a[<span class="number">2</span>, <span class="number">0</span>]])  <span class="comment"># Prints "[1 4 5]"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># When using integer array indexing, you can reuse the same</span></span><br><span class="line"><span class="comment"># element from the source array:</span></span><br><span class="line"><span class="keyword">print</span> a[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]  <span class="comment"># Prints "[2 2]"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Equivalent to the previous integer array indexing example</span></span><br><span class="line"><span class="keyword">print</span> np.array([a[<span class="number">0</span>, <span class="number">1</span>], a[<span class="number">0</span>, <span class="number">1</span>]])  <span class="comment"># Prints "[2 2]"</span></span><br></pre></td></tr></table></figure>
<p>整型数组访问语法还有个有用的技巧，可以用来选择或者更改矩阵中每行中的一个元素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a new array from which we will select elements</span></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> a  <span class="comment"># prints "array([[ 1,  2,  3],</span></span><br><span class="line">         <span class="comment">#                [ 4,  5,  6],</span></span><br><span class="line">         <span class="comment">#                [ 7,  8,  9],</span></span><br><span class="line">         <span class="comment">#                [10, 11, 12]])"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an array of indices</span></span><br><span class="line">b = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select one element from each row of a using the indices in b</span></span><br><span class="line"><span class="keyword">print</span> a[np.arange(<span class="number">4</span>), b]  <span class="comment"># Prints "[ 1  6  7 11]"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Mutate one element from each row of a using the indices in b</span></span><br><span class="line">a[np.arange(<span class="number">4</span>), b] += <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> a  <span class="comment"># prints "array([[11,  2,  3],</span></span><br><span class="line">         <span class="comment">#                [ 4,  5, 16],</span></span><br><span class="line">         <span class="comment">#                [17,  8,  9],</span></span><br><span class="line">         <span class="comment">#                [10, 21, 12]])</span></span><br></pre></td></tr></table></figure>
<p><strong>布尔型数组访问</strong>：布尔型数组访问可以让你选择数组中任意元素。通常，这种访问方式用于选取数组中满足某些条件的元素，举例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">bool_idx = (a &gt; <span class="number">2</span>)  <span class="comment"># Find the elements of a that are bigger than 2;</span></span><br><span class="line">                    <span class="comment"># this returns a numpy array of Booleans of the same</span></span><br><span class="line">                    <span class="comment"># shape as a, where each slot of bool_idx tells</span></span><br><span class="line">                    <span class="comment"># whether that element of a is &gt; 2.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> bool_idx      <span class="comment"># Prints "[[False False]</span></span><br><span class="line">                    <span class="comment">#          [ True  True]</span></span><br><span class="line">                    <span class="comment">#          [ True  True]]"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We use boolean array indexing to construct a rank 1 array</span></span><br><span class="line"><span class="comment"># consisting of the elements of a corresponding to the True values</span></span><br><span class="line"><span class="comment"># of bool_idx</span></span><br><span class="line"><span class="keyword">print</span> a[bool_idx]  <span class="comment"># Prints "[3 4 5 6]"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We can do all of the above in a single concise statement:</span></span><br><span class="line"><span class="keyword">print</span> a[a &gt; <span class="number">2</span>]     <span class="comment"># Prints "[3 4 5 6]"</span></span><br></pre></td></tr></table></figure>
<p>为了教程的简介，有很多数组访问的细节我们没有详细说明，可以查看<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/arrays.indexing.html" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p> <a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/arrays.indexing.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/arrays.indexing.html</a></p>
</blockquote>
<h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>每个Numpy数组都是数据类型相同的元素组成的网格。Numpy提供了很多的数据类型用于创建数组。当你创建数组的时候，Numpy会尝试猜测数组的数据类型，你也可以通过参数直接指定数据类型，例子如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>])  <span class="comment"># Let numpy choose the datatype</span></span><br><span class="line"><span class="keyword">print</span> x.dtype         <span class="comment"># Prints "int64"</span></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>])  <span class="comment"># Let numpy choose the datatype</span></span><br><span class="line"><span class="keyword">print</span> x.dtype             <span class="comment"># Prints "float64"</span></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">1</span>, <span class="number">2</span>], dtype=np.int64)  <span class="comment"># Force a particular datatype</span></span><br><span class="line"><span class="keyword">print</span> x.dtype                         <span class="comment"># Prints "int64"</span></span><br></pre></td></tr></table></figure>
<p>更多细节查看<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/arrays.dtypes.html" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/arrays.dtypes.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/arrays.dtypes.html</a></p>
</blockquote>
<h3 id="数组计算"><a href="#数组计算" class="headerlink" title="数组计算"></a>数组计算</h3><p>基本数学计算函数会对数组中元素逐个进行计算，既可以利用操作符重载，也可以使用函数方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]], dtype=np.float64)</span><br><span class="line">y = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]], dtype=np.float64)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elementwise sum; both produce the array</span></span><br><span class="line"><span class="comment"># [[ 6.0  8.0]</span></span><br><span class="line"><span class="comment">#  [10.0 12.0]]</span></span><br><span class="line"><span class="keyword">print</span> x + y</span><br><span class="line"><span class="keyword">print</span> np.add(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elementwise difference; both produce the array</span></span><br><span class="line"><span class="comment"># [[-4.0 -4.0]</span></span><br><span class="line"><span class="comment">#  [-4.0 -4.0]]</span></span><br><span class="line"><span class="keyword">print</span> x - y</span><br><span class="line"><span class="keyword">print</span> np.subtract(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elementwise product; both produce the array</span></span><br><span class="line"><span class="comment"># [[ 5.0 12.0]</span></span><br><span class="line"><span class="comment">#  [21.0 32.0]]</span></span><br><span class="line"><span class="keyword">print</span> x * y</span><br><span class="line"><span class="keyword">print</span> np.multiply(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elementwise division; both produce the array</span></span><br><span class="line"><span class="comment"># [[ 0.2         0.33333333]</span></span><br><span class="line"><span class="comment">#  [ 0.42857143  0.5       ]]</span></span><br><span class="line"><span class="keyword">print</span> x / y</span><br><span class="line"><span class="keyword">print</span> np.divide(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elementwise square root; produces the array</span></span><br><span class="line"><span class="comment"># [[ 1.          1.41421356]</span></span><br><span class="line"><span class="comment">#  [ 1.73205081  2.        ]]</span></span><br><span class="line"><span class="keyword">print</span> np.sqrt(x)</span><br></pre></td></tr></table></figure>
<p>和MATLAB不同，*是元素逐个相乘，而不是矩阵乘法。在Numpy中使用dot来进行矩阵乘法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">y = np.array([[<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line"></span><br><span class="line">v = np.array([<span class="number">9</span>,<span class="number">10</span>])</span><br><span class="line">w = np.array([<span class="number">11</span>, <span class="number">12</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Inner product of vectors; both produce 219</span></span><br><span class="line"><span class="keyword">print</span> v.dot(w)</span><br><span class="line"><span class="keyword">print</span> np.dot(v, w)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix / vector product; both produce the rank 1 array [29 67]</span></span><br><span class="line"><span class="keyword">print</span> x.dot(v)</span><br><span class="line"><span class="keyword">print</span> np.dot(x, v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Matrix / matrix product; both produce the rank 2 array</span></span><br><span class="line"><span class="comment"># [[19 22]</span></span><br><span class="line"><span class="comment">#  [43 50]]</span></span><br><span class="line"><span class="keyword">print</span> x.dot(y)</span><br><span class="line"><span class="keyword">print</span> np.dot(x, y)</span><br></pre></td></tr></table></figure>
<p>Numpy提供了很多计算数组的函数，其中最常用的一个是<strong>sum</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> np.sum(x)  <span class="comment"># Compute sum of all elements; prints "10"</span></span><br><span class="line"><span class="keyword">print</span> np.sum(x, axis=<span class="number">0</span>)  <span class="comment"># Compute sum of each column; prints "[4 6]"</span></span><br><span class="line"><span class="keyword">print</span> np.sum(x, axis=<span class="number">1</span>)  <span class="comment"># Compute sum of each row; prints "[3 7]"</span></span><br></pre></td></tr></table></figure>
<p>想要了解更多函数，可以查看<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/routines.math.html" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/routines.math.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/routines.math.html</a></p>
</blockquote>
<p>除了计算，我们还常常改变数组或者操作其中的元素。其中将矩阵转置是常用的一个，在Numpy中，使用<strong>T</strong>来转置矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line"><span class="keyword">print</span> x    <span class="comment"># Prints "[[1 2]</span></span><br><span class="line">           <span class="comment">#          [3 4]]"</span></span><br><span class="line"><span class="keyword">print</span> x.T  <span class="comment"># Prints "[[1 3]</span></span><br><span class="line">           <span class="comment">#          [2 4]]"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that taking the transpose of a rank 1 array does nothing:</span></span><br><span class="line">v = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="keyword">print</span> v    <span class="comment"># Prints "[1 2 3]"</span></span><br><span class="line"><span class="keyword">print</span> v.T  <span class="comment"># Prints "[1 2 3]"</span></span><br></pre></td></tr></table></figure>
<p>Numpy还提供了更多操作数组的方法，请查看<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html</a></p>
</blockquote>
<h3 id="广播Broadcasting"><a href="#广播Broadcasting" class="headerlink" title="广播Broadcasting"></a>广播Broadcasting</h3><p>广播是一种强有力的机制，它让Numpy可以让不同大小的矩阵在一起进行数学计算。我们常常会有一个小的矩阵和一个大的矩阵，然后我们会需要用小的矩阵对大的矩阵做一些计算。</p>
<p>举个例子，如果我们想要把一个向量加到矩阵的每一行，我们可以这样做：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># We will add the vector v to each row of the matrix x,</span></span><br><span class="line"><span class="comment"># storing the result in the matrix y</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">v = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y = np.empty_like(x)   <span class="comment"># Create an empty matrix with the same shape as x</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the vector v to each row of the matrix x with an explicit loop</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">    y[i, :] = x[i, :] + v</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now y is the following</span></span><br><span class="line"><span class="comment"># [[ 2  2  4]</span></span><br><span class="line"><span class="comment">#  [ 5  5  7]</span></span><br><span class="line"><span class="comment">#  [ 8  8 10]</span></span><br><span class="line"><span class="comment">#  [11 11 13]]</span></span><br><span class="line"><span class="keyword">print</span> y</span><br></pre></td></tr></table></figure>
<p>这样是行得通的，但是当x矩阵非常大，利用循环来计算就会变得很慢很慢。我们可以换一种思路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># We will add the vector v to each row of the matrix x,</span></span><br><span class="line"><span class="comment"># storing the result in the matrix y</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">v = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">vv = np.tile(v, (<span class="number">4</span>, <span class="number">1</span>))  <span class="comment"># Stack 4 copies of v on top of each other</span></span><br><span class="line"><span class="keyword">print</span> vv                 <span class="comment"># Prints "[[1 0 1]</span></span><br><span class="line">                         <span class="comment">#          [1 0 1]</span></span><br><span class="line">                         <span class="comment">#          [1 0 1]</span></span><br><span class="line">                         <span class="comment">#          [1 0 1]]"</span></span><br><span class="line">y = x + vv  <span class="comment"># Add x and vv elementwise</span></span><br><span class="line"><span class="keyword">print</span> y  <span class="comment"># Prints "[[ 2  2  4</span></span><br><span class="line">         <span class="comment">#          [ 5  5  7]</span></span><br><span class="line">         <span class="comment">#          [ 8  8 10]</span></span><br><span class="line">         <span class="comment">#          [11 11 13]]"</span></span><br></pre></td></tr></table></figure>
<p>Numpy广播机制可以让我们不用创建vv，就能直接运算，看看下面例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># We will add the vector v to each row of the matrix x,</span></span><br><span class="line"><span class="comment"># storing the result in the matrix y</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>], [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">v = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y = x + v  <span class="comment"># Add v to each row of x using broadcasting</span></span><br><span class="line"><span class="keyword">print</span> y  <span class="comment"># Prints "[[ 2  2  4]</span></span><br><span class="line">         <span class="comment">#          [ 5  5  7]</span></span><br><span class="line">         <span class="comment">#          [ 8  8 10]</span></span><br><span class="line">         <span class="comment">#          [11 11 13]]"</span></span><br></pre></td></tr></table></figure>
<p>对两个数组使用广播机制要遵守下列规则：</p>
<ol>
<li>如果数组的秩不同，使用1来将秩较小的数组进行扩展，直到两个数组的尺寸的长度都一样。</li>
<li>如果两个数组在某个维度上的长度是一样的，或者其中一个数组在该维度上长度为1，那么我们就说这两个数组在该维度上是<strong>相容</strong>的。</li>
<li>如果两个数组在所有维度上都是相容的，他们就能使用广播。</li>
<li>如果两个输入数组的尺寸不同，那么注意其中较大的那个尺寸。因为广播之后，两个数组的尺寸将和那个较大的尺寸一样。</li>
<li>在任何一个维度上，如果一个数组的长度为1，另一个数组长度大于1，那么在该维度上，就好像是对第一个数组进行了复制。</li>
</ol>
<p>如果上述解释看不明白，可以读一读<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener"><strong>文档</strong></a>和这个<a href="http://link.zhihu.com/?target=http%3A//scipy.github.io/old-wiki/pages/EricsBroadcastingDoc" target="_blank" rel="noopener"><strong>解释</strong></a>。<strong>译者注</strong>：强烈推荐阅读文档中的例子。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/user/basics.broadcasting.html</a></p>
<p><a href="http://link.zhihu.com/?target=http%3A//scipy.github.io/old-wiki/pages/EricsBroadcastingDoc" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//scipy.github.io/old-wiki/pages/EricsBroadcastingDoc</a></p>
</blockquote>
<p>支持广播机制的函数是全局函数。哪些是全局函数可以在<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/ufuncs.html%23available-ufuncs" target="_blank" rel="noopener"><strong>文档</strong></a>中查找。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/ufuncs.html%23available-ufuncs" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/ufuncs.html%23available-ufuncs</a></p>
</blockquote>
<p>下面是一些广播机制的使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute outer product of vectors</span></span><br><span class="line">v = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  <span class="comment"># v has shape (3,)</span></span><br><span class="line">w = np.array([<span class="number">4</span>,<span class="number">5</span>])    <span class="comment"># w has shape (2,)</span></span><br><span class="line"><span class="comment"># To compute an outer product, we first reshape v to be a column</span></span><br><span class="line"><span class="comment"># vector of shape (3, 1); we can then broadcast it against w to yield</span></span><br><span class="line"><span class="comment"># an output of shape (3, 2), which is the outer product of v and w:</span></span><br><span class="line"><span class="comment"># [[ 4  5]</span></span><br><span class="line"><span class="comment">#  [ 8 10]</span></span><br><span class="line"><span class="comment">#  [12 15]]</span></span><br><span class="line"><span class="keyword">print</span> np.reshape(v, (<span class="number">3</span>, <span class="number">1</span>)) * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add a vector to each row of a matrix</span></span><br><span class="line">x = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="comment"># x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),</span></span><br><span class="line"><span class="comment"># giving the following matrix:</span></span><br><span class="line"><span class="comment"># [[2 4 6]</span></span><br><span class="line"><span class="comment">#  [5 7 9]]</span></span><br><span class="line"><span class="keyword">print</span> x + v</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add a vector to each column of a matrix</span></span><br><span class="line"><span class="comment"># x has shape (2, 3) and w has shape (2,).</span></span><br><span class="line"><span class="comment"># If we transpose x then it has shape (3, 2) and can be broadcast</span></span><br><span class="line"><span class="comment"># against w to yield a result of shape (3, 2); transposing this result</span></span><br><span class="line"><span class="comment"># yields the final result of shape (2, 3) which is the matrix x with</span></span><br><span class="line"><span class="comment"># the vector w added to each column. Gives the following matrix:</span></span><br><span class="line"><span class="comment"># [[ 5  6  7]</span></span><br><span class="line"><span class="comment">#  [ 9 10 11]]</span></span><br><span class="line"><span class="keyword">print</span> (x.T + w).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Another solution is to reshape w to be a row vector of shape (2, 1);</span></span><br><span class="line"><span class="comment"># we can then broadcast it directly against x to produce the same</span></span><br><span class="line"><span class="comment"># output.</span></span><br><span class="line"><span class="keyword">print</span> x + np.reshape(w, (<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiply a matrix by a constant:</span></span><br><span class="line"><span class="comment"># x has shape (2, 3). Numpy treats scalars as arrays of shape ();</span></span><br><span class="line"><span class="comment"># these can be broadcast together to shape (2, 3), producing the</span></span><br><span class="line"><span class="comment"># following array:</span></span><br><span class="line"><span class="comment"># [[ 2  4  6]</span></span><br><span class="line"><span class="comment">#  [ 8 10 12]]</span></span><br><span class="line"><span class="keyword">print</span> x * <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>广播机制能够让你的代码更简洁更迅速，能够用的时候请尽量使用！</p>
<h3 id="Numpy文档"><a href="#Numpy文档" class="headerlink" title="Numpy文档"></a>Numpy文档</h3><p>这篇教程涉及了你需要了解的numpy中的一些重要内容，但是numpy远不止如此。可以查阅<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/" target="_blank" rel="noopener"><strong>numpy文献</strong></a>来学习更多。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/numpy/reference/</a></p>
</blockquote>
<h2 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h2><p>Numpy提供了高性能的多维数组，以及计算和操作数组的基本工具。<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/" target="_blank" rel="noopener"><strong>SciPy</strong></a>基于Numpy，提供了大量的计算和操作数组的函数，这些函数对于不同类型的科学和工程计算非常有用。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/</a></p>
</blockquote>
<p>熟悉SciPy的最好方法就是阅读<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/index.html" target="_blank" rel="noopener"><strong>文档</strong></a>。我们会强调对于本课程有用的部分。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/index.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/index.html</a></p>
</blockquote>
<h3 id="图像操作"><a href="#图像操作" class="headerlink" title="图像操作"></a>图像操作</h3><p>SciPy提供了一些操作图像的基本函数。比如，它提供了将图像从硬盘读入到数组的函数，也提供了将数组中数据写入的硬盘成为图像的函数。下面是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread, imsave, imresize</span><br><span class="line"></span><br><span class="line"><span class="comment"># Read an JPEG image into a numpy array</span></span><br><span class="line">img = imread(<span class="string">'assets/cat.jpg'</span>)</span><br><span class="line"><span class="keyword">print</span> img.dtype, img.shape  <span class="comment"># Prints "uint8 (400, 248, 3)"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We can tint the image by scaling each of the color channels</span></span><br><span class="line"><span class="comment"># by a different scalar constant. The image has shape (400, 248, 3);</span></span><br><span class="line"><span class="comment"># we multiply it by the array [1, 0.95, 0.9] of shape (3,);</span></span><br><span class="line"><span class="comment"># numpy broadcasting means that this leaves the red channel unchanged,</span></span><br><span class="line"><span class="comment"># and multiplies the green and blue channels by 0.95 and 0.9</span></span><br><span class="line"><span class="comment"># respectively.</span></span><br><span class="line">img_tinted = img * [<span class="number">1</span>, <span class="number">0.95</span>, <span class="number">0.9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize the tinted image to be 300 by 300 pixels.</span></span><br><span class="line">img_tinted = imresize(img_tinted, (<span class="number">300</span>, <span class="number">300</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Write the tinted image back to disk</span></span><br><span class="line">imsave(<span class="string">'assets/cat_tinted.jpg'</span>, img_tinted)</span><br></pre></td></tr></table></figure>
<p><strong>译者注</strong>：如果运行这段代码出现类似<strong>ImportError: cannot import name imread</strong>的报错，那么请利用pip进行Pillow的下载，可以解决问题。命令：pip install Pillow。</p>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/python_numpy/python_numpy_0.png?raw=true" width="300"></center>

<p>左边是原始图片，右边是变色和变形的图片。</p>
<p>—————————————————————————————————————————</p>
<h3 id="MATLAB文件"><a href="#MATLAB文件" class="headerlink" title="MATLAB文件"></a>MATLAB文件</h3><p>函数<strong>scipy.io.loadmat</strong>和<strong>scipy.io.savemat</strong>能够让你读和写MATLAB文件。具体请查看<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/io.html" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/io.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/io.html</a></p>
</blockquote>
<h3 id="点之间的距离"><a href="#点之间的距离" class="headerlink" title="点之间的距离"></a>点之间的距离</h3><p>SciPy定义了一些有用的函数，可以计算集合中点之间的距离。</p>
<p>函数<strong>scipy.spatial.distance.pdist</strong>能够计算集合中所有两点之间的距离：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.spatial.distance <span class="keyword">import</span> pdist, squareform</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the following array where each row is a point in 2D space:</span></span><br><span class="line"><span class="comment"># [[0 1]</span></span><br><span class="line"><span class="comment">#  [1 0]</span></span><br><span class="line"><span class="comment">#  [2 0]]</span></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">0</span>]])</span><br><span class="line"><span class="keyword">print</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the Euclidean distance between all rows of x.</span></span><br><span class="line"><span class="comment"># d[i, j] is the Euclidean distance between x[i, :] and x[j, :],</span></span><br><span class="line"><span class="comment"># and d is the following array:</span></span><br><span class="line"><span class="comment"># [[ 0.          1.41421356  2.23606798]</span></span><br><span class="line"><span class="comment">#  [ 1.41421356  0.          1.        ]</span></span><br><span class="line"><span class="comment">#  [ 2.23606798  1.          0.        ]]</span></span><br><span class="line">d = squareform(pdist(x, <span class="string">'euclidean'</span>))</span><br><span class="line"><span class="keyword">print</span> d</span><br></pre></td></tr></table></figure>
<p>具体细节请阅读<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html</a></p>
</blockquote>
<p>函数<strong>scipy.spatial.distance.cdist</strong>可以计算不同集合中点的距离，具体请查看<a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html</a></p>
</blockquote>
<h2 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h2><p>Matplotlib是一个作图库。这里简要介绍<strong>matplotlib.pyplot</strong>模块，功能和MATLAB的作图功能类似。</p>
<h3 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h3><p>matplotlib库中最重要的函数是<strong>Plot</strong>。该函数允许你做出2D图形，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the x and y coordinates for points on a sine curve</span></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">3</span> * np.pi, <span class="number">0.1</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the points using matplotlib</span></span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()  <span class="comment"># You must call plt.show() to make graphics appear.</span></span><br></pre></td></tr></table></figure>
<p>运行上面代码会产生下面的作图：</p>
<p>—————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/python_numpy/python_numpy_1.png?raw=true" width="250"></center>—————————————————————————————————————————</p>
<p>只需要少量工作，就可以一次画不同的线，加上标签，坐标轴标志等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the x and y coordinates for points on sine and cosine curves</span></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">3</span> * np.pi, <span class="number">0.1</span>)</span><br><span class="line">y_sin = np.sin(x)</span><br><span class="line">y_cos = np.cos(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the points using matplotlib</span></span><br><span class="line">plt.plot(x, y_sin)</span><br><span class="line">plt.plot(x, y_cos)</span><br><span class="line">plt.xlabel(<span class="string">'x axis label'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y axis label'</span>)</span><br><span class="line">plt.title(<span class="string">'Sine and Cosine'</span>)</span><br><span class="line">plt.legend([<span class="string">'Sine'</span>, <span class="string">'Cosine'</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>—————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/python_numpy/python_numpy_2.png?raw=true" width="250"></center>—————————————————————————————————————————</p>
<p>可以在<a href="http://link.zhihu.com/?target=http%3A//matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.plot" target="_blank" rel="noopener"><strong>文档</strong></a>中阅读更多关于plot的内容。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.plot" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.plot</a></p>
</blockquote>
<h3 id="绘制多个图像"><a href="#绘制多个图像" class="headerlink" title="绘制多个图像"></a>绘制多个图像</h3><p>可以使用<strong>subplot</strong>函数来在一幅图中画不同的东西：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute the x and y coordinates for points on sine and cosine curves</span></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">3</span> * np.pi, <span class="number">0.1</span>)</span><br><span class="line">y_sin = np.sin(x)</span><br><span class="line">y_cos = np.cos(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up a subplot grid that has height 2 and width 1,</span></span><br><span class="line"><span class="comment"># and set the first such subplot as active.</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make the first plot</span></span><br><span class="line">plt.plot(x, y_sin)</span><br><span class="line">plt.title(<span class="string">'Sine'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the second subplot as active, and make the second plot.</span></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x, y_cos)</span><br><span class="line">plt.title(<span class="string">'Cosine'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the figure.</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>—————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/python_numpy/python_numpy_3.png?raw=true" width="300"></center>—————————————————————————————————————————</p>
<p>关于<strong>subplot</strong>的更多细节，可以阅读<a href="http://link.zhihu.com/?target=http%3A//matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.subplot" target="_blank" rel="noopener"><strong>文档</strong></a>。</p>
<blockquote>
<p><a href="http://link.zhihu.com/?target=http%3A//matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.subplot" target="_blank" rel="noopener">http://link.zhihu.com/?target=http%3A//matplotlib.org/api/pyplot_api.html%23matplotlib.pyplot.subplot</a></p>
</blockquote>
<h3 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h3><p>你可以使用<strong>imshow</strong>函数来显示图像，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread, imresize</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">img = imread(<span class="string">'assets/cat.jpg'</span>)</span><br><span class="line">img_tinted = img * [<span class="number">1</span>, <span class="number">0.95</span>, <span class="number">0.9</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the original image</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.imshow(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Show the tinted image</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># A slight gotcha with imshow is that it might give strange results</span></span><br><span class="line"><span class="comment"># if presented with data that is not uint8. To work around this, we</span></span><br><span class="line"><span class="comment"># explicitly cast the image to uint8 before displaying it.</span></span><br><span class="line">plt.imshow(np.uint8(img_tinted))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>—————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/python_numpy/python_numpy_4.png?raw=true" width="300"></center>—————————————————————————————————————————</p>
<p><strong>本教程翻译完毕。</strong></p>
<blockquote>
<p>翻译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="noopener"><strong>Python Numpy Tutorial</strong></a>，由课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权进行翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/flood-sung" target="_blank" rel="noopener">Flood Sung</a>、<a href="https://www.zhihu.com/people/sunisdown" target="_blank" rel="noopener">SunisDown</a>、<a href="https://www.zhihu.com/people/gong-zi-jia-57" target="_blank" rel="noopener">巩子嘉</a>和一位不愿透露ID的知友对本翻译亦有贡献。</p>
<p>知乎地址：<a href="https://zhuanlan.zhihu.com/p/20878530" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20878530</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Neural_Network1/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Neural_Network1/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：神经网络笔记1</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-03-26 08:53:28" itemprop="dateModified" datetime="2018-03-26T08:53:28+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：神经网络笔记"><a href="#CS231n课程笔记翻译：神经网络笔记" class="headerlink" title="CS231n课程笔记翻译：神经网络笔记"></a>CS231n课程笔记翻译：神经网络笔记</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li>不用大脑做类比的快速简介</li>
<li>单个神经元建模<ul>
<li>生物动机和连接</li>
<li>作为线性分类器的单个神经元</li>
<li>常用的激活函数 </li>
</ul>
</li>
<li>神经网络结构<ul>
<li>层组织</li>
<li>前向传播计算例子</li>
<li>表达能力</li>
<li>设置层的数量和尺寸</li>
</ul>
</li>
<li>小节</li>
<li>参考文献</li>
</ul>
<h2 id="快速简介"><a href="#快速简介" class="headerlink" title="快速简介"></a>快速简介</h2><p>在不诉诸大脑的类比的情况下，依然是可以对神经网络算法进行介绍的。在线性分类一节中，在给出图像的情况下，是使用$s=Wx$来计算不同视觉类别的评分，其中$W$是一个矩阵，$x$是一个输入列向量，它包含了图像的全部像素数据。在使用数据库CIFAR-10的案例中，$x$是一个[3072x1]的列向量，$W$是一个[10x3072]的矩阵，所以输出的评分是一个包含10个分类评分的向量。</p>
<p>神经网络算法则不同，它的计算公式是$s=W_2max(0,W_1x)$。其中$W_1$的含义是这样的：举个例子来说，它可以是一个[100x3072]的矩阵，其作用是将图像转化为一个100维的过渡向量。函数$max(0,-)$是非线性的，它会作用到每个元素。这个非线性函数有多种选择，后续将会学到。但这个形式是一个最常用的选择，它就是简单地设置阈值，将所有小于0的值变成0。最终，矩阵$W_2$的尺寸是[10x100]，因此将得到10个数字，这10个数字可以解释为是分类的评分。注意非线性函数在计算上是至关重要的，如果略去这一步，那么两个矩阵将会合二为一，对于分类的评分计算将重新变成关于输入的线性函数。这个非线性函数就是<em>改变</em>的关键点。参数$W_1,W_2$将通过随机梯度下降来学习到，他们的梯度在反向传播过程中，通过链式法则来求导计算得出。</p>
<p>一个三层的神经网络可以类比地看做$s=W_3max(0,W_2max(0,W_1x))$，其中$W_1,W_2,W_3$是需要进行学习的参数。中间隐层的尺寸是网络的超参数，后续将学习如何设置它们。现在让我们先从神经元或者网络的角度理解上述计算。</p>
<h2 id="单个神经元建模"><a href="#单个神经元建模" class="headerlink" title="单个神经元建模"></a>单个神经元建模</h2><p>神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳，成为一个工程问题，并在机器学习领域取得良好效果。然而，讨论将还是从对生物系统的一个高层次的简略描述开始，因为神经网络毕竟是从这里得到了启发。</p>
<h3 id="生物动机与连接"><a href="#生物动机与连接" class="headerlink" title="生物动机与连接"></a>生物动机与连接</h3><p>大脑的基本计算单位是 <strong>神经元（neuron）</strong> 。人类的神经系统中大约有860亿个神经元，它们被大约10^14-10^15个 <strong>突触(synapses)</strong> 连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的 <strong>树突</strong> 获得输入信号，然后沿着它唯一的 <strong>轴突（axon）</strong> 产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。</p>
<p>在神经元的计算模型中，沿着轴突传播的信号（比如$x_0$）将基于突触的突触强度（比如$w_0$），与其他神经元的树突进行乘法交互（比如$w_0x_0$）。其观点是，突触的强度（也就是权重$w$），是可学习的且可以控制一个神经元对于另一个神经元的影响强度（还可以控制影响方向：使其兴奋（正权重）或使其抑制（负权重））。在基本模型中，树突将信号传递到细胞体，信号在细胞体中相加。如果最终之和高于某个阈值，那么神经元将会<em>激活</em>，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号的频率在交流信息。基于这个<em>速率编码</em>的观点，将神经元的激活率建模为<strong>激活函数（activation function）$f$</strong>，它表达了轴突上激活信号的频率。由于历史原因，激活函数常常选择使用<strong>sigmoid函数$\sigma$ </strong>，该函数输入实数值（求和后的信号强度），然后将输入值压缩到0-1之间。在本节后面部分会看到这些激活函数的各种细节。</p>
<p>————————————————————————————————————————</p>
<table><tr><br><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_0.png?raw=true" width="200" border="0"></center></td><br><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_1.jpg?raw=true" width="200" border="0"></center></td><br></tr></table>

<p>左边是生物神经元，右边是数学模型。</p>
<p>————————————————————————————————————————</p>
<p>一个神经元前向传播的实例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Neuron</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="comment"># ... </span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    <span class="string">""" 假设输入和权重是1-D的numpy数组，偏差是一个数字 """</span></span><br><span class="line">    cell_body_sum = np.sum(inputs * self.weights) + self.bias</span><br><span class="line">    firing_rate = <span class="number">1.0</span> / (<span class="number">1.0</span> + math.exp(-cell_body_sum)) <span class="comment"># sigmoid激活函数</span></span><br><span class="line">    <span class="keyword">return</span> firing_rate</span><br></pre></td></tr></table></figure>
<p>换句话说，每个神经元都对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数（或称为激活函数）。本例中使用的是sigmoid函数$\sigma(x)=1/(1+e^{-x})$。在本节的末尾部分将介绍不同激活函数的细节。</p>
<p><strong>粗糙模型</strong>：要注意这个对于生物神经元的建模是非常粗糙的：在实际中，有很多不同类型的神经元，每种都有不同的属性。生物神经元的树突可以进行复杂的非线性计算。突触并不就是一个简单的权重，它们是复杂的非线性动态系统。很多系统中，输出的峰值信号的精确时间点非常重要，说明速率编码的近似是不够全面的。鉴于所有这些已经介绍和更多未介绍的简化，如果你画出人类大脑和神经网络之间的类比，有神经科学背景的人对你的板书起哄也是非常自然的。如果你对此感兴趣，可以看看这份<a href="http://link.zhihu.com/?target=https%3A//physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf" target="_blank" rel="noopener"><strong>评论</strong></a>或者最新的<a href="http://link.zhihu.com/?target=http%3A//www.sciencedirect.com/science/article/pii/S0959438814000130" target="_blank" rel="noopener"><strong>另一份</strong></a>。</p>
<h3 id="作为线性分类器的单个神经元"><a href="#作为线性分类器的单个神经元" class="headerlink" title="作为线性分类器的单个神经元"></a>作为线性分类器的单个神经元</h3><p>神经元模型的前向计算数学公式看起来可能比较眼熟。就像在线性分类器中看到的那样，神经元有能力“喜欢”（激活函数值接近1），或者不喜欢（激活函数值接近0）输入空间中的某些线性区域。因此，只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。</p>
<p><strong>二分类Softmax分类器</strong>   举例来说，可以把$\displaystyle\sigma(\Sigma_iw_ix_i+b)$看做其中一个分类的概率$P(y_i=1|x_i;w)$，其他分类的概率为$P(y_i=0|x_i;w)=1-P(y_i=1|x_i;w)$，因为它们加起来必须为1。根据这种理解，可以得到交叉熵损失，这个在线性分一节中已经介绍。然后将它最优化为二分类的Softmax分类器（也就是逻辑回归）。因为sigmoid函数输出限定在0-1之间，所以分类器做出预测的基准是神经元的输出是否大于0.5。</p>
<p><strong>二分类SVM分类器</strong>    或者可以在神经元的输出外增加一个最大边界折叶损失（max-margin hinge loss）函数，将其训练成一个二分类的支持向量机。</p>
<p><strong>理解正则化</strong>  在SVM/Softmax的例子中，正则化损失从生物学角度可以看做<em>逐渐遗忘</em>，因为它的效果是让所有突触权重$w$在参数更新过程中逐渐向着0变化。</p>
<blockquote>
<p>一个单独的神经元可以用来实现一个二分类分类器，比如二分类的Softmax或者SVM分类器。</p>
</blockquote>
<h3 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h3><p>每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。下面是在实践中可能遇到的几种激活函数：</p>
<p>————————————————————————————————————————</p>
<p><table><tr></tr></table></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_2.jpg?raw=true" width="200"></center></td></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_3.jpg?raw=true" width="200"></center></td><br><br>左边是Sigmoid非线性函数，将实数压缩到[0,1]之间。右边是tanh函数，将实数压缩到[-1,1]。</p>
<p>————————————————————————————————————————</p>
<p><strong>Sigmoid</strong>  sigmoid非线性函数的数学公式是$\displaystyle\sigma(x)=1/(1+e^{-x})$，函数图像如上图的左边所示。在前一节中已经提到过，它输入实数值并将其“挤压”到0到1范围内。更具体地说，很大的负数变成0，很大的正数变成1。在历史上，sigmoid函数非常常用，这是因为它对于神经元的激活频率有良好的解释：从完全不激活（0）到在求和后的最大频率处的完全饱和（<strong>saturated</strong>）的激活（1）。然而现在sigmoid函数已经不太受欢迎，实际很少使用了，这是因为它有两个主要缺点：</p>
<ul>
<li><em>Sigmoid函数饱和使梯度消失</em>。sigmoid神经元有一个不好的特性，就是当神经元的激活在接近0或1处时会饱和：在这些区域，梯度几乎为0。回忆一下，在反向传播的时候，这个（局部）梯度将会与整个损失函数关于该门单元输出的梯度相乘。因此，如果局部梯度非常小，那么相乘的结果也会接近零，这会有效地“杀死”梯度，几乎就有没有信号通过神经元传到权重再到数据了。还有，为了防止饱和，必须对于权重矩阵初始化特别留意。比如，如果初始化权重过大，那么大多数神经元将会饱和，导致网络就几乎不学习了。</li>
<li><em>Sigmoid函数的输出不是零中心的</em>。这个性质并不是我们想要的，因为在神经网络后面层中的神经元得到的数据将不是零中心的。这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数（比如在$f=w^Tx+b$中每个元素都$x&gt;0$），那么关于$w$的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数（具体依整个表达式$f$而定）。这将会导致梯度下降权重更新时出现z字型的下降。然而，可以看到整个批量的数据的梯度被加起来后，对于权重的最终更新将会有不同的正负，这样就从一定程度上减轻了这个问题。因此，该问题相对于上面的神经元饱和问题来说只是个小麻烦，没有那么严重。</li>
</ul>
<p><strong>Tanh</strong>    tanh非线性函数图像如上图右边所示。它将实数值压缩到[-1,1]之间。和sigmoid神经元一样，它也存在饱和问题，但是和sigmoid神经元不同的是，它的输出是零中心的。因此，在实际操作中，<em>tanh非线性函数比sigmoid非线性函数更受欢迎</em>。注意tanh神经元是一个简单放大的sigmoid神经元，具体说来就是：$tanh(x)=2\sigma(2x)-1$。<br>————————————————————————————————————————</p>
<p><table><tr></tr></table></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_4.jpg?raw=true" width="200"></center></td></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_5.jpg?raw=true" width="200"></center></td><br><br>左边是ReLU（校正线性单元：Rectified Linear Unit）激活函数，当$x=0$时函数值为0。当$x&gt;0$函数的斜率为1。右边是从<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Efritz/absps/imagenet.pdf" target="_blank" rel="noopener"><strong>Krizhevsky</strong></a> 等的论文中截取的图表，指明使用ReLU比使用tanh的收敛快6倍。</p>
<p>————————————————————————————————————————</p>
<p><strong>ReLU</strong>   在近些年ReLU变得非常流行。它的函数公式是$f(x)=max(0,x)$)。换句话说，这个激活函数就是一个关于0的阈值（如上图左侧）。使用ReLU有以下一些优缺点：</p>
<ul>
<li>优点：相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用（<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Efritz/absps/imagenet.pdf" target="_blank" rel="noopener"><strong>Krizhevsky</strong></a> 等的论文指出有6倍之多）。据称这是由它的线性，非饱和的公式导致的。</li>
<li>优点：sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU可以简单地通过对一个矩阵进行阈值计算得到。</li>
<li>缺点：在训练的时候，ReLU单元比较脆弱并且可能“死掉”。举例来说，当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，在这种状态下神经元将无法被其他任何数据点再次激活。如果这种情况发生，那么从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。</li>
</ul>
<p><strong>Leaky ReLU</strong>    Leaky ReLU是为解决“ReLU死亡”问题的尝试。ReLU中当x&lt;0时，函数值为0。而Leaky ReLU则是给出一个很小的负数梯度值，比如0.01。所以其函数公式为</p>
<p><center>$$f(x)=1(x&lt;0)(\alpha x)+1(x&gt;=0)(x)$$</center><br>其中 $\alpha$是一个小的常量。有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定。Kaiming He等人在2015年发布的论文<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.01852" target="_blank" rel="noopener"><strong>Delving Deep into Rectifiers</strong></a> 中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。</p>
<p><strong>Maxout</strong>      一些其他类型的单元被提了出来，它们对于权重和数据的内积结果不再使用$f(w^Tx+b)$函数形式。一个相关的流行选择是Maxout（最近由<a href="http://link.zhihu.com/?target=http%3A//www-etud.iro.umontreal.ca/%7Egoodfeli/maxout.html" target="_blank" rel="noopener"><strong>Goodfellow</strong></a> 等发布）神经元。Maxout是对ReLU和leaky ReLU的一般化归纳，它的函数是：$max(w^T_1x+b_1,w^T_2x+b_2)$。ReLU和Leaky ReLU都是这个公式的特殊情况（比如ReLU就是当$w_1,b_1=0$的时候）。这样Maxout神经元就拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）。然而和ReLU对比，它每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。</p>
<p>以上就是一些常用的神经元及其激活函数。最后需要注意一点：在同一个网络中混合使用不同类型的神经元是非常少见的，虽然没有什么根本性问题来禁止这样做。</p>
<p><strong>一句话</strong>：“<em>那么该用那种呢？</em>”用ReLU非线性函数。注意设置好学习率，或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。</p>
<h2 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h2><h3 id="灵活地组织层"><a href="#灵活地组织层" class="headerlink" title="灵活地组织层"></a>灵活地组织层</h3><p><strong>将神经网络算法以神经元的形式图形化</strong>    神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接。也就是说，一些神经元的输出是另一些神经元的输入。在网络中是不允许循环的，因为这样会导致前向传播的无限循环。通常神经网络模型中神经元是分层的，而不是像生物神经元一样聚合成大小不一的团状。对于普通神经网络，最普通的层的类型是<strong>全连接层（fully-connected layer）</strong>。全连接层中的神经元与其前后两层的神经元是完全成对连接的，但是在同一个全连接层内的神经元之间没有连接。下面是两个神经网络的图例，都使用的全连接层：</p>
<p>————————————————————————————————————————</p>
<p><table><tr></tr></table></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_6.jpg?raw=true" width="250"></center></td></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_7.jpg?raw=true" width="250"></center></td><br><br>左边是一个2层神经网络，隐层由4个神经元（也可称为单元（unit））组成，输出层由2个神经元组成，输入层是3个神经元。右边是一个3层神经网络，两个含4个神经元的隐层。注意：层与层之间的神经元是全连接的，但是层内的神经元不连接。</p>
<p>————————————————————————————————————————</p>
<p><strong>命名规则</strong>     当我们说N层神经网络的时候，我们没有把输入层算入。因此，单层的神经网络就是没有隐层的（输入直接映射到输出）。因此，有的研究者会说逻辑回归或者SVM只是单层神经网络的一个特例。研究者们也会使用<em>人工神经网络（</em>Artificial Neural Networks <em>缩写ANN）</em>或者<em>多层感知器（Multi-Layer Perceptrons 缩写MLP）</em>来指代神经网络。很多研究者并不喜欢神经网络算法和人类大脑之间的类比，他们更倾向于用<em>单元（unit）</em>而不是神经元作为术语。</p>
<p><strong>输出层</strong>    和神经网络中其他层不同，输出层的神经元一般是不会有激活函数的（或者也可以认为它们有一个线性相等的激活函数）。这是因为最后的输出层大多用于表示分类评分值，因此是任意值的实数，或者某种实数值的目标数（比如在回归中）。</p>
<p><strong>确定网络尺寸</strong>    用来度量神经网络的尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数，用上面图示的两个网络举例：</p>
<ul>
<li>第一个网络有4+2=6个神经元（输入层不算），[3x4]+[4x2]=20个权重，还有4+2=6个偏置，共26个可学习的参数。</li>
<li>第二个网络有4+4+1=9个神经元，[3x4]+[4x4]+[4x1]=32个权重，4+4+1=9个偏置，共41个可学习的参数。</li>
</ul>
<p>为了方便对比，现代卷积神经网络能包含约1亿个参数，可由10-20层构成（这就是深度学习）。然而，<em>有效（effective）</em>连接的个数因为参数共享的缘故大大增多。在后面的卷积神经网络内容中我们将学习更多。</p>
<h3 id="前向传播计算举例"><a href="#前向传播计算举例" class="headerlink" title="前向传播计算举例"></a>前向传播计算举例</h3><p><em>不断重复的矩阵乘法与激活函数交织</em>。将神经网络组织成层状的一个主要原因，就是这个结构让神经网络算法使用矩阵向量操作变得简单和高效。用上面那个3层神经网络举例，输入是[3x1]的向量。一个层所有连接的强度可以存在一个单独的矩阵中。比如第一个隐层的权重<strong>W1</strong>是[4x3]，所有单元的偏置储存在<strong>b1</strong>中，尺寸[4x1]。这样，每个神经元的权重都在<strong>W1</strong>的一个行中，于是矩阵乘法<strong>np.dot(W1, x)</strong>就能计算该层中所有神经元的激活数据。类似的，<strong>W2</strong>将会是[4x4]矩阵，存储着第二个隐层的连接，<strong>W3</strong>是[1x4]的矩阵，用于输出层。完整的3层神经网络的前向传播就是简单的3次矩阵乘法，其中交织着激活函数的应用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个3层神经网络的前向传播:</span></span><br><span class="line">f = <span class="keyword">lambda</span> x: <span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-x)) <span class="comment"># 激活函数(用的sigmoid)</span></span><br><span class="line">x = np.random.randn(<span class="number">3</span>, <span class="number">1</span>) <span class="comment"># 含3个数字的随机输入向量(3x1)</span></span><br><span class="line">h1 = f(np.dot(W1, x) + b1) <span class="comment"># 计算第一个隐层的激活数据(4x1)</span></span><br><span class="line">h2 = f(np.dot(W2, h1) + b2) <span class="comment"># 计算第二个隐层的激活数据(4x1)</span></span><br><span class="line">out = np.dot(W3, h2) + b3 <span class="comment"># 神经元输出(1x1)</span></span><br></pre></td></tr></table></figure>
<p>在上面的代码中，<strong>W1，W2，W3，b1，b2，b3</strong>都是网络中可以学习的参数。注意<strong>x</strong>并不是一个单独的列向量，而可以是一个批量的训练数据（其中每个输入样本将会是<strong>x</strong>中的一列），所有的样本将会被并行化的高效计算出来。注意神经网络最后一层通常是没有激活函数的（例如，在分类任务中它给出一个实数值的分类评分）。</p>
<blockquote>
<p>全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。</p>
</blockquote>
<h3 id="表达能力"><a href="#表达能力" class="headerlink" title="表达能力"></a>表达能力</h3><p>理解具有全连接层的神经网络的一个方式是：可以认为它们定义了一个由一系列函数组成的函数族，网络的权重就是每个函数的参数。如此产生的问题是：该函数族的表达能力如何？存在不能被神经网络表达的函数吗？</p>
<p>现在看来，拥有至少一个隐层的神经网络是一个<em>通用的近似器</em>。在研究（例如1989年的论文<a href="http://link.zhihu.com/?target=http%3A//www.dartmouth.edu/%257Egvc/Cybenko_MCSS.pdf" target="_blank" rel="noopener"><strong>Approximation by Superpositions of Sigmoidal Function</strong></a> ，或者<a href="http://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap4.html" target="_blank" rel="noopener"><strong>Michael Nielsen</strong></a> 的这个直观解释。）中已经证明，给出任意连续函数$f(x)$和任意$\epsilon &gt;0$，均存在一个至少含1个隐层的神经网络$g(x)$（并且网络中有合理选择的非线性激活函数，比如sigmoid），对于$\forall x$，使得$|f(x)-g(x)|&lt;\epsilon$。换句话说，神经网络可以近似任何连续函数。</p>
<p>既然一个隐层就能近似任何函数，那为什么还要构建更多层来将网络做得更深？答案是：虽然一个2层网络在数学理论上能完美地近似所有连续函数，但在实际操作中效果相对较差。在一个维度上，虽然以$a,b,c$为参数向量“指示块之和”函数$g(x)=\sum_ic_i1(a_i&lt;x&lt;b_i) $也是通用的近似器，但是谁也不会建议在机器学习中使用这个函数公式。神经网络在实践中非常好用，是因为它们表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，网络通过最优化算法（例如梯度下降）能比较容易地学习到这个函数。类似的，虽然在理论上深层网络（使用了多个隐层）和单层网络的表达能力是一样的，但是就实践经验而言，深度网络效果比单层网络好。</p>
<p>另外，在实践中3层的神经网络会比2层的表现好，然而继续加深（做到4，5，6层）很少有太大帮助。卷积神经网络的情况却不同，在卷积神经网络中，对于一个良好的识别系统来说，深度是一个极端重要的因素（比如数十(以10为量级)个可学习的层）。对于该现象的一种解释观点是：因为图像拥有层次化结构（比如脸是由眼睛等组成，眼睛又是由边缘组成），所以多层处理对于这种数据就有直观意义。</p>
<p>全面的研究内容还很多，近期研究的进展也很多。如果你对此感兴趣，我么推荐你阅读下面文献：</p>
<ul>
<li><a href="http://link.zhihu.com/?target=http%3A//www.deeplearningbook.org/" target="_blank" rel="noopener"><strong>Deep Learning</strong></a> 的<a href="http://link.zhihu.com/?target=http%3A//www.deeplearningbook.org/contents/mlp.html" target="_blank" rel="noopener"><strong>Chapter6.4</strong></a> ，作者是Bengio等。</li>
<li><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.6184" target="_blank" rel="noopener"><strong>Do Deep Nets Really Need to be Deep?</strong></a> </li>
<li><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1412.6550" target="_blank" rel="noopener"><strong>FitNets: Hints for Thin Deep Nets</strong></a> </li>
</ul>
<h3 id="设置层的数量和尺寸"><a href="#设置层的数量和尺寸" class="headerlink" title="设置层的数量和尺寸"></a>设置层的数量和尺寸</h3><p>在面对一个具体问题的时候该确定网络结构呢？到底是不用隐层呢？还是一个隐层？两个隐层或更多？每个层的尺寸该多大？</p>
<p>首先，要知道当我们增加层的数量和尺寸时，网络的容量上升了。即神经元们可以合作表达许多复杂函数，所以表达函数的空间增加。例如，如果有一个在二维平面上的二分类问题。我们可以训练3个不同的神经网络，每个网络都只有一个隐层，但是每层的神经元数目不同：</p>
<p>————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_8.jpg?raw=true" width="400"></center><br>更大的神经网络可以表达更复杂的函数。数据是用不同颜色的圆点表示他们的不同类别，决策边界是由训练过的神经网络做出的。你可以在 <a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html" target="_blank" rel="noopener"><strong>ConvNetsJS demo</strong></a> 上练练手。</p>
<p>————————————————————————————————————————</p>
<p>在上图中，可以看见有更多神经元的神经网络可以表达更复杂的函数。然而这既是优势也是不足，优势是可以分类更复杂的数据，不足是可能造成对训练数据的过拟合。<strong>过拟合（Overfitting）</strong> 是网络对数据中的噪声有很强的拟合能力，而没有重视数据间（假设）的潜在基本关系。举例来说，有20个神经元隐层的网络拟合了所有的训练数据，但是其代价是把决策边界变成了许多不相连的红绿区域。而有3个神经元的模型的表达能力只能用比较宽泛的方式去分类数据。它将数据看做是两个大块，并把个别在绿色区域内的红色点看做噪声。在实际中，这样可以在测试数据中获得更好的 <strong>泛化（generalization）</strong> 能力。</p>
<p>基于上面的讨论，看起来如果数据不是足够复杂，则似乎小一点的网络更好，因为可以防止过拟合。然而并非如此，防止神经网络的过拟合有很多方法（L2正则化，dropout和输入噪音等），后面会详细讨论。在实践中，使用这些方法来控制过拟合比减少网络神经元数目要好得多。</p>
<p>不要减少网络神经元数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练：虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。因为神经网络是非凸的，就很难从数学上研究这些特性。即便如此，还是有一些文章尝试对这些目标函数进行理解，例如<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1412.0233" target="_blank" rel="noopener"><strong>The Loss Surfaces of Multilayer Networks</strong></a> 这篇论文。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。</p>
<p>重申一下，正则化强度是控制神经网络过拟合的好方法。看下图结果：</p>
<p>————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_9.jpg?raw=true" width="400"></center><br>不同正则化强度的效果：每个神经网络都有20个隐层神经元，但是随着正则化强度增加，它的决策边界变得更加平滑。你可以在 <a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html" target="_blank" rel="noopener"><strong>ConvNetsJS demo</strong></a> 上练练手。</p>
<p>————————————————————————————————————————</p>
<p>需要记住的是：不应该因为害怕出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>小结如下：</p>
<ul>
<li><p>介绍了生物神经元的粗略模型；</p>
</li>
<li><p>讨论了几种不同类型的激活函数，其中ReLU是最佳推荐；</p>
</li>
<li><p>介绍了<strong>神经网络</strong>，神经元通过<strong>全连接层</strong>连接，层间神经元两两相连，但是层内神经元不连接；</p>
</li>
<li><p>理解了分层的结构能够让神经网络高效地进行矩阵乘法和激活函数运算；</p>
</li>
<li><p>理解了神经网络是一个<strong>通用函数近似器</strong>，但是该性质与其广泛使用无太大关系。之所以使用神经网络，是因为它们对于实际问题中的函数的公式能够某种程度上做出“正确”假设。</p>
</li>
<li><p>讨论了更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。在后续章节中我们讲学习更多正则化的方法，尤其是dropout。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1></li>
<li><p>使用Theano的<a href="http://link.zhihu.com/?target=http%3A//www.deeplearning.net/tutorial/mlp.html" target="_blank" rel="noopener"><strong>deeplearning.net tutorial</strong></a></p>
</li>
<li><p><a href="http://link.zhihu.com/?target=http%3A//www.deeplearning.net/tutorial/mlp.html" target="_blank" rel="noopener"><strong>ConvNetJS</strong></a></p>
</li>
<li><p><a href="http://link.zhihu.com/?target=http%3A//neuralnetworksanddeeplearning.com/chap1.html" target="_blank" rel="noopener"><strong>Michael Nielsen’s tutorials</strong></a></p>
</li>
</ul>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-1/" target="_blank" rel="noopener"><strong>Neural Nets notes 1</strong></a> ，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a> 授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/hmonkey" target="_blank" rel="noopener">巩子嘉</a>和<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改.</p>
<p>知乎地址： <a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Optimizer/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Optimizer/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：最优化笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-03-26 08:54:02" itemprop="dateModified" datetime="2018-03-26T08:54:02+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：最优化笔记"><a href="#CS231n课程笔记翻译：最优化笔记" class="headerlink" title="CS231n课程笔记翻译：最优化笔记"></a>CS231n课程笔记翻译：最优化笔记</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li><p>简介</p>
</li>
<li><p>损失函数可视化</p>
</li>
<li><p>最优化</p>
<ul>
<li>策略#1：随机搜索</li>
</ul>
</li>
</ul>
<ul>
<li>策略#2：随机局部搜索</li>
<li>策略#3：跟随梯度 </li>
</ul>
<ul>
<li><p>梯度计算</p>
<ul>
<li>使用有限差值进行数值计算</li>
</ul>
</li>
</ul>
<ul>
<li>微分计算梯度</li>
</ul>
<ul>
<li><p>梯度下降</p>
</li>
<li><p>小结</p>
</li>
</ul>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在上一节中，我们介绍了图像分类任务中的两个关键部分：</p>
<ol>
<li>基于参数的<strong>评分函数。</strong>该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。</li>
<li><strong>损失函数</strong>。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。</li>
</ol>
<p>上节中，线性函数的形式是$f(x_i, W)=Wx_i$，而SVM实现的公式是：</p>
<center>$$L=\displaystyle\frac{1}{N}\sum_i\sum_{j\not= y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)]+\alpha R(W)$$</center>

<p>对于图像数据$x_i$，如果基于参数集$W$做出的分类预测与真实情况比较一致，那么计算出来的损失值$L$)就很低。现在介绍第三个，也是最后一个关键部分：<strong>最优化Optimization</strong>。最优化是寻找能使得损失函数值最小化的参数$W$的过程。</p>
<p><strong>铺垫</strong>：一旦理解了这三个部分是如何相互运作的，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化过程这两个部分将会保持相对稳定。</p>
<h2 id="损失函数可视化"><a href="#损失函数可视化" class="headerlink" title="损失函数可视化"></a>损失函数可视化</h2><p>本课中讨论的损失函数一般都是定义在高维度的空间中（比如，在CIFAR-10中一个线性分类器的权重矩阵大小是[10x3073]，就有30730个参数），这样要将其可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，就能得到一些直观感受。例如，随机生成一个权重矩阵$W$，该矩阵就与高维空间中的一个点对应。然后沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向$W_1$并且沿着此方向计算损失值，计算方法是根据不同的$a$值来计算$L(W+aW_1)$。这个过程将生成一个图表，其x轴是$a$值，y轴是损失函数值。同样的方法还可以用在两个维度上，通过改变$a,b$来计算损失值$L(W+aW_1+bW_2)$，从而给出二维的图像。在图像中，$a,b$可以分别用x和y轴表示，而损失函数的值可以用颜色变化表示：</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_0.jpg?raw=true" width="350"></center>

<p>一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的100个数据。<strong>左</strong>：$a$值变化在某个维度方向上对应的的损失值变化。<strong>中和右</strong>：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。</p>
<p>—————————————————————————————————————————</p>
<p>我们可以通过数学公式来解释损失函数的分段线性结构。对于一个单独的数据，有损失函数的计算公式如下：</p>
<center>$$Li=\sum_{j\not=y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+1)]$$</center>

<p>通过公式可见，每个样本的数据损失值是以$W$为参数的线性函数的总和（零阈值来源于$max(0,-)$函数）。$W$的每一行（即$w_j$），有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有3个只有1个维度的点，数据集数据点有3个类别。那么完整的无正则化SVM的损失值计算如下：</p>
<center>$$L_0=max(0,w^T_1x_0-w^T_0x_0+1)+max(0,w^T_2x_0-w^T_0x_0+1)$$</center><br><center>$$L_1=max(0,w^T_0x_1-w^T_1x_1+1)+max(0,w^T_2x_1-w^T_1x_1+1)$$</center><br><center>$$L_2=max(0,w^T_0x_2-w^T_2x_2+1)+max(0,w^T_1x_2-w^T_2x_2+1)$$</center><br><center>$$L=(L_0+L_1+L_2)/3$$</center>

<p>因为这些例子都是一维的，所以数据$x_i$和权重$w_j$都是数字。观察$w_0$，可以看到上面的式子中一些项是$w_0$的线性函数，且每一项都会与0比较，取两者的最大值。可作图如下：——————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_1.png?raw=true" width="350"></center>

<p>从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。完整的SVM数据损失就是这个形状的30730维版本。</p>
<p>——————————————————————————————————————</p>
<p>需要多说一句的是，你可能根据SVM的损失函数的碗状外观猜出它是一个<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Convex_function" target="_blank" rel="noopener"><strong>凸函数</strong></a>。关于如何高效地最小化凸函数的论文有很多，你也可以学习斯坦福大学关于（<a href="http://link.zhihu.com/?target=http%3A//stanford.edu/%7Eboyd/cvxbook/" target="_blank" rel="noopener"><strong>凸函数最优化</strong></a>）的课程。但是一旦我们将<img src="http://www.zhihu.com/equation?tex=f" alt="f">函数扩展到神经网络，目标函数就就不再是凸函数了，图像也不会像上面那样是个碗状，而是凹凸不平的复杂地形形状。</p>
<p><em>不可导的损失函数。</em>作为一个技术笔记，你要注意到：由于max操作，损失函数中存在一些<em>不可导点（kinks），</em>这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Subderivative" target="_blank" rel="noopener"><strong>次梯度（subgradient）</strong></a>依然存在且常常被使用。在本课中，我们将交换使用<em>次梯度</em>和<em>梯度</em>两个术语。</p>
<h2 id="最优化-Optimization"><a href="#最优化-Optimization" class="headerlink" title="最优化 Optimization"></a>最优化 Optimization</h2><p>重申一下：损失函数可以量化某个具体权重集<strong>W</strong>的质量。而最优化的目标就是找到能够最小化损失函数值的<strong>W</strong> 。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于有一些经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM 损失函数）是一个凸函数问题。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。</p>
<h3 id="策略-1：一个差劲的初始方案：随机搜索"><a href="#策略-1：一个差劲的初始方案：随机搜索" class="headerlink" title="策略#1：一个差劲的初始方案：随机搜索"></a><strong>策略#1：一个差劲的初始方案：随机搜索</strong></h3><p>既然确认参数集<strong>W</strong>的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）</span></span><br><span class="line"><span class="comment"># 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）</span></span><br><span class="line"><span class="comment"># 假设函数L对损失函数进行评价</span></span><br><span class="line"></span><br><span class="line">bestloss = float(<span class="string">"inf"</span>) <span class="comment"># Python assigns the highest possible float value</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.0001</span> <span class="comment"># generate random parameters</span></span><br><span class="line">  loss = L(X_train, Y_train, W) <span class="comment"># get the loss over the entire training set</span></span><br><span class="line">  <span class="keyword">if</span> loss &lt; bestloss: <span class="comment"># keep track of the best solution</span></span><br><span class="line">    bestloss = loss</span><br><span class="line">    bestW = W</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'in attempt %d the loss was %f, best %f'</span> % (num, loss, bestloss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># in attempt 0 the loss was 9.401632, best 9.401632</span></span><br><span class="line"><span class="comment"># in attempt 1 the loss was 8.959668, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 2 the loss was 9.044034, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 3 the loss was 9.278948, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 4 the loss was 8.857370, best 8.857370</span></span><br><span class="line"><span class="comment"># in attempt 5 the loss was 8.943151, best 8.857370</span></span><br><span class="line"><span class="comment"># in attempt 6 the loss was 8.605604, best 8.605604</span></span><br><span class="line"><span class="comment"># ... (trunctated: continues for 1000 lines)</span></span><br></pre></td></tr></table></figure>
<p>在上面的代码中，我们尝试了若干随机生成的权重矩阵<strong>W</strong>，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重<strong>W</strong>取出，然后去跑测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]</span></span><br><span class="line">scores = Wbest.dot(Xte_cols) <span class="comment"># 10 x 10000, the class scores for all test examples</span></span><br><span class="line"><span class="comment"># 找到在每列中评分值最大的索引（即预测的分类）</span></span><br><span class="line">Yte_predict = np.argmax(scores, axis = <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 以及计算准确率</span></span><br><span class="line">np.mean(Yte_predict == Yte)</span><br><span class="line"><span class="comment"># 返回 0.1555</span></span><br></pre></td></tr></table></figure>
<p>验证集上表现最好的权重<strong>W</strong>跑测试集的准确率是<strong>15.5%，</strong>而完全随机猜的准确率是10%，如此看来，这个准确率对于这样一个不经过大脑的策略来说，还算不错嘛！</p>
<p><strong>核心思路：迭代优化</strong>。当然，我们肯定能做得更好些。核心思路是：虽然找到最优的权重<strong>W</strong>非常困难，甚至是不可能的（尤其当<strong>W</strong>中存的是整个神经网络的权重的时候），但如果问题转化为：对一个权重矩阵集<strong>W</strong>取优，使其损失值稍微减少。那么问题的难度就大大降低了。换句话说，我们的方法从一个随机的<strong>W</strong>开始，然后对其迭代取优，每次都让它的损失值变得更小一点。</p>
<blockquote>
<p>我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。</p>
</blockquote>
<p><strong>蒙眼徒步者的比喻</strong>：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为<strong>W</strong>是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。</p>
<h3 id="策略-2：随机本地搜索"><a href="#策略-2：随机本地搜索" class="headerlink" title="策略#2：随机本地搜索"></a><strong>策略#2：随机本地搜索</strong></h3><p>第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个随机的扰动$\delta W$ ，只有当$W+\delta W$的损失值变低，我们才会更新。这个过程的具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 生成随机初始W</span></span><br><span class="line">bestloss = float(<span class="string">"inf"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  step_size = <span class="number">0.0001</span></span><br><span class="line">  Wtry = W + np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * step_size</span><br><span class="line">  loss = L(Xtr_cols, Ytr, Wtry)</span><br><span class="line">  <span class="keyword">if</span> loss &lt; bestloss:</span><br><span class="line">    W = Wtry</span><br><span class="line">    bestloss = loss</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'iter %d loss is %f'</span> % (i, bestloss)</span><br></pre></td></tr></table></figure>
<p>使用同样的数据（1000），这个方法可以得到<strong>21.4%</strong>的分类准确率。这个比策略一好，但是依然过于浪费计算资源。</p>
<h3 id="策略-3：跟随梯度"><a href="#策略-3：跟随梯度" class="headerlink" title="策略#3：跟随梯度"></a><strong>策略#3：跟随梯度</strong></h3><p>前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的<strong>梯度（gradient）</strong>。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。</p>
<p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数<strong>derivatives</strong>）。对一维函数的求导公式如下：</p>
<center>$$\displaystyle\frac{df(x)}{dx}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$</center>

<p>当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p>
<h2 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h2><p>计算梯度有两种方法：一个是缓慢的近似方法（<strong>数值梯度法</strong>），但实现相对简单。另一个方法（<strong>分析梯度法</strong>）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对两种方法进行介绍：</p>
<h3 id="利用有限差值计算梯度"><a href="#利用有限差值计算梯度" class="headerlink" title="利用有限差值计算梯度"></a><strong>利用有限差值计算梯度</strong></h3><p>上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数<strong>f</strong>和向量<strong>x，</strong>计算<strong>f</strong>的梯度的通用函数，它返回函数<strong>f</strong>在点<strong>x处</strong>的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_numerical_gradient</span><span class="params">(f, x)</span>:</span></span><br><span class="line">  <span class="string">"""  </span></span><br><span class="line"><span class="string">  一个f在x处的数值梯度法的简单实现</span></span><br><span class="line"><span class="string">  - f是只有一个参数的函数</span></span><br><span class="line"><span class="string">  - x是计算梯度的点</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line"></span><br><span class="line">  fx = f(x) <span class="comment"># 在原点计算函数值</span></span><br><span class="line">  grad = np.zeros(x.shape)</span><br><span class="line">  h = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对x中所有的索引进行迭代</span></span><br><span class="line">  it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算x+h处的函数值</span></span><br><span class="line">    ix = it.multi_index</span><br><span class="line">    old_value = x[ix]</span><br><span class="line">    x[ix] = old_value + h <span class="comment"># 增加h</span></span><br><span class="line">    fxh = f(x) <span class="comment"># 计算f(x + h)</span></span><br><span class="line">    x[ix] = old_value <span class="comment"># 存到前一个值中 (非常重要)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算偏导数</span></span><br><span class="line">    grad[ix] = (fxh - fx) / h <span class="comment"># 坡度</span></span><br><span class="line">    it.iternext() <span class="comment"># 到下个维度</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<p>根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化h，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量<strong>grad</strong>中。</p>
<p><strong>实践考量</strong>：注意在数学公式中，<strong>h</strong>的取值是趋近于0的，然而在实际中，用一个很小的数值（比如例子中的1e-5）就足够了。在不产生数值计算出错的理想前提下，你会使用尽可能小的h。还有，实际中用<strong>中心差值公式（centered difference formula)</strong> $[f(x+h)-f(x-h)]/2h$ 效果较好。细节可查看<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Numerical_differentiation" target="_blank" rel="noopener"><strong>wiki</strong></a>。</p>
<p>可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要使用上面的代码我们需要一个只有一个参数的函数</span></span><br><span class="line"><span class="comment"># (在这里参数就是权重)所以也包含了X_train和Y_train</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CIFAR10_loss_fun</span><span class="params">(W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> L(X_train, Y_train, W)</span><br><span class="line"></span><br><span class="line">W = np.random.rand(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 随机权重向量</span></span><br><span class="line">df = eval_numerical_gradient(CIFAR10_loss_fun, W) <span class="comment"># 得到梯度</span></span><br></pre></td></tr></table></figure>
<p>梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">loss_original = CIFAR10_loss_fun(W) <span class="comment"># 初始损失值</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'original loss: %f'</span> % (loss_original, )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同步长的效果</span></span><br><span class="line"><span class="keyword">for</span> step_size_log <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">-9</span>, <span class="number">-8</span>, <span class="number">-7</span>, <span class="number">-6</span>, <span class="number">-5</span>,<span class="number">-4</span>,<span class="number">-3</span>,<span class="number">-2</span>,<span class="number">-1</span>]:</span><br><span class="line">  step_size = <span class="number">10</span> ** step_size_log</span><br><span class="line">  W_new = W - step_size * df <span class="comment"># 权重空间中的新位置</span></span><br><span class="line">  loss_new = CIFAR10_loss_fun(W_new)</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'for step size %f new loss: %f'</span> % (step_size, loss_new)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># original loss: 2.200718</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-10 new loss: 2.200652</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-09 new loss: 2.200057</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-08 new loss: 2.194116</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-07 new loss: 2.135493</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-06 new loss: 1.647802</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-05 new loss: 2.844355</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-04 new loss: 25.558142</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-03 new loss: 254.086573</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-02 new loss: 2539.370888</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-01 new loss: 25392.214036</span></span><br></pre></td></tr></table></figure>
<p><strong>在梯度负方向上更新</strong>：在上面的代码中，为了计算<strong>W_new</strong>，要注意我们是向着梯度<strong>df</strong>的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。</p>
<p><strong>步长的影响</strong>：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。在后续的课程中可以看到，选择步长（也叫作<em>学习率</em>）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。还是用蒙眼徒步者下山的比喻，这就好比我们可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多长的步长呢？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在上面的代码中就能看见反例，在某些点如果步长过大，反而可能越过最低点导致更高的损失值。</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_2.jpg?raw=true" width="250"></center>

<p>将步长效果视觉化的图例。从某个具体的点W开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为<strong>学习率</strong>）将会是我们在调参中最重要的超参数之一。</p>
<p>————————————————————————————————————————</p>
<p><strong>效率问题</strong>：你可能已经注意到，计算数值梯度的复杂性和参数的量线性相关。在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度。现代神经网络很容易就有上千万的参数，因此这个问题只会越发严峻。显然这个策略不适合大规模数据，我们需要更好的策略。</p>
<h3 id="微分分析计算梯度"><a href="#微分分析计算梯度" class="headerlink" title="微分分析计算梯度"></a>微分分析计算梯度</h3><p>使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于<em>h</em>值是选取了一个很小的数值，但真正的梯度定义中<em>h</em>趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做<strong>梯度检查</strong>。</p>
<p>用SVM的损失函数在某个数据点上的计算来举例：</p>
<p><cneter>$$L_i=\displaystyle\sum_{j\not =y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)]$$</cneter></p>
<p>可以对函数进行微分。比如，对$w_{y_i}$进行微分得到：</p>
<center>$$ \nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$</center>

<p><strong>译者注：原公式中1为空心字体，尝试\mathbb{}等多种方法仍无法实现，请知友指点。</strong></p>
<p>其中$\mathbb{1}​$是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以$x_i​$就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些$j\not =y_i​$行的梯度是：</p>
<center>$$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i$$</center>

<p>一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为<em>梯度下降</em>，他的<strong>普通</strong>版本是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 进行梯度更新</span></span><br></pre></td></tr></table></figure>
<p>这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。</p>
<p><strong>小批量数据梯度下降（**</strong>Mini-batch gradient descent<strong>**）</strong>：在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的<strong>小批量（batches）</strong>数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是多少呢？一百二十万个。这个小批量数据就用来实现一个参数更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的小批量数据梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  data_batch = sample_training_data(data, <span class="number">256</span>) <span class="comment"># 256个数据</span></span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 参数更新</span></span><br></pre></td></tr></table></figure>
<p>这个方法之所以效果不错，是因为训练集中的数据都是相关的。要理解这一点，可以想象一个极端情况：在ILSVRC中的120万个图像是1000张不同图片的复制（每个类别1张图片，每张图片有1200张复制）。那么显然计算这1200张复制图像的梯度就应该是一样的。对比120万张图片的数据损失的均值与只计算1000张的子集的数据损失均值时，结果应该是一样的。实际情况中，数据集肯定不会包含重复图像，那么小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。</p>
<p>小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为<strong>随机梯度下降（Stochastic Gradient Descent 简称SGD）</strong>，有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_3.jpg?raw=true" width="300"></center>

<p>信息流的总结图例。数据集中的(x,y)是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量<strong>f</strong>中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分f和实际标签y之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。</p>
<p>—————————————————————————————————————————</p>
<p>在本节课中：</p>
<ul>
<li>将损失函数比作了一个<strong>高维度的最优化地形</strong>，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。</li>
<li>提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。</li>
<li>函数的<strong>梯度</strong>给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是<em>h</em>，用来计算数值梯度）。</li>
<li>参数更新需要有技巧地设置<strong>步长</strong>。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。</li>
<li>讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用<strong>梯度检查</strong>来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。</li>
<li>介绍了<strong>梯度下降</strong>算法，它在循环中迭代地计算梯度并更新参数。</li>
</ul>
<p><strong>预告</strong>：这节课的核心内容是：理解并能计算损失函数关于权重的梯度，是设计、训练和理解神经网络的核心能力。下节中，将介绍如何使用链式法则来高效地计算梯度，也就是通常所说的<strong>反向传播（backpropagation）机制</strong>。该机制能够对包含卷积神经网络在内的几乎所有类型的神经网络的损失函数进行高效的最优化。</p>
<p><strong>最优化笔记全文翻译完</strong>。</p>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/optimization-1/" target="_blank" rel="noopener"><strong>Optimization Note</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/li-yi-ying-73" target="_blank" rel="noopener">李艺颖</a>和<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改</p>
<p>知乎地址：（上，下）</p>
<p><a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Neural_Network3/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Neural_Network3/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：神经网络笔记3</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-03-26 08:53:42" itemprop="dateModified" datetime="2018-03-26T08:53:42+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：神经网络笔记3"><a href="#CS231n课程笔记翻译：神经网络笔记3" class="headerlink" title="CS231n课程笔记翻译：神经网络笔记3"></a>CS231n课程笔记翻译：神经网络笔记3</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li>梯度检查</li>
<li>合理性（Sanity）检查</li>
<li>检查学习过程<ul>
<li>损失函数</li>
<li>训练集与验证集准确率</li>
<li>权重：更新比例</li>
<li>每层的激活数据与梯度分布</li>
<li>可视化 </li>
</ul>
</li>
<li>参数更新<ul>
<li>一阶（随机梯度下降）方法，动量方法，Nesterov动量方法</li>
<li>学习率退火</li>
<li>二阶方法</li>
<li>逐参数适应学习率方法（Adagrad，RMSProp）</li>
</ul>
</li>
<li>超参数调优</li>
<li>评价<ul>
<li>模型集成</li>
</ul>
</li>
<li>总结</li>
<li>拓展引用</li>
</ul>
<h2 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h2><p>在前面章节中，我们讨论了神经网络的静态部分：如何创建网络的连接、数据和损失函数。本节将致力于讲解神经网络的动态部分，即神经网络学习参数和搜索最优超参数的过程。</p>
<h2 id="梯度检查"><a href="#梯度检查" class="headerlink" title="梯度检查"></a>梯度检查</h2><p>理论上将进行梯度检查很简单，就是简单地把解析梯度和数值计算梯度进行比较。然而从实际操作层面上来说，这个过程更加复杂且容易出错。下面是一些提示、技巧和需要仔细注意的事情：</p>
<p><strong>使用中心化公式。</strong>在使用有限差值近似来计算数值梯度的时候，常见的公式是：</p>
<center>$$\frac{df(x)}{dx} = \frac{f(x + h) - f(x)}{h} \hspace{0.1in} \text{(bad, do not use)}$$</center>

<p>其中$h$是一个很小的数字，在实践中近似为1e-5。在实践中证明，使用<em>中心化</em>公式效果更好：</p>
<center>$$\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h} \hspace{0.1in} \text{(use instead)}$$</center>

<p>该公式在检查梯度的每个维度的时候，会要求计算两次损失函数（所以计算资源的耗费也是两倍），但是梯度的近似值会准确很多。要理解这一点，对$f(x+h)$和$f(x-h)$使用泰勒展开，可以看到第一个公式的误差近似$O(h)$，第二个公式的误差近似$O(h^2)$（是个二阶近似）。<strong>*（译者注：泰勒展开相关内容可阅读《高等数学》第十二章第四节：函数展开成幂级数。）*</strong></p>
<p><strong>使用相对误差来比较</strong>。比较数值梯度$f’_n$和解析梯度$f’_a$的细节有哪些？如何得知此两者不匹配？你可能会倾向于监测它们的差的绝对值$|f’_a-f’_n|$或者差的平方值，然后定义该值如果超过某个规定阈值，就判断梯度实现失败。然而该思路是有问题的。想想，假设这个差值是1e-4，如果两个梯度值在1.0左右，这个差值看起来就很合适，可以认为两个梯度是匹配的。然而如果梯度值是1e-5或者更低，那么1e-4就是非常大的差距，梯度实现肯定就是失败的了。因此，使用<em>相对误差</em>总是更合适一些：</p>
<center>$$\frac{\mid f’_a - f’_n \mid}{\max(\mid f’_a \mid, \mid f’_n \mid)}$$</center>

<p>上式考虑了差值占两个梯度绝对值的比例。注意通常相对误差公式只包含两个式子中的一个（任意一个均可），但是我更倾向取两个式子的最大值或者取两个式子的和。这样做是为了防止在其中一个式子为0时，公式分母为0（这种情况，在ReLU中是经常发生的）。然而，还必须注意两个式子都为零且通过梯度检查的情况。在实践中：</p>
<ul>
<li>相对误差&gt;1e-2：通常就意味着梯度可能出错。</li>
<li>1e-2&gt;相对误差&gt;1e-4：要对这个值感到不舒服才行。</li>
<li>1e-4&gt;相对误差：这个值的相对误差对于有不可导点的目标函数是OK的。但如果目标函数中没有kink（使用tanh和softmax），那么相对误差值还是太高。</li>
<li>1e-7或者更小：好结果，可以高兴一把了。</li>
</ul>
<p>要知道的是网络的深度越深，相对误差就越高。所以如果你是在对一个10层网络的输入数据做梯度检查，那么1e-2的相对误差值可能就OK了，因为误差一直在累积。相反，如果一个可微函数的相对误差值是1e-2，那么通常说明梯度实现不正确。</p>
<p><strong>使用双精度。</strong>一个常见的错误是使用单精度浮点数来进行梯度检查。这样会导致即使梯度实现正确，相对误差值也会很高（比如1e-2）。在我的经验而言，出现过使用单精度浮点数时相对误差为1e-2，换成双精度浮点数时就降低为1e-8的情况。</p>
<p><strong>保持在浮点数的有效范围。</strong>建议通读《<a href="http://link.zhihu.com/?target=http%3A//docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html" target="_blank" rel="noopener"><strong>What Every Computer Scientist Should Konw About Floating-Point Artthmetic</strong></a>》一文，该文将阐明你可能犯的错误，促使你写下更加细心的代码。例如，在神经网络中，在一个批量的数据上对损失函数进行归一化是很常见的。但是，如果每个数据点的梯度很小，然后又用数据点的数量去除，就使得数值更小，这反过来会导致更多的数值问题。这就是我为什么总是会把原始的解析梯度和数值梯度数据打印出来，确保用来比较的数字的值不是过小（通常绝对值小于1e-10就绝对让人担心）。如果确实过小，可以使用一个常数暂时将损失函数的数值范围扩展到一个更“好”的范围，在这个范围中浮点数变得更加致密。比较理想的是1.0的数量级上，即当浮点数指数为0时。</p>
<p><strong>目标函数的不可导点（kinks）</strong>。在进行梯度检查时，一个导致不准确的原因是不可导点问题。不可导点是指目标函数不可导的部分，由ReLU（$max(0,x)$）等函数，或SVM损失，Maxout神经元等引入。考虑当$x=-1e6$的时，对ReLU函数进行梯度检查。因为$x&lt;0$，所以解析梯度在该点的梯度为0。然而，在这里数值梯度会突然计算出一个非零的梯度值，因为$f(x+h)$可能越过了不可导点(例如：如果$h&gt;1e-6$)，导致了一个非零的结果。你可能会认为这是一个极端的案例，但实际上这种情况很常见。例如，一个用CIFAR-10训练的SVM中，因为有50,000个样本，且根据目标函数每个样本产生9个式子，所以包含有450,000个$max(0,x)$式子。而一个用SVM进行分类的神经网络因为采用了ReLU，还会有更多的不可导点。</p>
<p>注意，在计算损失的过程中是可以知道不可导点有没有被越过的。在具有$max(x,y)$形式的函数中持续跟踪所有“赢家”的身份，就可以实现这一点。其实就是看在前向传播时，到底x和y谁更大。如果在计算$f(x+h)$和$f(x-h)$的时候，至少有一个“赢家”的身份变了，那就说明不可导点被越过了，数值梯度会不准确。</p>
<p><strong>使用少量数据点。</strong>解决上面的不可导点问题的一个办法是使用更少的数据点。因为含有不可导点的损失函数(例如：因为使用了ReLU或者边缘损失等函数)的数据点越少，不可导点就越少，所以在计算有限差值近似时越过不可导点的几率就越小。还有，如果你的梯度检查对2-3个数据点都有效，那么基本上对整个批量数据进行梯度检查也是没问题的。所以使用很少量的数据点，能让梯度检查更迅速高效。</p>
<p><strong>谨慎设置步长h。</strong>在实践中h并不是越小越好，因为当$h$特别小的时候，就可能就会遇到数值精度问题。有时候如果梯度检查无法进行，可以试试将$h$调到1e-4或者1e-6，然后突然梯度检查可能就恢复正常。这篇<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Numerical_differentiation" target="_blank" rel="noopener"><strong>维基百科文章</strong></a>中有一个图表，其x轴为$h$值，y轴为数值梯度误差。</p>
<p><strong>在操作的特性模式中梯度检查。</strong>有一点必须要认识到：梯度检查是在参数空间中的一个特定（往往还是随机的）的单独点进行的。即使是在该点上梯度检查成功了，也不能马上确保全局上梯度的实现都是正确的。还有，一个随机的初始化可能不是参数空间最优代表性的点，这可能导致进入某种病态的情况，即梯度看起来是正确实现了，实际上并没有。例如，SVM使用小数值权重初始化，就会把一些接近于0的得分分配给所有的数据点，而梯度将会在所有的数据点上展现出某种模式。一个不正确实现的梯度也许依然能够产生出这种模式，但是不能泛化到更具代表性的操作模式，比如在一些的得分比另一些得分更大的情况下就不行。因此为了安全起见，最好让网络学习（“预热”）一小段时间，等到损失函数开始下降的之后再进行梯度检查。在第一次迭代就进行梯度检查的危险就在于，此时可能正处在不正常的边界情况，从而掩盖了梯度没有正确实现的事实。</p>
<p><strong>不要让正则化吞没数据。</strong>通常损失函数是数据损失和正则化损失的和（例如L2对权重的惩罚）。需要注意的危险是正则化损失可能吞没掉数据损失，在这种情况下梯度主要来源于正则化部分（正则化部分的梯度表达式通常简单很多）。这样就会掩盖掉数据损失梯度的不正确实现。因此，推荐先关掉正则化对数据损失做单独检查，然后对正则化做单独检查。对于正则化的单独检查可以是修改代码，去掉其中数据损失的部分，也可以提高正则化强度，确认其效果在梯度检查中是无法忽略的，这样不正确的实现就会被观察到了。</p>
<p><strong>记得关闭随机失活（dropout）和数据扩张（augmentation）</strong>。在进行梯度检查时，记得关闭网络中任何不确定的效果的操作，比如随机失活，随机数据扩展等。不然它们会在计算数值梯度的时候导致巨大误差。关闭这些操作不好的一点是无法对它们进行梯度检查（例如随机失活的反向传播实现可能有错误）。因此，一个更好的解决方案就是在计算$f(x+h)$和$f(x-h)$前强制增加一个特定的随机种子，在计算解析梯度时也同样如此。</p>
<p><strong>检查少量的维度。</strong>在实际中，梯度可以有上百万的参数，在这种情况下只能检查其中一些维度然后假设其他维度是正确的。<strong>注意：</strong>确认在所有不同的参数中都抽取一部分来梯度检查。在某些应用中，为了方便，人们将所有的参数放到一个巨大的参数向量中。在这种情况下，例如偏置就可能只占用整个向量中的很小一部分，所以不要随机地从向量中取维度，一定要把这种情况考虑到，确保所有参数都收到了正确的梯度。</p>
<h2 id="学习之前：合理性检查的提示与技巧"><a href="#学习之前：合理性检查的提示与技巧" class="headerlink" title="学习之前：合理性检查的提示与技巧"></a>学习之前：合理性检查的提示与技巧</h2><p>在进行费时费力的最优化之前，最好进行一些合理性检查：</p>
<ul>
<li><strong>寻找特定情况的正确损失值。</strong>在使用小参数进行初始化时，确保得到的损失值与期望一致。最好先单独检查数据损失（让正则化强度为0）。例如，对于一个跑CIFAR-10的Softmax分类器，一般期望它的初始损失值是2.302，这是因为初始时预计每个类别的概率是0.1（因为有10个类别），然后Softmax损失值正确分类的负对数概率：-ln(0.1)=2.302。对于Weston Watkins SVM，假设所有的边界都被越过（因为所有的分值都近似为零），所以损失值是9（因为对于每个错误分类，边界值是1）。如果没看到这些损失值，那么初始化中就可能有问题。</li>
<li>第二个合理性检查：提高正则化强度时导致损失值变大。</li>
<li><strong>对小数据子集过拟合。</strong>最后也是最重要的一步，在整个数据集进行训练之前，尝试在一个很小的数据集上进行训练（比如20个数据），然后确保能到达0的损失值。进行这个实验的时候，最好让正则化强度为0，不然它会阻止得到0的损失。除非能通过这一个正常性检查，不然进行整个数据集训练是没有意义的。但是注意，能对小数据集进行过拟合并不代表万事大吉，依然有可能存在不正确的实现。比如，因为某些错误，数据点的特征是随机的，这样算法也可能对小数据进行过拟合，但是在整个数据集上跑算法的时候，就没有任何泛化能力。</li>
</ul>
<h2 id="检查整个学习过程"><a href="#检查整个学习过程" class="headerlink" title="检查整个学习过程"></a>检查整个学习过程</h2><p>在训练神经网络的时候，应该跟踪多个重要数值。这些数值输出的图表是观察训练进程的一扇窗口，是直观理解不同的超参数设置效果的工具，从而知道如何修改超参数以获得更高效的学习过程。</p>
<p>在下面的图表中，x轴通常都是表示<strong>周期（epochs）</strong>单位，该单位衡量了在训练中每个样本数据都被观察过次数的期望（一个周期意味着每个样本数据都被观察过了一次）。相较于迭代次数（iterations），一般更倾向跟踪周期，这是因为迭代次数与数据的批尺寸（batchsize）有关，而批尺寸的设置又可以是任意的。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>训练期间第一个要跟踪的数值就是损失值，它在前向传播时对每个独立的批数据进行计算。下图展示的是随着损失值随时间的变化，尤其是曲线形状会给出关于学习率设置的情况：</p>
<p>————————————————————————————————————————</p>
<p><table><tr></tr></table></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_14.jpg?raw=true" width="250"></center></td></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_15.jpg?raw=true" width="250"></center></td><br><br><strong>左图</strong>展示了不同的学习率的效果。过低的学习率导致算法的改善是线性的。高一些的学习率会看起来呈几何指数下降，更高的学习率会让损失值很快下降，但是接着就停在一个不好的损失值上（绿线）。这是因为最优化的“能量”太大，参数在混沌中随机震荡，不能最优化到一个很好的点上。<strong>右图</strong>显示了一个典型的随时间变化的损失函数值，在CIFAR-10数据集上面训练了一个小的网络，这个损失函数值曲线看起来比较合理（虽然可能学习率有点小，但是很难说），而且指出了批数据的数量可能有点太小（因为损失值的噪音很大）。</p>
<p>————————————————————————————————————————</p>
<p>损失值的震荡程度和批尺寸（batch size）有关，当批尺寸为1，震荡会相对较大。当批尺寸就是整个数据集时震荡就会最小，因为每个梯度更新都是单调地优化损失函数（除非学习率设置得过高）。</p>
<p>有的研究者喜欢用对数域对损失函数值作图。因为学习过程一般都是采用指数型的形状，图表就会看起来更像是能够直观理解的直线，而不是呈曲棍球一样的曲线状。还有，如果多个交叉验证模型在一个图上同时输出图像，它们之间的差异就会比较明显。</p>
<p>有时候损失函数看起来很有意思：<a href="http://link.zhihu.com/?target=http%3A//lossfunctions.tumblr.com" target="_blank" rel="noopener"><strong>lossfunctions.tumblr.com</strong></a>。</p>
<h3 id="训练集和验证集准确率"><a href="#训练集和验证集准确率" class="headerlink" title="训练集和验证集准确率"></a>训练集和验证集准确率</h3><p>在训练分类器的时候，需要跟踪的第二重要的数值是验证集和训练集的准确率。这个图表能够展现知道模型过拟合的程度：</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_16.jpg?raw=true" width="300"></center>

<p>在训练集准确率和验证集准确率中间的空隙指明了模型过拟合的程度。在图中，蓝色的验证集曲线显示相较于训练集，验证集的准确率低了很多，这就说明模型有很强的过拟合。遇到这种情况，就应该增大正则化强度（更强的L2权重惩罚，更多的随机失活等）或收集更多的数据。另一种可能就是验证集曲线和训练集曲线如影随形，这种情况说明你的模型容量还不够大：应该通过增加参数数量让模型容量更大些。</p>
<p>————————————————————————————————————————</p>
<h3 id="权重更新比例"><a href="#权重更新比例" class="headerlink" title="权重更新比例"></a>权重更新比例</h3><p>最后一个应该跟踪的量是权重中更新值的数量和全部值的数量之间的比例。注意：是<em>更新的</em>，而不是原始梯度（比如，在普通sgd中就是梯度乘以学习率）。需要对每个参数集的更新比例进行单独的计算和跟踪。一个经验性的结论是这个比例应该在1e-3左右。如果更低，说明学习率可能太小，如果更高，说明学习率可能太高。下面是具体例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设参数向量为W，其梯度向量为dW</span></span><br><span class="line">param_scale = np.linalg.norm(W.ravel())</span><br><span class="line">update = -learning_rate*dW <span class="comment"># 简单SGD更新</span></span><br><span class="line">update_scale = np.linalg.norm(update.ravel())</span><br><span class="line">W += update <span class="comment"># 实际更新</span></span><br><span class="line"><span class="keyword">print</span> update_scale / param_scale <span class="comment"># 要得到1e-3左右</span></span><br></pre></td></tr></table></figure>
<p>相较于跟踪最大和最小值，有研究者更喜欢计算和跟踪梯度的范式及其更新。这些矩阵通常是相关的，也能得到近似的结果。</p>
<h3 id="每层的激活数据及梯度分布"><a href="#每层的激活数据及梯度分布" class="headerlink" title="每层的激活数据及梯度分布"></a>每层的激活数据及梯度分布</h3><p>一个不正确的初始化可能让学习过程变慢，甚至彻底停止。还好，这个问题可以比较简单地诊断出来。其中一个方法是输出网络中所有层的激活数据和梯度分布的柱状图。直观地说，就是如果看到任何奇怪的分布情况，那都不是好兆头。比如，对于使用tanh的神经元，我们应该看到激活数据的值在整个[-1,1]区间中都有分布。如果看到神经元的输出全部是0，或者全都饱和了往-1和1上跑，那肯定就是有问题了。</p>
<h3 id="第一层可视化"><a href="#第一层可视化" class="headerlink" title="第一层可视化"></a>第一层可视化</h3><p>最后，如果数据是图像像素数据，那么把第一层特征可视化会有帮助：</p>
<p>————————————————————————————————————————</p>
<p><table><tr></tr></table></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_17.jpg?raw=true" width="250"></center></td></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_18.jpg?raw=true" width="250"></center></td><br><br>将神经网络第一层的权重可视化的例子。<strong>左图</strong>中的特征充满了噪音，这暗示了网络可能出现了问题：网络没有收敛，学习率设置不恰当，正则化惩罚的权重过低。<strong>右图</strong>的特征不错，平滑，干净而且种类繁多，说明训练过程进行良好。</p>
<h2 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h2><p>一旦能使用反向传播计算解析梯度，梯度就能被用来进行参数更新了。进行参数更新有好几种方法，接下来都会进行讨论。</p>
<p>深度网络的最优化是现在非常活跃的研究领域。本节将重点介绍一些公认有效的常用的技巧，这些技巧都是在实践中会遇到的。我们将简要介绍这些技巧的直观概念，但不进行细节分析。对于细节感兴趣的读者，我们提供了一些拓展阅读。</p>
<h3 id="随机梯度下降及各种更新方法"><a href="#随机梯度下降及各种更新方法" class="headerlink" title="随机梯度下降及各种更新方法"></a>随机梯度下降及各种更新方法</h3><p><strong>普通更新</strong>。最简单的更新形式是沿着负梯度方向改变参数（因为梯度指向的是上升方向，但是我们通常希望最小化损失函数）。假设有一个参数向量<strong>x</strong>及其梯度<strong>dx</strong>，那么最简单的更新的形式是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通更新</span></span><br><span class="line">x += - learning_rate * dx</span><br></pre></td></tr></table></figure>
<p>其中learning_rate是一个超参数，它是一个固定的常量。当在整个数据集上进行计算时，只要学习率足够低，总是能在损失函数上得到非负的进展。</p>
<p><strong>动量（Momentum）更新</strong>是另一个方法，这个方法在深度网络上几乎总能得到更好的收敛速度。该方法可以看成是从物理角度上对于最优化问题得到的启发。损失值可以理解为是山的高度（因此高度势能是$U=mgh$，所以有$U\propto h$）。用随机数字初始化参数等同于在某个位置给质点设定初始速度为0。这样最优化过程可以看做是模拟参数向量（即质点）在地形上滚动的过程。</p>
<p>因为作用于质点的力与梯度的潜在能量（$F=-\nabla U$）有关，质点<strong>所受的力</strong>就是损失函数的<strong>（负）梯度</strong>。还有，因为$F=ma$，所以在这个观点下（负）梯度与质点的加速度是成比例的。注意这个理解和上面的随机梯度下降（SDG）是不同的，在普通版本中，梯度直接影响位置。而在这个版本的更新中，物理观点建议梯度只是影响速度，然后速度再影响位置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 动量更新</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 与速度融合</span></span><br><span class="line">x += v <span class="comment"># 与位置融合</span></span><br></pre></td></tr></table></figure>
<p>在这里引入了一个初始化为0的变量<strong>v</strong>和一个超参数<strong>mu</strong>。说得不恰当一点，这个变量（mu）在最优化的过程中被看做<em>动量</em>（一般值设为0.9），但其物理意义与摩擦系数更一致。这个变量有效地抑制了速度，降低了系统的动能，不然质点在山底永远不会停下来。通过交叉验证，这个参数通常设为[0.5,0.9,0.95,0.99]中的一个。和学习率随着时间退火（下文有讨论）类似，动量随时间变化的设置有时能略微改善最优化的效果，其中动量在学习过程的后阶段会上升。一个典型的设置是刚开始将动量设为0.5而在后面的多个周期（epoch）中慢慢提升到0.99。</p>
<blockquote>
<p>通过动量更新，参数向量会在任何有持续梯度的方向上增加速度。</p>
</blockquote>
<p><strong>Nesterov动量</strong>与普通动量有些许不同，最近变得比较流行。在理论上对于凸函数它能得到更好的收敛，在实践中也确实比标准动量表现更好一些。</p>
<p>Nesterov动量的核心思路是，当参数向量位于某个位置<strong>x</strong>时，观察上面的动量更新公式可以发现，动量部分（忽视带梯度的第二个部分）会通过<strong>mu * v</strong>稍微改变参数向量。因此，如果要计算梯度，那么可以将未来的近似位置<strong>x + mu * v</strong>看做是“向前看”，这个点在我们一会儿要停止的位置附近。因此，计算<strong>x + mu * v</strong>的梯度而不是“旧”位置<strong>x</strong>的梯度就有意义了。</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_19.jpg?raw=true" width="350"></center><br>Nesterov动量。既然我们知道动量将会把我们带到绿色箭头指向的点，我们就不要在原点（红色点）那里计算梯度了。使用Nesterov动量，我们就在这个“向前看”的地方计算梯度。<br><br>————————————————————————————————————————<br><br>也就是说，添加一些注释后，实现代码如下：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_ahead = x + mu * v</span><br><span class="line"><span class="comment"># 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)</span></span><br><span class="line">v = mu * v - learning_rate * dx_ahead</span><br><span class="line">x += v</span><br></pre></td></tr></table></figure><br><br>然而在实践中，人们更喜欢和普通SGD或上面的动量方法一样简单的表达式。通过对<strong>x_ahead = x + mu * v</strong>使用变量变换进行改写是可以做到的，然后用<strong>x_ahead</strong>而不是<strong>x</strong>来表示上面的更新。也就是说，实际存储的参数向量总是向前一步的那个版本。<strong>x_ahead</strong>的公式（将其重新命名为<strong>x</strong>）就变成了：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v <span class="comment"># 存储备份</span></span><br><span class="line">v = mu * v - learning_rate * dx <span class="comment"># 速度更新保持不变</span></span><br><span class="line">x += -mu * v_prev + (<span class="number">1</span> + mu) * v <span class="comment"># 位置更新变了形式</span></span><br></pre></td></tr></table></figure><br><br>对于NAG（Nesterov’s Accelerated Momentum）的来源和数学公式推导，我们推荐以下的拓展阅读：<br><br>- Yoshua Bengio的<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1212.0901v2.pdf" target="_blank" rel="noopener"><strong>Advances in optimizing Recurrent Networks</strong></a>，Section 3.5。<br>- <a href="http://link.zhihu.com/?target=http%3A//www.cs.utoronto.ca/%257Eilya/pubs/ilya_sutskever_phd_thesis.pdf" target="_blank" rel="noopener"><strong>Ilya Sutskever’s thesis</strong></a> (pdf)在section 7.2对于这个主题有更详尽的阐述。<br><br>### 学习率退火<br><br>在训练深度网络的时候，让学习率随着时间退火通常是有帮助的。可以这样理解：如果学习率很高，系统的动能就过大，参数向量就会无规律地跳动，不能够稳定到损失函数更深更窄的部分去。知道什么时候开始衰减学习率是有技巧的：慢慢减小它，可能在很长时间内只能是浪费计算资源地看着它混沌地跳动，实际进展很少。但如果快速地减少它，系统可能过快地失去能量，不能到达原本可以到达的最好位置。通常，实现学习率退火有3种方式：<br><br>- <strong>随步数衰减</strong>：每进行几个周期就根据一些因素降低学习率。典型的值是每过5个周期就将学习率减少一半，或者每20个周期减少到之前的0.1。这些数值的设定是严重依赖具体问题和模型的选择的。在实践中可能看见这么一种经验做法：使用一个固定的学习率来进行训练的同时观察验证集错误率，每当验证集错误率停止下降，就乘以一个常数（比如0.5）来降低学习率。<br>- <strong>指数衰减</strong>。数学公式是$\alpha=\alpha_0e^{-kt}$，其中$\alpha_0,k$是超参数，$t$是迭代次数（也可以使用周期作为单位）。<br>- <strong>1/t衰减</strong>的数学公式是$\alpha=\alpha_0/(1+kt)$，其中$\alpha_0,k$是超参数，t是迭代次数。<br><br>在实践中，我们发现随步数衰减的随机失活（dropout）更受欢迎，因为它使用的超参数（衰减系数和以周期为时间单位的步数）比$k$更有解释性。最后，如果你有足够的计算资源，可以让衰减更加缓慢一些，让训练时间更长些。<br><br>### 二阶方法<br><br>在深度网络背景下，第二类常用的最优化方法是基于<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Newton%2527s_method_in_optimization" target="_blank" rel="noopener"><strong>牛顿法</strong></a>的，其迭代如下：<br><br><center>$$x \leftarrow x - [H f(x)]^{-1} \nabla f(x)$$</center>

<p>这里$Hf(x)$是<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Hessian_matrix" target="_blank" rel="noopener"><strong>Hessian矩阵</strong></a>，它是函数的二阶偏导数的平方矩阵。$\nabla f(x)$是梯度向量，这和梯度下降中一样。直观理解上，Hessian矩阵描述了损失函数的局部曲率，从而使得可以进行更高效的参数更新。具体来说，就是乘以Hessian转置矩阵可以让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进。需要重点注意的是，在这个公式中是没有学习率这个超参数的，这相较于一阶方法是一个巨大的优势。</p>
<p>然而上述更新方法很难运用到实际的深度学习应用中去，这是因为计算（以及求逆）Hessian矩阵操作非常耗费时间和空间。举例来说，假设一个有一百万个参数的神经网络，其Hessian矩阵大小就是[1,000,000 x 1,000,000]，将占用将近3,725GB的内存。这样，各种各样的<em>拟</em>-牛顿法就被发明出来用于近似转置Hessian矩阵。在这些方法中最流行的是<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Limited-memory_BFGS" target="_blank" rel="noopener"><strong>L-BFGS</strong></a>，该方法使用随时间的梯度中的信息来隐式地近似（也就是说整个矩阵是从来没有被计算的）。</p>
<p>然而，即使解决了存储空间的问题，L-BFGS应用的一个巨大劣势是需要对整个训练集进行计算，而整个训练集一般包含几百万的样本。和小批量随机梯度下降（mini-batch SGD）不同，让L-BFGS在小批量上运行起来是很需要技巧，同时也是研究热点。</p>
<p><strong>实践</strong>。在深度学习和卷积神经网络中，使用L-BFGS之类的二阶方法并不常见。相反，基于（Nesterov的）动量更新的各种随机梯度下降方法更加常用，因为它们更加简单且容易扩展。</p>
<p>参考资料：</p>
<ul>
<li><a href="http://link.zhihu.com/?target=http%3A//research.google.com/archive/large_deep_networks_nips2012.html" target="_blank" rel="noopener"><strong>Large Scale Distributed Deep Networks</strong></a> 一文来自谷歌大脑团队，比较了在大规模数据情况下L-BFGS和SGD算法的表现。</li>
<li><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1311.2115" target="_blank" rel="noopener"><strong>SFO</strong></a>算法想要把SGD和L-BFGS的优势结合起来。</li>
</ul>
<h3 id="逐参数适应学习率方法"><a href="#逐参数适应学习率方法" class="headerlink" title="逐参数适应学习率方法"></a>逐参数适应学习率方法</h3><p>前面讨论的所有方法都是对学习率进行全局地操作，并且对所有的参数都是一样的。学习率调参是很耗费计算资源的过程，所以很多工作投入到发明能够适应性地对学习率调参的方法，甚至是逐个参数适应学习率调参。很多这些方法依然需要其他的超参数设置，但是其观点是这些方法对于更广范围的超参数比原始的学习率方法有更良好的表现。在本小节我们会介绍一些在实践中可能会遇到的常用适应算法：</p>
<p><strong>Adagrad</strong>是一个由<a href="http://link.zhihu.com/?target=http%3A//jmlr.org/papers/v12/duchi11a.html" target="_blank" rel="noopener"><strong>Duchi等</strong></a>提出的适应性学习率算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设有梯度和参数向量x</span></span><br><span class="line">cache += dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure>
<p>注意，变量<strong>cache</strong>的尺寸和梯度矩阵的尺寸是一样的，还跟踪了每个参数的梯度的平方和。这个一会儿将用来归一化参数更新步长，归一化是逐元素进行的。注意，接收到高梯度值的权重更新的效果被减弱，而接收到低梯度值的权重的更新效果将会增强。有趣的是平方根的操作非常重要，如果去掉，算法的表现将会糟糕很多。用于平滑的式子<strong>eps</strong>（一般设为1e-4到1e-8之间）是防止出现除以0的情况。Adagrad的一个缺点是，在深度学习中单调的学习率被证明通常过于激进且过早停止学习。</p>
<p><strong>RMSprop</strong>。是一个非常高效，但没有公开发表的适应性学习率方法。有趣的是，每个使用这个方法的人在他们的论文中都引用自Geoff Hinton的Coursera课程的<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%257Etijmen/csc321/slides/lecture_slides_lec6.pdf" target="_blank" rel="noopener"><strong>第六课的第29页PPT</strong></a>。这个方法用一种很简单的方式修改了Adagrad方法，让它不那么激进，单调地降低了学习率。具体说来，就是它使用了一个梯度平方的滑动平均：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cache =  decay_rate * cache + (<span class="number">1</span> - decay_rate) * dx**<span class="number">2</span></span><br><span class="line">x += - learning_rate * dx / (np.sqrt(cache) + eps)</span><br></pre></td></tr></table></figure>
<p>在上面的代码中，decay_rate是一个超参数，常用的值是[0.9,0.99,0.999]。其中<strong>x+=</strong>和Adagrad中是一样的，但是<strong>cache</strong>变量是不同的。因此，RMSProp仍然是基于梯度的大小来对每个权重的学习率进行修改，这同样效果不错。但是和Adagrad不同，其更新不会让学习率单调变小。</p>
<p><strong>Adam</strong>。<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1412.6980" target="_blank" rel="noopener"><strong>Adam</strong></a>是最近才提出的一种更新方法，它看起来像是RMSProp的动量版。简化的代码是下面这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">m = beta1*m + (<span class="number">1</span>-beta1)*dx</span><br><span class="line">v = beta2*v + (<span class="number">1</span>-beta2)*(dx**<span class="number">2</span>)</span><br><span class="line">x += - learning_rate * m / (np.sqrt(v) + eps)</span><br></pre></td></tr></table></figure>
<p>注意这个更新方法看起来真的和RMSProp很像，除了使用的是平滑版的梯度<strong>m</strong>，而不是用的原始梯度向量<strong>dx</strong>。论文中推荐的参数值<strong>eps=1e-8, beta1=0.9, beta2=0.999</strong>。在实际操作中，我们推荐Adam作为默认的算法，一般而言跑起来比RMSProp要好一点。但是也可以试试SGD+Nesterov动量。完整的Adam更新算法也包含了一个偏置<em>（bias）矫正</em>机制，因为<strong>m,v</strong>两个矩阵初始为0，在没有完全热身之前存在偏差，需要采取一些补偿措施。建议读者可以阅读论文查看细节，或者课程的PPT。</p>
<p>拓展阅读：</p>
<ul>
<li><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.6055" target="_blank" rel="noopener"><strong>Unit Tests for Stochastic Optimization</strong></a>一文展示了对于随机最优化的测试。</li>
</ul>
<p>——————————————————————————————————————————</p>
<p><table><tr></tr></table></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_20.gif?raw=true" width="250"></center></td></p>
<p><td><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_21.gif?raw=true" width="250"></center></td><br><strong>译者注：上图原文中为动图，知乎专栏不支持动图，知友可点击原文链接查看。</strong></p>
<p>上面的动画可以帮助你理解学习的动态过程。<strong>左边</strong>是一个损失函数的等高线图，上面跑的是不同的最优化算法。注意基于动量的方法出现了射偏了的情况，使得最优化过程看起来像是一个球滚下山的样子。<strong>右边</strong>展示了一个马鞍状的最优化地形，其中对于不同维度它的曲率不同（一个维度下降另一个维度上升）。注意SGD很难突破对称性，一直卡在顶部。而RMSProp之类的方法能够看到马鞍方向有很低的梯度。因为在RMSProp更新方法中的分母项，算法提高了在该方向的有效学习率，使得RMSProp能够继续前进。图片版权：<a href="http://link.zhihu.com/?target=https%3A//twitter.com/alecrad" target="_blank" rel="noopener"><strong>Alec Radford</strong></a>。</p>
<p>——————————————————————————————————————————</p>
<h2 id="超参数调优"><a href="#超参数调优" class="headerlink" title="超参数调优"></a>超参数调优</h2><p>我们已经看到，训练一个神经网络会遇到很多超参数设置。神经网络最常用的设置有：</p>
<ul>
<li>初始学习率。</li>
<li>学习率衰减方式（例如一个衰减常量）。</li>
<li>正则化强度（L2惩罚，随机失活强度）。</li>
</ul>
<p>但是也可以看到，还有很多相对不那么敏感的超参数。比如在逐参数适应学习方法中，对于动量及其时间表的设置等。在本节中将介绍一些额外的调参要点和技巧：</p>
<p><strong>实现</strong>。更大的神经网络需要更长的时间去训练，所以调参可能需要几天甚至几周。记住这一点很重要，因为这会影响你设计代码的思路。一个具体的设计是用<strong>仆程序</strong>持续地随机设置参数然后进行最优化。在训练过程中，<strong>仆程序</strong>会对每个周期后验证集的准确率进行监控，然后向文件系统写下一个模型的记录点（记录点中有各种各样的训练统计数据，比如随着时间的损失值变化等），这个文件系统最好是可共享的。在文件名中最好包含验证集的算法表现，这样就能方便地查找和排序了。然后还有一个<strong>主程序</strong>，它可以启动或者结束计算集群中的<strong>仆程序</strong>，有时候也可能根据条件查看<strong>仆程序</strong>写下的记录点，输出它们的训练统计数据等。</p>
<p><strong>比起交叉验证最好使用一个验证集</strong>。在大多数情况下，一个尺寸合理的验证集可以让代码更简单，不需要用几个数据集来交叉验证。你可能会听到人们说他们“交叉验证”一个参数，但是大多数情况下，他们实际是使用的一个验证集。</p>
<p><strong>超参数范围</strong>。在对数尺度上进行超参数搜索。例如，一个典型的学习率应该看起来是这样：<strong>learning_rate = 10 * uniform(-6, 1)</strong>。也就是说，我们从标准分布中随机生成了一个数字，然后让它成为10的阶数。对于正则化强度，可以采用同样的策略。直观地说，这是因为学习率和正则化强度都对于训练的动态进程有乘的效果。例如：当学习率是0.001的时候，如果对其固定地增加0.01，那么对于学习进程会有很大影响。然而当学习率是10的时候，影响就微乎其微了。这就是因为学习率乘以了计算出的梯度。因此，比起加上或者减少某些值，思考学习率的范围是乘以或者除以某些值更加自然。但是有一些参数（比如随机失活）还是在原始尺度上进行搜索（例如：<strong>dropout=uniform(0,1)</strong>）。</p>
<p><strong>随机搜索优于网格搜索</strong>。Bergstra和Bengio在文章<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener"><strong>Random Search for Hyper-Parameter Optimization</strong></a>中说“随机选择比网格化的选择更加有效”，而且在实践中也更容易实现。</p>
<p>——————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_22.jpg?raw=true" width="350"></center><br>在<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" target="_blank" rel="noopener"><strong>Random Search for Hyper-Parameter Optimization</strong></a>中的核心说明图。通常，有些超参数比其余的更重要，通过随机搜索，而不是网格化的搜索，可以让你更精确地发现那些比较重要的超参数的好数值。</p>
<p>——————————————————————————————————————————</p>
<p><strong>对于边界上的最优值要小心</strong>。这种情况一般发生在你在一个不好的范围内搜索超参数（比如学习率）的时候。比如，假设我们使用<strong>learning_rate = 10 \</strong> uniform(-6,1)**来进行搜索。一旦我们得到一个比较好的值，一定要确认你的值不是出于这个范围的边界上，不然你可能错过更好的其他搜索范围。</p>
<p><strong>从粗到细地分阶段搜索</strong>。在实践中，先进行初略范围（比如10 ** [-6, 1]）搜索，然后根据好的结果出现的地方，缩小范围进行搜索。进行粗搜索的时候，让模型训练一个周期就可以了，因为很多超参数的设定会让模型没法学习，或者突然就爆出很大的损失值。第二个阶段就是对一个更小的范围进行搜索，这时可以让模型运行5个周期，而最后一个阶段就在最终的范围内进行仔细搜索，运行很多次周期。</p>
<p><strong>贝叶斯超参数最优化</strong>是一整个研究领域，主要是研究在超参数空间中更高效的导航算法。其核心的思路是在不同超参数设置下查看算法性能时，要在探索和使用中进行合理的权衡。基于这些模型，发展出很多的库，比较有名的有： <a href="http://link.zhihu.com/?target=https%3A//github.com/JasperSnoek/spearmint" target="_blank" rel="noopener"><strong>Spearmint</strong></a>, <a href="http://link.zhihu.com/?target=http%3A//www.cs.ubc.ca/labs/beta/Projects/SMAC/" target="_blank" rel="noopener"><strong>SMAC</strong></a>, 和<a href="http://link.zhihu.com/?target=http%3A//jaberg.github.io/hyperopt/" target="_blank" rel="noopener"><strong>Hyperopt</strong></a>。然而，在卷积神经网络的实际使用中，比起上面介绍的先认真挑选的一个范围，然后在该范围内随机搜索的方法，这个方法还是差一些。<a href="http://link.zhihu.com/?target=http%3A//nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html" target="_blank" rel="noopener"><strong>这里</strong></a>有更详细的讨论。</p>
<h2 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h2><h3 id="模型集成"><a href="#模型集成" class="headerlink" title="模型集成"></a>模型集成</h3><p>在实践的时候，有一个总是能提升神经网络几个百分点准确率的办法，就是在训练的时候训练几个独立的模型，然后在测试的时候平均它们预测结果。集成的模型数量增加，算法的结果也单调提升（但提升效果越来越少）。还有模型之间的差异度越大，提升效果可能越好。进行集成有以下几种方法：</p>
<ul>
<li><strong>同一个模型，不同的初始化</strong>。使用交叉验证来得到最好的超参数，然后用最好的参数来训练不同初始化条件的模型。这种方法的风险在于多样性只来自于不同的初始化条件。</li>
<li><strong>在交叉验证中发现最好的模型</strong>。使用交叉验证来得到最好的超参数，然后取其中最好的几个（比如10个）模型来进行集成。这样就提高了集成的多样性，但风险在于可能会包含不够理想的模型。在实际操作中，这样操作起来比较简单，在交叉验证后就不需要额外的训练了。</li>
<li><strong>一个模型设置多个记录点</strong>。如果训练非常耗时，那就在不同的训练时间对网络留下记录点（比如每个周期结束），然后用它们来进行模型集成。很显然，这样做多样性不足，但是在实践中效果还是不错的，这种方法的优势是代价比较小。</li>
<li><strong>在训练的时候跑参数的平均值</strong>。和上面一点相关的，还有一个也能得到1-2个百分点的提升的小代价方法，这个方法就是在训练过程中，如果损失值相较于前一次权重出现指数下降时，就在内存中对网络的权重进行一个备份。这样你就对前几次循环中的网络状态进行了平均。你会发现这个“平滑”过的版本的权重总是能得到更少的误差。直观的理解就是目标函数是一个碗状的，你的网络在这个周围跳跃，所以对它们平均一下，就更可能跳到中心去。</li>
</ul>
<p>模型集成的一个劣势就是在测试数据的时候会花费更多时间。最近Geoff Hinton在“<a href="http://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DEK61htlw8hY" target="_blank" rel="noopener"><strong>Dark Knowledge</strong></a>”上的工作很有启发：其思路是通过将集成似然估计纳入到修改的目标函数中，从一个好的集成中抽出一个单独模型。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>训练一个神经网络需要：</p>
<ul>
<li>利用小批量数据对实现进行梯度检查，还要注意各种错误。</li>
<li>进行合理性检查，确认初始损失值是合理的，在小数据集上能得到100%的准确率。</li>
<li>在训练时，跟踪损失函数值，训练集和验证集准确率，如果愿意，还可以跟踪更新的参数量相对于总参数量的比例（一般在1e-3左右），然后如果是对于卷积神经网络，可以将第一层的权重可视化。</li>
<li>推荐的两个更新方法是SGD+Nesterov动量方法，或者Adam方法。</li>
<li>随着训练进行学习率衰减。比如，在固定多少个周期后让学习率减半，或者当验证集准确率下降的时候。</li>
<li>使用随机搜索（不要用网格搜索）来搜索最优的超参数。分阶段从粗（比较宽的超参数范围训练1-5个周期）到细（窄范围训练很多个周期）地来搜索。</li>
<li>进行模型集成来获得额外的性能提高。</li>
</ul>
<h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><ul>
<li>Leon Bottou的《<a href="http://link.zhihu.com/?target=http%3A//research.microsoft.com/pubs/192769/tricks-2012.pdf" target="_blank" rel="noopener"><strong>SGD要点和技巧</strong></a>》。</li>
<li>Yann LeCun的《<a href="http://link.zhihu.com/?target=http%3A//yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank" rel="noopener"><strong>Efficient BackProp</strong></a>》。</li>
<li>Yoshua Bengio的《<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1206.5533v2.pdf" target="_blank" rel="noopener"><strong>Practical Recommendations for Gradient-Based Training of Deep Architectures</strong></a>》。</li>
</ul>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-3/" target="_blank" rel="noopener"><strong>Neural Nets notes 3</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>和<a href="https://www.zhihu.com/people/hmonkey" target="_blank" rel="noopener">巩子嘉</a>进行校对修改</p>
<p>知乎地址：<a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Convolution_Network/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/avatar.gif"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Convolution_Network/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：卷积神经网络笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2018-03-02 16:54:34" itemprop="dateModified" datetime="2018-03-02T16:54:34+08:00">2018-03-02</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">in</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：卷积神经网络笔记"><a href="#CS231n课程笔记翻译：卷积神经网络笔记" class="headerlink" title="CS231n课程笔记翻译：卷积神经网络笔记"></a>CS231n课程笔记翻译：卷积神经网络笔记</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li><strong>结构概述</strong></li>
<li><strong>用来构建卷积神经网络的各种层</strong><ul>
<li>卷积层</li>
<li>汇聚层</li>
<li>归一化层</li>
<li>全连接层</li>
<li>将全连接层转化成卷积层</li>
</ul>
</li>
<li><strong>卷积神经网络的结构</strong><ul>
<li>层的排列规律</li>
<li>层的尺寸设置规律</li>
<li>案例学习（LeNet / AlexNet / ZFNet / GoogLeNet / VGGNet）</li>
<li>计算上的考量</li>
</ul>
</li>
<li><strong>拓展资源</strong></li>
</ul>
<h2 id="卷积神经网络（CNNs-ConvNets）"><a href="#卷积神经网络（CNNs-ConvNets）" class="headerlink" title="卷积神经网络（CNNs / ConvNets）"></a><strong>卷积神经网络（CNNs / ConvNets）</strong></h2><p>卷积神经网络和上一章讲的常规神经网络非常相似：它们都是由神经元组成，神经元中有具有学习能力的权重和偏差。每个神经元都得到一些输入数据，进行内积运算后再进行激活函数运算。整个网络依旧是一个可导的评分函数：该函数的输入是原始的图像像素，输出是不同类别的评分。在最后一层（往往是全连接层），网络依旧有一个损失函数（比如SVM或Softmax），并且在神经网络中我们实现的各种技巧和要点依旧适用于卷积神经网络。</p>
<p>那么有哪些地方变化了呢？卷积神经网络的结构基于一个假设，即输入数据是图像，基于该假设，我们就向结构中添加了一些特有的性质。这些特有属性使得前向传播函数实现起来更高效，并且大幅度降低了网络中参数的数量。</p>
<h2 id="结构概述"><a href="#结构概述" class="headerlink" title="结构概述"></a><strong>结构概述</strong></h2><p>回顾：常规神经网络。在上一章中，神经网络的输入是一个向量，然后在一系列的隐层中对它做变换。每个隐层都是由若干的神经元组成，每个神经元都与前一层中的所有神经元连接。但是在一个隐层中，神经元相互独立不进行任何连接。最后的全连接层被称为“输出层”，在分类问题中，它输出的值被看做是不同类别的评分值。</p>
<p>常规神经网络对于大尺寸图像效果不尽人意。在CIFAR-10中，图像的尺寸是<em>32x32x3</em>（宽高均为<em>32</em>像素，<em>3</em>个颜色通道），因此，对应的的常规神经网络的第一个隐层中，每一个单独的全连接神经元就有<em>32x32x3=3072</em>个权重。这个数量看起来还可以接受，但是很显然这个全连接的结构不适用于更大尺寸的图像。举例说来，一个尺寸为<em>200x200x3</em>的图像，会让神经元包含<em>200x200x3=120,000</em>个权重值。而网络中肯定不止一个神经元，那么参数的量就会快速增加！显而易见，这种全连接方式效率低下，大量的参数也很快会导致网络过拟合。</p>
<p>神经元的三维排列。卷积神经网络针对输入全部是图像的情况，将结构调整得更加合理，获得了不小的优势。与常规神经网络不同，卷积神经网络的各层中的神经元是3维排列的：<strong>宽度</strong>、<strong>高度</strong>和<strong>深度</strong>（这里的<strong>深度</strong>指的是激活数据体的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数）。举个例子，CIFAR-10中的图像是作为卷积神经网络的输入，该数据体的维度是<em>32x32x3</em>（宽度，高度和深度）。我们将看到，层中的神经元将只与前一层中的一小块区域连接，而不是采取全连接方式。对于用来分类CIFAR-10中的图像的卷积网络，其最后的输出层的维度是<em>1x1x10</em>，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。下面是例子：</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/convnets/convnets_0.jpg?raw=true" width="400"></center>

<p>左边是一个<em>3</em>层的神经网络。右边是一个卷积神经网络，图例中网络将它的神经元都排列成<em>3</em>个维度（宽、高和深度）。卷积神经网络的每一层都将<em>3D</em>的输入数据变化为神经元<em>3D</em>的激活数据并输出。在这个例子中，红色的输入层装的是图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是<em>3</em>（代表了红、绿、蓝3种颜色通道）。</p>
<blockquote>
<p>卷积神经网络是由层组成的。每一层都有一个简单的API：用一些含或者不含参数的可导的函数，将输入的3D数据变换为3D的输出数据。</p>
</blockquote>
<h3 id="用来构建卷积网络的各种层"><a href="#用来构建卷积网络的各种层" class="headerlink" title="用来构建卷积网络的各种层"></a><strong>用来构建卷积网络的各种层</strong></h3><p>一个简单的卷积神经网络是由各种层按照顺序排列组成，网络中的每个层使用一个可以微分的函数将激活数据从一个层传递到另一个层。卷积神经网络主要由三种类型的层构成：<strong>卷积层</strong>，<strong>汇聚（Pooling）层</strong>和<strong>全连接层</strong>（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。</p>
<p>网络结构例子：这仅仅是个概述，下面会更详解的介绍细节。一个用于CIFAR-10图像数据分类的卷积神经网络的结构可以是[输入层-卷积层-ReLU层-汇聚层-全连接层]。细节如下：</p>
<ul>
<li>输入<em>[32x32x3]</em>存有图像的原始像素值，本例中图像宽高均为32，有3个颜色通道。</li>
<li>卷积层中，神经元与输入层中的一个局部区域相连，每个神经元都计算自己与输入层相连的小区域与自己权重的内积。卷积层会计算所有神经元的输出。如果我们使用12个滤波器（也叫作核），得到的输出数据体的维度就是[32x32x12]。</li>
<li>ReLU层将会逐个元素地进行激活函数操作，比如使用以0为阈值的$max(0,x)$作为激活函数。该层对数据尺寸没有改变，还是[32x32x12]。</li>
<li>汇聚层在在空间维度（宽度和高度）上进行降采样（downsampling）操作，数据尺寸变为[16x16x12]。</li>
<li>全连接层将会计算分类评分，数据尺寸变为[1x1x10]，其中10个数字对应的就是CIFAR-10中10个类别的分类评分值。正如其名，全连接层与常规神经网络一样，其中每个神经元都与前一层中所有神经元相连接。</li>
</ul>
<p>由此看来，卷积神经网络一层一层地将图像从原始像素值变换成最终的分类评分值。其中有的层含有参数，有的没有。具体说来，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数（神经元的突触权值和偏差）。而ReLU层和汇聚层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。</p>
<p><strong>小结</strong>：</p>
<ul>
<li><p>简单案例中卷积神经网络的结构，就是一系列的层将输入数据变换为输出数据（比如分类评分）。</p>
</li>
<li><p>卷积神经网络结构中有几种不同类型的层（目前最流行的有卷积层、全连接层、ReLU层和汇聚层）。</p>
</li>
<li><p>每个层的输入是3D数据，然后使用一个可导的函数将其变换为3D的输出数据。</p>
</li>
<li><p>有的层有参数，有的没有（卷积层和全连接层有，ReLU层和汇聚层没有）。</p>
</li>
<li><p>有的层有额外的超参数，有的没有（卷积层、全连接层和汇聚层有，ReLU层没有）。</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/convnets/convnets_1.jpg?raw=true" width="400"></center>

</li>
</ul>
<p>一个卷积神经网络的激活输出例子。左边的输入层存有原始图像像素，右边的输出层存有类别分类评分。在处理流程中的每个激活数据体是铺成一列来展示的。因为对3D数据作图比较困难，我们就把每个数据体切成层，然后铺成一列显示。最后一层装的是针对不同类别的分类得分，这里只显示了得分最高的5个评分值和对应的类别。完整的</p>
<p><a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">网页演示</a></p>
<p>在我们的课程主页。本例中的结构是一个小的VGG网络，VGG网络后面会有讨论。</p>
<p>现在讲解不同的层，层的超参数和连接情况的细节。</p>
<h4 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h4><p>卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。</p>
<p><strong>概述和直观介绍</strong>：首先讨论的是，在没有大脑和生物意义上的神经元之类的比喻下，卷积层到底在计算什么。卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，但是深度和输入数据一致。举例来说，卷积神经网络第一层的一个典型的滤波器的尺寸可以是5x5x3（宽高都是5像素，深度是3是因为图像应为颜色通道，所以有3的深度）。在前向传播的时候，让每个滤波器都在输入数据的宽度和高度上滑动（更精确地说是卷积），然后计算整个滤波器和输入数据任一处的内积。当滤波器沿着输入数据的宽度和高度滑过后，会生成一个2维的激活图（activation map），激活图给出了在每个空间位置处滤波器的反应。直观地来说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层上某些颜色的斑点，甚至可以是网络更高层上的蜂巢状或者车轮状图案。</p>
<p>在每个卷积层上，我们会有一整个集合的滤波器（比如12个），每个都会生成一个不同的二维激活图。将这些激活映射在深度方向上层叠起来就生成了输出数据。</p>
<p><strong>以大脑做比喻</strong>：如果你喜欢用大脑和生物神经元来做比喻，那么输出的3D数据中的每个数据项可以被看做是神经元的一个输出，而该神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数（因为这些数字都是使用同一个滤波器得到的结果）。现在开始讨论神经元的连接，它们在空间中的排列，以及它们参数共享的模式。</p>
<p><strong>局部连接</strong>：在处理图像这样的高维度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。该连接的空间大小叫做神经元的<strong>感受野（receptive field）</strong>，它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致。</p>
<p>例1：假设输入数据体尺寸为[32x32x3]（比如CIFAR-10的RGB图像），如果感受野（或滤波器尺寸）是5x5，那么卷积层中的每个神经元会有输入数据体中[5x5x3]区域的权重，共5x5x3=75个权重（还要加一个偏差参数）。注意这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。</p>
<p>例2：假设输入数据体的尺寸是[16x16x20]，感受野尺寸是3x3，那么卷积层中每个神经元和输入数据体就有3x3x20=180个连接。再次提示：在空间上连接是局部的（3x3），但是在深度上是和输入数据体一致的（20）。</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/convnets/convnets_2.jpg?raw=true" width="350"></center>

<p><strong>左边</strong>：红色的是输入数据体（比如CIFAR-10中的图像），蓝色的部分是第一个卷积层中的神经元。卷积层中的每个神经元都只是与输入数据体的一个局部在空间上相连，但是与输入数据体的所有深度维度全部相连（所有颜色通道）。在深度方向上有多个神经元（本例中5个），它们都接受输入数据的同一块区域（<strong>感受野</strong>相同）。至于深度列的讨论在下文中有。</p>
<p><strong>右边</strong>：神经网络章节中介绍的神经元保持不变，它们还是计算权重和输入的内积，然后进行激活函数运算，只是它们的连接被限制在一个局部空间。</p>
<p><strong>空间排列</strong>：上文讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：<strong>深度（depth），步长（stride）</strong>和<strong>零填充（zero-padding）</strong>。下面是对它们的讨论：</p>
<ol>
<li>首先，输出数据体的深度是一个超参数：它和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西。举例来说，如果第一个卷积层的输入是原始图像，那么在深度维度上的不同神经元将可能被不同方向的边界，或者是颜色斑点激活。我们将这些沿着深度方向排列、感受野相同的神经元集合称为<strong>深度列（depth column）</strong>，也有人使用纤维（fibre）来称呼它们。</li>
<li>其次，在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素。当步长为2（或者不常用的3，或者更多，这些在实际中很少使用），滤波器滑动时每次移动2个像素。这个操作会让输出数据体在空间上变小。</li>
<li>在下文可以看到，有时候将输入数据体用0在边缘处进行填充是很方便的。这个<strong>零填充（zero-padding）</strong>的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，这样输入和输出的宽高都相等）。</li>
</ol>
<p>输出数据体在空间上的尺寸可以通过输入数据体尺寸（W），卷积层中神经元的感受野尺寸（F），步长（S）和零填充的数量（P）的函数来计算。（<strong>译者注</strong>：这里假设输入数组的空间形状是正方形，即高度和宽度相等）输出数据体的空间尺寸为(W-F +2P)/S+1。比如输入是7x7，滤波器是3x3，步长为1，填充为0，那么就能得到一个5x5的输出。如果步长为2，输出就是3x3。下面是例子：</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/convnets/convnets_3.jpg?raw=true" width="400"></center>

<p>空间排列的图示。在本例中只有一个空间维度（x轴），神经元的感受野尺寸F=3，输入尺寸W=5，零填充P=1。左边：神经元使用的步长S=1，所以输出尺寸是(5-3+2)/1+1=5。右边：神经元的步长S=2，则输出尺寸是(5-3+2)/2+1=3。注意当步长S=3时是无法使用的，因为它无法整齐地穿过数据体。从等式上来说，因为(5-3+2)=4是不能被3整除的。</p>
<p>本例中，神经元的权重是[1,0,-1]，显示在图的右上角，偏差值为0。这些权重是被所有黄色的神经元共享的（参数共享的内容看下文相关内容）。</p>
<p>使用零填充：在上面左边例子中，注意输入维度是5，输出维度也是5。之所以如此，是因为感受野是3并且使用了1的零填充。如果不使用零填充，则输出数据体的空间维度就只有3，因为这就是滤波器整齐滑过并覆盖原始数据需要的数目。一般说来，当步长$S=1$时，零填充的值是$P=(F-1)/2$，这样就能保证输入和输出数据体有相同的空间尺寸。这样做非常常见，在介绍卷积神经网络的结构的时候我们会详细讨论其原因。</p>
<p><em>步长的限制</em>：注意这些空间排列的超参数之间是相互限制的。举例说来，当输入尺寸$W=10$，不使用零填充则$P=0$，滤波器尺寸$F=3$，这样步长$S=2$就行不通，因为$(W-F+2P)/S+1=(10-3+0)/2+1=4.5$，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体。因此，这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，或者修改零填充值来让设置合理，或者修改输入数据体尺寸来让设置合理，或者其他什么措施。在后面的卷积神经网络结构小节中，读者可以看到合理地设置网络的尺寸让所有的维度都能正常工作，这件事可是相当让人头痛的。而使用零填充和遵守其他一些设计策略将会有效解决这个问题。</p>
<p><em>真实案例</em>：<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Krizhevsky</a>构架赢得了2012年的ImageNet挑战，其输入图像的尺寸是[227x227x3]。在第一个卷积层，神经元使用的感受野尺寸$F=11$，步长$S=4$，不使用零填充$P=0$。因为(227-11)/4+1=55，卷积层的深度$K=96$，则卷积层的输出数据体尺寸为[55x55x96]。55x55x96个神经元中，每个都和输入数据体中一个尺寸为[11x11x3]的区域全连接。在深度列上的96个神经元都是与输入数据体中同一个[11x11x3]区域连接，但是权重不同。有一个有趣的细节，在原论文中，说的输入图像尺寸是224x224，这是肯定错误的，因为(224-11)/4+1的结果不是整数。这件事在卷积神经网络的历史上让很多人迷惑，而这个错误到底是怎么发生的没人知道。我的猜测是Alex忘记在论文中指出自己使用了尺寸为3的额外的零填充。</p>
<p><strong>参数共享</strong>：在卷积层中使用参数共享是用来控制参数的数量。就用上面的例子，在第一个卷积层就有55x55x96=290,400个神经元，每个有11x11x3=364个参数和1个偏差。将这些合起来就是290400x364=105,705,600个参数。单单第一层就有这么多参数，显然这个数目是非常大的。</p>
<p>作一个合理的假设：如果一个特征在计算某个空间位置(x,y)的时候有用，那么它在计算另一个不同位置(x2,y2)的时候也有用。基于这个假设，可以显著地减少参数数量。换言之，就是将深度维度上一个单独的2维切片看做<strong>深度切片（depth slice）</strong>，比如一个数据体尺寸为[55x55x96]的就有96个深度切片，每个尺寸为[55x55]。在每个深度切片上的神经元都使用同样的权重和偏差。在这样的参数共享下，例子中的第一个卷积层就只有96个不同的权重集了，一个权重集对应一个深度切片，共有96x11x11x3=34,848个不同的权重，或34,944个参数（+96个偏差）。在每个深度切片中的55x55个权重使用的都是同样的参数。在反向传播的时候，都要计算每个神经元对它的权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。</p>
<p>注意，如果在一个深度切片中的所有权重都使用同一个权重向量，那么卷积层的前向传播在每个深度切片中可以看做是在计算神经元权重和输入数据体的<strong>卷积</strong>（这就是“卷积层”名字由来）。这也是为什么总是将这些权重集合称为<strong>滤波器（filter）</strong>（或<strong>卷积核（kernel）</strong>），因为它们和输入进行了卷积。</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/convnets/convnets_4.jpg?raw=true" width="400"></center>

<p>Krizhevsky等学习到的滤波器例子。这96个滤波器的尺寸都是[11x11x3]，在一个深度切片中，每个滤波器都被55x55个神经元共享。注意参数共享的假设是有道理的：如果在图像某些地方探测到一个水平的边界是很重要的，那么在其他一些地方也会同样是有用的，这是因为图像结构具有平移不变性。所以在卷积层的输出数据体的55x55个不同位置中，就没有必要重新学习去探测一个水平边界了。</p>
<p>注意有时候参数共享假设可能没有意义，特别是当卷积神经网络的输入图像是一些明确的中心结构时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征。一个具体的例子就是输入图像是人脸，人脸一般都处于图片中心。你可能期望不同的特征，比如眼睛特征或者头发特征可能（也应该）会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为<strong>局部连接层</strong>（Locally-Connected Layer）。</p>
<p><strong>Numpy例子</strong>：为了让讨论更加的具体，我们用代码来展示上述思路。假设输入数据体是numpy数组<strong>X</strong>。那么：</p>
<ul>
<li>一个位于<strong>(x,y)</strong>的深度列（或纤维）将会是<strong>X[x,y,:]</strong>。</li>
<li>在深度为<strong>d</strong>处的深度切片，或激活图应该是<strong>X[:,:,d]</strong>。</li>
</ul>
<p>卷积层例子：假设输入数据体<strong>X</strong>的尺寸<strong>X.shape:(11,11,4)</strong>，不使用零填充（$P=0$），滤波器的尺寸是$F=5$</p>
<p>，步长$S=2$。那么输出数据体的空间尺寸就是(11-5)/2+1=4，即输出数据体的宽度和高度都是4。那么在输出数据体中的激活映射（称其为<strong>V</strong>）看起来就是下面这样（在这个例子中，只有部分元素被计算）：</p>
<ul>
<li><strong>V[0,0,0] = np.sum(X[:5,:5,:] * W0) + b0</strong></li>
<li><strong>V[1,0,0] = np.sum(X[2:7,:5,:] * W0) + b0</strong></li>
<li><strong>V[2,0,0] = np.sum(X[4:9,:5,:] * W0) + b0</strong></li>
<li><strong>V[3,0,0] = np.sum(X[6:11,:5,:] * W0) + b0</strong></li>
</ul>
<p>在numpy中，<strong>*</strong>操作是进行数组间的逐元素相乘。权重向量<strong>W0</strong>是该神经元的权重，<strong>b0</strong>是其偏差。在这里，<strong>W0</strong>被假设尺寸是<strong>W0.shape: (5,5,4)</strong>，因为滤波器的宽高是5，输入数据量的深度是4。注意在每一个点，计算点积的方式和之前的常规神经网络是一样的。同时，计算内积的时候使用的是同一个权重和偏差（因为参数共享），在宽度方向的数字每次上升2（因为步长为2）。要构建输出数据体中的第二张激活图，代码应该是：</p>
<ul>
<li><p><strong>V[0,0,1] = np.sum(X[:5,:5,:] * W1) + b1</strong></p>
</li>
<li><p><strong>V[1,0,1] = np.sum(X[2:7,:5,:] * W1) + b1</strong></p>
</li>
<li><p><strong>V[2,0,1] = np.sum(X[4:9,:5,:] * W1) + b1</strong></p>
</li>
<li><p><strong>V[3,0,1] = np.sum(X[6:11,:5,:] * W1) + b1</strong></p>
</li>
<li><p><strong>V[0,1,1] = np.sum(X[:5,2:7,:] * W1) + b1</strong></p>
<p>（在y方向上）</p>
</li>
<li><p><strong>V[2,3,1] = np.sum(X[4:9,6:11,:] * W1) + b1</strong></p>
<p>（或两个方向上同时）</p>
</li>
</ul>
<p>我们访问的是<strong>V</strong>的深度维度上的第二层（即index1），因为是在计算第二个激活图，所以这次试用的参数集就是<strong>W1</strong>了。在上面的例子中，为了简洁略去了卷积层对于输出数组<strong>V</strong>中其他部分的操作。还有，要记得这些卷积操作通常后面接的是ReLU层，对激活图中的每个元素做激活函数运算，这里没有显示。<strong>小结</strong>： 我们总结一下卷积层的性质：</p>
<ul>
<li><p>输入数据体的尺寸为$W_1<em>H_1</em>D_1$</p>
</li>
<li><p>4个超参数：</p>
<ul>
<li>滤波器的数量$K$</li>
<li>滤波器的空间尺寸$F$</li>
<li>步长$S$</li>
<li>零填充数量$P$</li>
</ul>
</li>
<li><p>输出数据体的尺寸为$W_2<em>H_2</em>D_2$，其中：</p>
<p>$$W_2=(W_1-F+2P)/S+1$$</p>
<p>$$H_2=(H_1-F+2P)/S+1$$（宽度和高度的计算方法相同）</p>
<p>$$D_2=K$$</p>
</li>
<li><p>由于参数共享，每个滤波器包含$F<em>F</em>D_1$个权重，卷积层一共有$F<em>F</em>D_1*K$</p>
<p>个权重和$K$个偏置。</p>
</li>
<li><p>在输出数据体中，第$d$个深度切片（空间尺寸是$W_2*H_2$），用第$d$个滤波器和输入数据进行有效卷积运算的结果（使用步长$S$），最后在加上第$d$个偏差。</p>
<p>​</p>
</li>
</ul>
<p>对这些超参数，常见的设置是$F=3$，$S=1$，$P=1$。同时设置这些超参数也有一些约定俗成的惯例和经验，可以在下面的卷积神经网络结构章节中查看。</p>
<p>卷积层演示：下面是一个卷积层的运行演示。因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采取将深度切片按照列的方式排列展现。输入数据体的尺寸是$W_1=5$,$H_1=5$,$D_1=3$，卷积层参数$K=2$,$F=3$,$S=2$,$P=1$。就是说，有2个滤波器，滤波器的尺寸是$3\cdot3$，它们的步长是2.因此，输出数据体的空间尺寸是(5-3+2)/2+1=3。注意输入数据体使用了零填充$P=1$，所以输入数据体外边缘一圈都是0。下面的例子在绿色的输出激活数据上循环演示，展示了其中每个元素都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后求其总和，最后加上偏差得来。</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/convnets/convnets_5.png?raw=true" width="400"></center>

<p><strong>译者注</strong>：请点击图片查看动画演示。如果gif不能正确播放，请读者前往<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">斯坦福课程官网</a>查看此演示。</p>
<p><strong>用矩阵乘法实现</strong>：卷积运算本质上就是在滤波器和输入数据的局部区域间做点积。卷积层的常用实现方式就是利用这一点，将卷积层的前向传播变成一个巨大的矩阵乘法：</p>
<ol>
<li>输入图像的局部区域被<strong>im2col</strong>操作拉伸为列。比如，如果输入是[227x227x3]，要与尺寸为11x11x3的滤波器以步长为4进行卷积，就取输入中的[11x11x3]数据块，然后将其拉伸为长度为11x11x3=363的列向量。重复进行这一过程，因为步长为4，所以输出的宽高为(227-11)/4+1=55，所以得到im2col操作的输出矩阵<strong>X_col</strong>的尺寸是[363x3025]，其中每列是拉伸的感受野，共有55x55=3,025个。注意因为感受野之间有重叠，所以输入数据体中的数字在不同的列中可能有重复。</li>
<li>卷积层的权重也同样被拉伸成行。举例，如果有96个尺寸为[11x11x3]的滤波器，就生成一个矩阵<strong>W_row</strong>，尺寸为[96x363]。</li>
<li>现在卷积的结果和进行一个大矩阵乘<strong>np.dot(W_row, X_col)</strong>是等价的了，能得到每个滤波器和每个感受野间的点积。在我们的例子中，这个操作的输出是[96x3025]，给出了每个滤波器在每个位置的点积输出。</li>
<li>结果最后必须被重新变为合理的输出尺寸[55x55x96]。</li>
</ol>
<p>这个方法的缺点就是占用内存太多，因为在输入数据体中的某些值在<strong>X_col</strong>中被复制了多次。但是，其优点是矩阵乘法有非常多的高效实现方式，我们都可以使用（比如常用的<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">BLAS</a>API）。还有，同样的im2col思路可以用在汇聚操作中。</p>
<p>反向传播：卷积操作的反向传播（同时对于数据和权重）还是一个卷积（但是是和空间上翻转的滤波器）。使用一个1维的例子比较容易演示。</p>
<p><strong>1x1卷积</strong>：一些论文中使用了1x1的卷积，这个方法最早是在论文<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Network in Network</a>中出现。人们刚开始看见这个1x1卷积的时候比较困惑，尤其是那些具有信号处理专业背景的人。因为信号是2维的，所以1x1卷积就没有意义。但是，在卷积神经网络中不是这样，因为这里是对3个维度进行操作，滤波器和输入数据体的深度是一样的。比如，如果输入是[32x32x3]，那么1x1卷积就是在高效地进行3维点积（因为输入深度是3个通道）。</p>
<p><strong>扩张卷积</strong>：最近一个研究（<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Fisher Yu和Vladlen Koltun的论文</a>）给卷积层引入了一个新的叫扩张（dilation）的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。举例，在某个维度上滤波器<strong>w</strong>的尺寸是3，那么计算输入<strong>x</strong>的方式是：<strong>w[0]*x[0] + w[1]<em>x[1] + w[2]</em>x[2]</strong>，此时扩张为0。如果扩张为1，那么计算为：<strong>w[0]*x[0] + w[1]<em>x[2] + w[2]</em>x[4]</strong>。换句话说，操作中存在1的间隙。在某些设置中，扩张卷积与正常卷积结合起来非常有用，因为在很少的层数内更快地汇集输入图片的大尺度特征。比如，如果上下重叠2个3x3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5x5的区域（可以成这些神经元的有效感受野是5x5）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。</p>
<h4 id="汇聚层"><a href="#汇聚层" class="headerlink" title="汇聚层"></a>汇聚层</h4><p>通常，在连续的卷积层之间会周期性地插入一个汇聚层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网络中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2x2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2x2的区域）。深度保持不变。汇聚层的一些公式：</p>
<ul>
<li><p>输入数据体尺寸$W_1\cdot H_1\cdot D_1$</p>
</li>
<li><p>有两个超参数：</p>
<ul>
<li><p>空间大小$F$</p>
</li>
<li><p>步长$S$</p>
</li>
</ul>
</li>
<li><p>输出数据体尺寸$W_2\cdot H_2\cdot D_2$，其中</p>
<p>$$W_2=(W_1-F)/S+1$$</p>
<p>$$H_2=(H_1-F)/S+1$$（宽度和高度的计算方法相同）</p>
<p>$$D_2=D_1$$</p>
</li>
<li><p>因为对输入进行的是固定函数计算，所以没有引入参数</p>
</li>
<li><p>在汇聚层中很少使用零填充</p>
<p>在实践中，最大汇聚层通常只有两种形式：一种是$F=3,S=2$，也叫重叠汇聚（overlapping pooling），另一个更常用的是$F=2,S=2$。对更大感受野进行汇聚需要的汇聚尺寸也更大，而且往往对网络有破坏性。</p>
</li>
</ul>
<p><strong>普通汇聚（General Pooling）</strong>：除了最大汇聚，汇聚单元还可以使用其他的函数，比如平均汇聚（average pooling）或L-2范式汇聚（L2-norm pooling）。平均汇聚历史上比较常用，但是现在已经很少使用了。因为实践证明，最大汇聚的效果比平均汇聚要好。</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/convnets/convnets_6.jpg?raw=true" width="400"></center>

<p> 汇聚层在输入数据体的每个深度切片上，独立地对其进行空间上的降采样。左边：本例中，输入数据体尺寸[224x224x64]被降采样到了[112x112x64]，采取的滤波器尺寸是2，步长为2，而深度不变。右边：最常用的降采样操作是取最大值，也就是最大汇聚，这里步长为2，每个取最大值操作是从4个数字中选取（即2x2的方块区域中）。</p>
<p><strong>反向传播：</strong>回顾一下反向传播的内容，其中$max(x,y)$函数的反向传播可以简单理解为将梯度只沿最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作<strong>道岔（switches）</strong>），这样在反向传播的时候梯度的路由就很高效。</p>
<p><strong>不使用汇聚层</strong>：很多人不喜欢汇聚操作，认为可以不使用它。比如在<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Striving for Simplicity: The All Convolutional Net</a>一文中，提出使用一种只有重复的卷积层组成的结构，抛弃汇聚层。通过在卷积层中使用更大的步长来降低数据体的尺寸。有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的。比如变化自编码器（VAEs：variational autoencoders）和生成性对抗网络（GANs：generative adversarial networks）。现在看起来，未来的卷积网络结构中，无汇聚层的结构不太可能扮演重要的角色。</p>
<h4 id="归一化层"><a href="#归一化层" class="headerlink" title="归一化层"></a>归一化层</h4><p>在卷积神经网络的结构中，提出了很多不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明它们的效果即使存在，也是极其有限的。对于不同类型的归一化层，可以看看Alex Krizhevsky的关于<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">cuda-convnet library API</a>的讨论。</p>
<h4 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h4><p>在全连接层中，神经元对于前一层中的所有激活数据是全部连接的，这个常规神经网络中一样。它们的激活可以先用矩阵乘法，再加上偏差。更多细节请查看神经网络章节。</p>
<h2 id="把全连接层转化成卷积层"><a href="#把全连接层转化成卷积层" class="headerlink" title="把全连接层转化成卷积层"></a>把全连接层转化成卷积层</h2><p>全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：</p>
<ul>
<li>对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块（这是因为有局部连接），其余部分都是零。而在其中大部分块中，元素都是相等的（因为参数共享）。</li>
<li>相反，任何全连接层都可以被转化为卷积层。比如，一个$K=4096$的全连接层，输入数据体的尺寸是$7\times7\times512$，这个全连接层可以被等效地看做一个的$F=7,P=0,S=1,K=4096$卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成$1\times1\times4096$，这个结果就和使用初始的那个全连接层一样了。</li>
</ul>
<p><strong>全连接层转化为卷积层</strong>：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224x224x3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7x7x512的激活数据体（在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7）。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：</p>
<ul>
<li>针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为$F=7$，这样输出数据体就为[1x1x4096]了。</li>
<li>针对第二个全连接层，令其滤波器尺寸为$F=1$，这样输出数据体为[1x1x4096]。</li>
<li>对最后一个全连接层也做类似的，令其$F=1$，最终输出为[1x1x1000]</li>
</ul>
<p>实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动（<strong>译者注</strong>：即把一张更大的图片的不同区域都分别带入到卷积网络，得到每个区域的得分），得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。</p>
<p>举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！（<strong>译者注</strong>：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解）</p>
<p>面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。</p>
<p>自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。</p>
<p>最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。（<strong>译者注</strong>：这一段的翻译与原文不同，经过了译者较多的修改，使更容易理解）</p>
<ul>
<li><p><a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Net Surgery</a></p>
<p>上一个使用Caffe演示如何在进行变换的IPython Note教程。</p>
</li>
</ul>
<h3 id="卷积神经网络的结构"><a href="#卷积神经网络的结构" class="headerlink" title="卷积神经网络的结构"></a><strong>卷积神经网络的结构</strong></h3><p>卷积神经网络通常是由三种层构成：卷积层，汇聚层（除非特别说明，一般就是最大值汇聚）和全连接层（简称FC）。ReLU激活函数也应该算是是一层，它逐元素地进行激活函数操作。在本节中将讨论在卷积神经网络中这些层通常是如何组合在一起的。</p>
<h4 id="层的排列规律"><a href="#层的排列规律" class="headerlink" title="层的排列规律"></a>层的排列规律</h4><p>卷积神经网络最常见的形式就是将一些卷积层和ReLU层放在一起，其后紧跟汇聚层，然后重复如此直到图像在空间上被缩小到一个足够小的尺寸，在某个地方过渡成成全连接层也较为常见。最后的全连接层得到输出，比如分类评分等。换句话说，最常见的卷积神经网络结构如下：</p>
<p><strong>INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]<em>M -&gt; [FC -&gt; RELU]</em>K -&gt; FC</strong></p>
<p>其中<strong>*</strong>指的是重复次数，<strong>POOL?</strong>指的是一个可选的汇聚层。其中<strong>N &gt;=0</strong>,通常<strong>N&lt;=3</strong>,<strong>M&gt;=0</strong>,<strong>K&gt;=0</strong>,通常<strong>K&lt;3</strong>。例如，下面是一些常见的网络结构规律：</p>
<ul>
<li><strong>INPUT -&gt; FC</strong>,实现一个线性分类器，此处<strong>N = M = K = 0</strong>。</li>
<li><strong>INPUT -&gt; CONV -&gt; RELU -&gt; FC</strong></li>
<li><strong>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC</strong>。此处在每个汇聚层之间有一个卷积层。</li>
<li><strong>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC</strong>。此处每个汇聚层前有两个卷积层，这个思路适用于更大更深的网络，因为在执行具有破坏性的汇聚操作前，多重的卷积层可以从输入数据中学习到更多的复杂特征。</li>
</ul>
<p>几个小滤波器卷积层的组合比一个大滤波器卷积层好：假设你一层一层地重叠了3个3x3的卷积层（层与层之间有非线性激活函数）。在这个排列下，第一个卷积层中的每个神经元都对输入数据体有一个3x3的视野。第二个卷积层上的神经元对第一个卷积层有一个3x3的视野，也就是对输入数据体有5x5的视野。同样，在第三个卷积层上的神经元对第二个卷积层有3x3的视野，也就是对输入数据体有7x7的视野。假设不采用这3个3x3的卷积层，二是使用一个单独的有7x7的感受野的卷积层，那么所有神经元的感受野也是7x7，但是就有一些缺点。首先，多个卷积层与非线性的激活层交替的结构，比单一卷积层的结构更能提取出深层的更好的特征。其次，假设所有的数据有$C$个通道，那么单独的7x7卷积层将会包含$C\times(7\times 7\times C)=49C^2)$个参数，而3个3x3的卷积层的组合仅有$3\times (C\times (3\times 3\times C))=27C^2$个参数。直观说来，最好选择带有小滤波器的卷积层组合，而不是用一个带有大的滤波器的卷积层。前者可以表达出输入数据中更多个强力特征，使用的参数也更少。唯一的不足是，在进行反向传播时，中间的卷积层可能会导致占用更多的内存。</p>
<p>最新进展：传统的将层按照线性进行排列的方法已经受到了挑战，挑战来自谷歌的Inception结构和微软亚洲研究院的残差网络（Residual Net）结构。这两个网络（下文案例学习小节中有细节）的特征更加复杂，连接结构也不同。</p>
<h4 id="层的尺寸设置规律"><a href="#层的尺寸设置规律" class="headerlink" title="层的尺寸设置规律"></a>层的尺寸设置规律</h4><p>到现在为止，我们都没有提及卷积神经网络中每层的超参数的使用。现在先介绍设置结构尺寸的一般性规则，然后根据这些规则进行讨论：</p>
<p><strong>输入层</strong>（包含图像的）应该能被2整除很多次。常用数字包括32（比如CIFAR-10），64，96（比如STL-10）或224（比如ImageNet卷积神经网络），384和512。</p>
<p><strong>卷积层 </strong>应该使用小尺寸滤波器（比如3x3或最多5x5），使用步长$S=1$。还有一点非常重要，就是对输入数据进行零填充，这样卷积层就不会改变输入数据在空间维度上的尺寸。比如，当$F=3$，那就使用$P=1$来保持输入尺寸。当$F=5,P=2$，一般对于任意$F$，当$P=(F-1)/2$的时候能保持输入尺寸。如果必须使用更大的滤波器尺寸（比如7x7之类），通常只用在第一个面对原始图像的卷积层上。</p>
<p><strong>汇聚层 </strong>负责对输入数据的空间维度进行降采样。最常用的设置是用用2x2感受野（即$F=2$）的最大值汇聚，步长为2（$S=2$）。注意这一操作将会把输入数据中75%的激活数据丢弃（因为对宽度和高度都进行了2的降采样）。另一个不那么常用的设置是使用3x3的感受野，步长为2。最大值汇聚的感受野尺寸很少有超过3的，因为汇聚操作过于激烈，易造成数据信息丢失，这通常会导致算法性能变差。</p>
<p>减少尺寸设置的问题：上文中展示的两种设置是很好的，因为所有的卷积层都能保持其输入数据的空间尺寸，汇聚层只负责对数据体从空间维度进行降采样。如果使用的步长大于1并且不对卷积层的输入数据使用零填充，那么就必须非常仔细地监督输入数据体通过整个卷积神经网络结构的过程，确认所有的步长和滤波器都尺寸互相吻合，卷积神经网络的结构美妙对称地联系在一起。</p>
<p>为什么在卷积层使用1的步长？在实际应用中，更小的步长效果更好。上文也已经提过，步长为1可以让空间维度的降采样全部由汇聚层负责，卷积层只负责对输入数据体的深度进行变换。</p>
<p>为何使用零填充？使用零填充除了前面提到的可以让卷积层的输出数据保持和输入数据在空间维度的不变，还可以提高算法性能。如果卷积层值进行卷积而不进行零填充，那么数据体的尺寸就会略微减小，那么图像边缘的信息就会过快地损失掉。</p>
<p>因为内存限制所做的妥协：在某些案例（尤其是早期的卷积神经网络结构）中，基于前面的各种规则，内存的使用量迅速飙升。例如，使用64个尺寸为3x3的滤波器对224x224x3的图像进行卷积，零填充为1，得到的激活数据体尺寸是[224x224x64]。这个数量就是一千万的激活数据，或者就是72MB的内存（每张图就是这么多，激活函数和梯度都是）。因为GPU通常因为内存导致性能瓶颈，所以做出一些妥协是必须的。在实践中，人们倾向于在网络的第一个卷积层做出妥协。例如，可以妥协可能是在第一个卷积层使用步长为2，尺寸为7x7的滤波器（比如在ZFnet中）。在AlexNet中，滤波器的尺寸的11x11，步长为4。</p>
<h4 id="案例学习"><a href="#案例学习" class="headerlink" title="案例学习"></a>案例学习</h4><p>下面是卷积神经网络领域中比较有名的几种结构：</p>
<ul>
<li><strong>LeNet</strong>： 第一个成功的卷积神经网络应用，是Yann LeCun在上世纪90年代实现的。当然，最著名还是被应用在识别数字和邮政编码等的<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">LeNet</a>结构。</li>
<li><strong>AlexNet</strong>：<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">AlexNet</a>卷积神经网络在计算机视觉领域中受到欢迎，它由Alex Krizhevsky，Ilya Sutskever和Geoff Hinton实现。AlexNet在2012年的<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">ImageNet ILSVRC 竞赛</a>中夺冠，性能远远超出第二名（16%的top5错误率，第二名是26%的top5错误率）。这个网络的结构和LeNet非常类似，但是更深更大，并且使用了层叠的卷积层来获取特征（之前通常是只用一个卷积层并且在其后马上跟着一个汇聚层）。</li>
<li><strong>ZF Net</strong>：Matthew Zeiler和Rob Fergus发明的网络在ILSVRC 2013比赛中夺冠，它被称为<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">ZFNet</a>（Zeiler &amp; Fergus Net的简称）。它通过修改结构中的超参数来实现对AlexNet的改良，具体说来就是增加了中间卷积层的尺寸，让第一层的步长和滤波器尺寸更小。</li>
<li><strong>GoogLeNet</strong>：ILSVRC 2014的胜利者是谷歌的<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Szeged等</a>实现的卷积神经网络。它主要的贡献就是实现了一个奠基模块，它能够显著地减少网络中参数的数量（AlexNet中有60M，该网络中只有4M）。还有，这个论文中没有使用卷积神经网络顶部使用全连接层，而是使用了一个平均汇聚，把大量不是很重要的参数都去除掉了。GooLeNet还有几种改进的版本，最新的一个是<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Inception-v4</a>。</li>
<li><strong>VGGNet</strong>：ILSVRC 2014的第二名是Karen Simonyan和 Andrew Zisserman实现的卷积神经网络，现在称其为<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">VGGNet</a>。它主要的贡献是展示出网络的深度是算法优良性能的关键部分。他们最好的网络包含了16个卷积/全连接层。网络的结构非常一致，从头到尾全部使用的是3x3的卷积和2x2的汇聚。他们的<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">预训练模型</a>是可以在网络上获得并在Caffe中使用的。VGGNet不好的一点是它耗费更多计算资源，并且使用了更多的参数，导致更多的内存占用（140M）。其中绝大多数的参数都是来自于第一个全连接层。后来发现这些全连接层即使被去除，对于性能也没有什么影响，这样就显著降低了参数数量。</li>
<li><strong>ResNet</strong>：<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">残差网络</a>（Residual Network）是ILSVRC2015的胜利者，由何恺明等实现。它使用了特殊的跳跃链接，大量使用了<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">批量归一化</a>（batch normalization）。这个结构同样在最后没有使用全连接层。读者可以查看何恺明的的演讲（<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">视频</a>，<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">PPT</a>），以及一些使用Torch重现网络的<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">实验</a>。ResNet当前最好的卷积神经网络模型（2016年五月）。何开明等最近的工作是对原始结构做一些优化，可以看论文<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Identity Mappings in Deep Residual Networks</a>，2016年3月发表。</li>
</ul>
<p><strong>VGGNet的细节：</strong>我们进一步对<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">VGGNet</a>的细节进行分析学习。整个VGGNet中的卷积层都是以步长为1进行3x3的卷积，使用了1的零填充，汇聚层都是以步长为2进行了2x2的最大值汇聚。可以写出处理过程中每一步数据体尺寸的变化，然后对数据尺寸和整体权重的数量进行查看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">INPUT: [<span class="number">224</span>x224x3]        memory:  <span class="number">224</span>*<span class="number">224</span>*<span class="number">3</span>=<span class="number">150</span>K   weights: <span class="number">0</span></span><br><span class="line">CONV3<span class="number">-64</span>: [<span class="number">224</span>x224x64]  memory:  <span class="number">224</span>*<span class="number">224</span>*<span class="number">64</span>=<span class="number">3.2</span>M   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">3</span>)*<span class="number">64</span> = <span class="number">1</span>,<span class="number">728</span></span><br><span class="line">CONV3<span class="number">-64</span>: [<span class="number">224</span>x224x64]  memory:  <span class="number">224</span>*<span class="number">224</span>*<span class="number">64</span>=<span class="number">3.2</span>M   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">64</span>)*<span class="number">64</span> = <span class="number">36</span>,<span class="number">864</span></span><br><span class="line">POOL2: [<span class="number">112</span>x112x64]  memory:  <span class="number">112</span>*<span class="number">112</span>*<span class="number">64</span>=<span class="number">800</span>K   weights: <span class="number">0</span></span><br><span class="line">CONV3<span class="number">-128</span>: [<span class="number">112</span>x112x128]  memory:  <span class="number">112</span>*<span class="number">112</span>*<span class="number">128</span>=<span class="number">1.6</span>M   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">64</span>)*<span class="number">128</span> = <span class="number">73</span>,<span class="number">728</span></span><br><span class="line">CONV3<span class="number">-128</span>: [<span class="number">112</span>x112x128]  memory:  <span class="number">112</span>*<span class="number">112</span>*<span class="number">128</span>=<span class="number">1.6</span>M   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span>)*<span class="number">128</span> = <span class="number">147</span>,<span class="number">456</span></span><br><span class="line">POOL2: [<span class="number">56</span>x56x128]  memory:  <span class="number">56</span>*<span class="number">56</span>*<span class="number">128</span>=<span class="number">400</span>K   weights: <span class="number">0</span></span><br><span class="line">CONV3<span class="number">-256</span>: [<span class="number">56</span>x56x256]  memory:  <span class="number">56</span>*<span class="number">56</span>*<span class="number">256</span>=<span class="number">800</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">128</span>)*<span class="number">256</span> = <span class="number">294</span>,<span class="number">912</span></span><br><span class="line">CONV3<span class="number">-256</span>: [<span class="number">56</span>x56x256]  memory:  <span class="number">56</span>*<span class="number">56</span>*<span class="number">256</span>=<span class="number">800</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">256</span>)*<span class="number">256</span> = <span class="number">589</span>,<span class="number">824</span></span><br><span class="line">CONV3<span class="number">-256</span>: [<span class="number">56</span>x56x256]  memory:  <span class="number">56</span>*<span class="number">56</span>*<span class="number">256</span>=<span class="number">800</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">256</span>)*<span class="number">256</span> = <span class="number">589</span>,<span class="number">824</span></span><br><span class="line">POOL2: [<span class="number">28</span>x28x256]  memory:  <span class="number">28</span>*<span class="number">28</span>*<span class="number">256</span>=<span class="number">200</span>K   weights: <span class="number">0</span></span><br><span class="line">CONV3<span class="number">-512</span>: [<span class="number">28</span>x28x512]  memory:  <span class="number">28</span>*<span class="number">28</span>*<span class="number">512</span>=<span class="number">400</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">256</span>)*<span class="number">512</span> = <span class="number">1</span>,<span class="number">179</span>,<span class="number">648</span></span><br><span class="line">CONV3<span class="number">-512</span>: [<span class="number">28</span>x28x512]  memory:  <span class="number">28</span>*<span class="number">28</span>*<span class="number">512</span>=<span class="number">400</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">CONV3<span class="number">-512</span>: [<span class="number">28</span>x28x512]  memory:  <span class="number">28</span>*<span class="number">28</span>*<span class="number">512</span>=<span class="number">400</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">POOL2: [<span class="number">14</span>x14x512]  memory:  <span class="number">14</span>*<span class="number">14</span>*<span class="number">512</span>=<span class="number">100</span>K   weights: <span class="number">0</span></span><br><span class="line">CONV3<span class="number">-512</span>: [<span class="number">14</span>x14x512]  memory:  <span class="number">14</span>*<span class="number">14</span>*<span class="number">512</span>=<span class="number">100</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">CONV3<span class="number">-512</span>: [<span class="number">14</span>x14x512]  memory:  <span class="number">14</span>*<span class="number">14</span>*<span class="number">512</span>=<span class="number">100</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">CONV3<span class="number">-512</span>: [<span class="number">14</span>x14x512]  memory:  <span class="number">14</span>*<span class="number">14</span>*<span class="number">512</span>=<span class="number">100</span>K   weights: (<span class="number">3</span>*<span class="number">3</span>*<span class="number">512</span>)*<span class="number">512</span> = <span class="number">2</span>,<span class="number">359</span>,<span class="number">296</span></span><br><span class="line">POOL2: [<span class="number">7</span>x7x512]  memory:  <span class="number">7</span>*<span class="number">7</span>*<span class="number">512</span>=<span class="number">25</span>K  weights: <span class="number">0</span></span><br><span class="line">FC: [<span class="number">1</span>x1x4096]  memory:  <span class="number">4096</span>  weights: <span class="number">7</span>*<span class="number">7</span>*<span class="number">512</span>*<span class="number">4096</span> = <span class="number">102</span>,<span class="number">760</span>,<span class="number">448</span></span><br><span class="line">FC: [<span class="number">1</span>x1x4096]  memory:  <span class="number">4096</span>  weights: <span class="number">4096</span>*<span class="number">4096</span> = <span class="number">16</span>,<span class="number">777</span>,<span class="number">216</span></span><br><span class="line">FC: [<span class="number">1</span>x1x1000]  memory:  <span class="number">1000</span> weights: <span class="number">4096</span>*<span class="number">1000</span> = <span class="number">4</span>,<span class="number">096</span>,<span class="number">000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TOTAL memory: <span class="number">24</span>M * <span class="number">4</span> bytes ~= <span class="number">93</span>MB / image (only forward! ~*<span class="number">2</span> <span class="keyword">for</span> bwd)</span><br><span class="line">TOTAL params: <span class="number">138</span>M parameters</span><br></pre></td></tr></table></figure>
<p>注意，大部分的内存和计算时间都被前面的卷积层占用，大部分的参数都用在后面的全连接层，这在卷积神经网络中是比较常见的。在这个例子中，全部参数有140M，但第一个全连接层就包含了100M的参数。</p>
<h2 id="计算上的考量"><a href="#计算上的考量" class="headerlink" title="计算上的考量"></a>计算上的考量</h2><p>在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源：</p>
<ul>
<li>来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。</li>
<li>来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。</li>
<li>卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。</li>
</ul>
<p>一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。</p>
<p>在构建卷积神经网络结构时，最大的瓶颈是内存瓶颈。大部分现代GPU的内存是3/4/6GB，最好的GPU大约有12GB的内存。要注意三种内存占用来源：</p>
<ul>
<li>来自中间数据体尺寸：卷积神经网络中的每一层中都有激活数据体的原始数值，以及损失函数对它们的梯度（和激活数据体尺寸一致）。通常，大部分激活数据都是在网络中靠前的层中（比如第一个卷积层）。在训练时，这些数据需要放在内存中，因为反向传播的时候还会用到。但是在测试时可以聪明点：让网络在测试运行时候每层都只存储当前的激活数据，然后丢弃前面层的激活数据，这样就能减少巨大的激活数据量。</li>
<li>来自参数尺寸：即整个网络的参数的数量，在反向传播时它们的梯度值，以及使用momentum、Adagrad或RMSProp等方法进行最优化时的每一步计算缓存。因此，存储参数向量的内存通常需要在参数向量的容量基础上乘以3或者更多。</li>
<li>卷积神经网络实现还有各种零散的内存占用，比如成批的训练数据，扩充的数据等等。</li>
</ul>
<p>一旦对于所有这些数值的数量有了一个大略估计（包含激活数据，梯度和各种杂项），数量应该转化为以GB为计量单位。把这个值乘以4，得到原始的字节数（因为每个浮点数占用4个字节，如果是双精度浮点数那就是占用8个字节），然后多次除以1024分别得到占用内存的KB，MB，最后是GB计量。如果你的网络工作得不好，一个常用的方法是降低批尺寸（batch size），因为绝大多数的内存都是被激活数据消耗掉了。</p>
<h2 id="拓展资源"><a href="#拓展资源" class="headerlink" title="拓展资源"></a><strong>拓展资源</strong></h2><p>和实践相关的拓展资源：</p>
<ul>
<li><a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Soumith benchmarks for CONV performance</a></li>
<li><a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">ConvNetJS CIFAR-10 demo</a>可以让你在服务器上实时地调试卷积神经网络的结构，观察计算结果。</li>
<li><a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Caffe</a>，一个流行的卷积神经网络库。</li>
<li><a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">State of the art ResNets in Torch7</a></li>
</ul>
<p><strong>卷积神经网络笔记</strong>结束。</p>
<blockquote>
<p>注:本文翻译自斯坦福CS231n课程笔记<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">ConvNet notes</a>，由课程教师<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">Andrej Karpathy</a>授权进行翻译。本篇教程由<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">杜客</a>和<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">猴子</a>翻译完成，<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">堃堃</a>和<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">李艺颖</a>进行校对修改。来自网址:<a href="https://www.gitbook.com/book/heroinlin/mybook/edit#" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/22038289</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Vorherige Seite"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Heroinlin</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">35</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">11</span>
                    <span class="site-state-item-name">Kategorien</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">59</span>
                    <span class="site-state-item-name">schlagwörter</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/heroinlin" title="GitHub &rarr; https://github.com/heroinlin" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="/heroinlj@gmail.com" title="E-Mail &rarr; heroinlj@gmail.com"><i class="fa fa-fw fa-gmail"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://twitter.com/Heroin" title="Twitter &rarr; https://twitter.com/Heroin" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Heroinlin</span>

  

  
</div>


  <div class="powered-by">Erstellt mit  <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.6.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Design – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.1.2</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/affix.js?v=7.1.2"></script>

  <script src="/js/schemes/pisces.js?v=7.1.2"></script>



  

  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  



  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
