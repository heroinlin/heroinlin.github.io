<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>


















  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"/>







<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.1.2"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/leaf.jpg?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/leaf.ico?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/leaf1.ico?v=7.1.2">


  <link rel="mask-icon" href="/images/leaf.jpg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Heroinlin&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Heroinlin&#39;s Blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Heroinlin&#39;s Blog">





  
  
  <link rel="canonical" href="http://yoursite.com/page/3/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Heroinlin's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Heroinlin's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">

    
    
    
      
    

    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br/>Schedule</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/12/Python/Python_String/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/12/Python/Python_String/" class="post-title-link" itemprop="url">Python之String模块</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-12 10:48:11 / Modified: 10:51:28" itemprop="dateCreated datePublished" datetime="2018-03-12T10:48:11+08:00">2018-03-12</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>#Python之String模块</p>
<p>本部分是对python中的string模块进行解析，添加函数中文说明与实例说明</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""A collection of string operations (most are no longer used).</span></span><br><span class="line"><span class="string">Warning: most of the code you see here isn't normally used nowadays.</span></span><br><span class="line"><span class="string">Beginning with Python 1.6, many of these functions are implemented as</span></span><br><span class="line"><span class="string">methods on the standard string object. They used to be implemented by</span></span><br><span class="line"><span class="string">a built-in module called strop, but strop is now obsolete itself.</span></span><br><span class="line"><span class="string">Public module variables:</span></span><br><span class="line"><span class="string">whitespace -- a string containing all characters considered whitespace</span></span><br><span class="line"><span class="string">lowercase -- a string containing all characters considered lowercase letters</span></span><br><span class="line"><span class="string">uppercase -- a string containing all characters considered uppercase letters</span></span><br><span class="line"><span class="string">letters -- a string containing all characters considered letters</span></span><br><span class="line"><span class="string">digits -- a string containing all characters considered decimal digits</span></span><br><span class="line"><span class="string">hexdigits -- a string containing all characters considered hexadecimal digits</span></span><br><span class="line"><span class="string">octdigits -- a string containing all characters considered octal digits</span></span><br><span class="line"><span class="string">punctuation -- a string containing all characters considered punctuation</span></span><br><span class="line"><span class="string">printable -- a string containing all characters considered printable</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Some strings for ctype-style character classification</span></span><br><span class="line">whitespace = <span class="string">' \t\n\r\v\f'</span></span><br><span class="line">lowercase = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">uppercase = <span class="string">'ABCDEFGHIJKLMNOPQRSTUVWXYZ'</span></span><br><span class="line">letters = lowercase + uppercase</span><br><span class="line">ascii_lowercase = lowercase</span><br><span class="line">ascii_uppercase = uppercase</span><br><span class="line">ascii_letters = ascii_lowercase + ascii_uppercase</span><br><span class="line">digits = <span class="string">'0123456789'</span></span><br><span class="line">hexdigits = digits + <span class="string">'abcdef'</span> + <span class="string">'ABCDEF'</span></span><br><span class="line">octdigits = <span class="string">'01234567'</span></span><br><span class="line">punctuation = <span class="string">"""!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`&#123;|&#125;~"""</span></span><br><span class="line">printable = digits + letters + punctuation + whitespace</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case conversion helpers</span></span><br><span class="line"><span class="comment"># Use str to convert Unicode literal in case of -U</span></span><br><span class="line">l = map(chr, xrange(<span class="number">256</span>))</span><br><span class="line">_idmap = str(<span class="string">''</span>).join(l)</span><br><span class="line"><span class="keyword">del</span> l</span><br><span class="line"></span><br><span class="line"><span class="comment"># Functions which aren't available as string methods.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Capitalize the words in a string, e.g. " aBc  dEf " -&gt; "Abc Def".</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">capwords</span><span class="params">(s, sep=None)</span>:</span></span><br><span class="line">    <span class="string">"""capwords(s [,sep]) -&gt; string</span></span><br><span class="line"><span class="string">    用split拆分参数转换为单词，利用capitalize使单词首字母大写，并且用join连接这些单词。</span></span><br><span class="line"><span class="string">    如果可选的第二个参数sep是缺省或无，以单个空格代替一串空白字符串，</span></span><br><span class="line"><span class="string">    开头和结尾的空格被删除，否则以sep为分隔符来分割和连接单词.</span></span><br><span class="line"><span class="string">    如string.capwords("   nice   to   meet  you     "),输出为：Nice To Meet You</span></span><br><span class="line"><span class="string">    如string.capwords(" niceto  meet  you ","e"),输出为： niceTo  meeT  you</span></span><br><span class="line"><span class="string">    Split the argument into words using split, capitalize each</span></span><br><span class="line"><span class="string">    word using capitalize, and join the capitalized words using</span></span><br><span class="line"><span class="string">    join.  If the optional second argument sep is absent or None,</span></span><br><span class="line"><span class="string">    runs of whitespace characters are replaced by a single space</span></span><br><span class="line"><span class="string">    and leading and trailing whitespace are removed, otherwise</span></span><br><span class="line"><span class="string">    sep is used to split and join the words.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> (sep <span class="keyword">or</span> <span class="string">' '</span>).join(x.capitalize() <span class="keyword">for</span> x <span class="keyword">in</span> s.split(sep))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct a translation string</span></span><br><span class="line">_idmapL = <span class="keyword">None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maketrans</span><span class="params">(fromstr, tostr)</span>:</span></span><br><span class="line">    <span class="string">"""maketrans(frm, to) -&gt; string</span></span><br><span class="line"><span class="string">    返回一个转换表适用于string.translate使用（字符串长256字节）。字符串frm和to必须具有相同的长度</span></span><br><span class="line"><span class="string">    Return a translation table (a string of 256 bytes long)</span></span><br><span class="line"><span class="string">    suitable for use in string.translate.  The strings frm and to</span></span><br><span class="line"><span class="string">    must be of the same length.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> len(fromstr) != len(tostr):</span><br><span class="line">        <span class="keyword">raise</span> ValueError, <span class="string">"maketrans arguments must have same length"</span></span><br><span class="line">    <span class="keyword">global</span> _idmapL</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> _idmapL:</span><br><span class="line">        _idmapL = list(_idmap)</span><br><span class="line">    L = _idmapL[:]</span><br><span class="line">    fromstr = map(ord, fromstr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(fromstr)):</span><br><span class="line">        L[fromstr[i]] = tostr[i]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(L)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">####################################################################</span></span><br><span class="line"><span class="keyword">import</span> re <span class="keyword">as</span> _re</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_multimap</span>:</span></span><br><span class="line">    <span class="string">"""Helper class for combining multiple mappings.</span></span><br><span class="line"><span class="string">    Used by .&#123;safe_,&#125;substitute() to combine the mapping and keyword</span></span><br><span class="line"><span class="string">    arguments.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, primary, secondary)</span>:</span></span><br><span class="line">        self._primary = primary</span><br><span class="line">        self._secondary = secondary</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self._primary[key]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">return</span> self._secondary[key]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_TemplateMetaclass</span><span class="params">(type)</span>:</span></span><br><span class="line">    pattern = <span class="string">r"""</span></span><br><span class="line"><span class="string">    %(delim)s(?:</span></span><br><span class="line"><span class="string">      (?P&lt;escaped&gt;%(delim)s) |   # Escape sequence of two delimiters</span></span><br><span class="line"><span class="string">      (?P&lt;named&gt;%(id)s)      |   # delimiter and a Python identifier</span></span><br><span class="line"><span class="string">      &#123;(?P&lt;braced&gt;%(id)s)&#125;   |   # delimiter and a braced identifier</span></span><br><span class="line"><span class="string">      (?P&lt;invalid&gt;)              # Other ill-formed delimiter exprs</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(cls, name, bases, dct)</span>:</span></span><br><span class="line">        super(_TemplateMetaclass, cls).__init__(name, bases, dct)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'pattern'</span> <span class="keyword">in</span> dct:</span><br><span class="line">            pattern = cls.pattern</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pattern = _TemplateMetaclass.pattern % &#123;</span><br><span class="line">                <span class="string">'delim'</span> : _re.escape(cls.delimiter),</span><br><span class="line">                <span class="string">'id'</span>    : cls.idpattern,</span><br><span class="line">                &#125;</span><br><span class="line">        cls.pattern = _re.compile(pattern, _re.IGNORECASE | _re.VERBOSE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Template</span>:</span></span><br><span class="line">    <span class="string">"""A string class for supporting $-substitutions."""</span></span><br><span class="line">    __metaclass__ = _TemplateMetaclass</span><br><span class="line"></span><br><span class="line">    delimiter = <span class="string">'$'</span></span><br><span class="line">    idpattern = <span class="string">r'[_a-z][_a-z0-9]*'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, template)</span>:</span></span><br><span class="line">        self.template = template</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Search for $$, $identifier, $&#123;identifier&#125;, and any bare $'s</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_invalid</span><span class="params">(self, mo)</span>:</span></span><br><span class="line">        i = mo.start(<span class="string">'invalid'</span>)</span><br><span class="line">        lines = self.template[:i].splitlines(<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> lines:</span><br><span class="line">            colno = <span class="number">1</span></span><br><span class="line">            lineno = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            colno = i - len(<span class="string">''</span>.join(lines[:<span class="number">-1</span>]))</span><br><span class="line">            lineno = len(lines)</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid placeholder in string: line %d, col %d'</span> %</span><br><span class="line">                         (lineno, colno))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">substitute</span><span class="params">(*args, **kws)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"descriptor 'substitute' of 'Template' object "</span></span><br><span class="line">                            <span class="string">"needs an argument"</span>)</span><br><span class="line">        self, args = args[<span class="number">0</span>], args[<span class="number">1</span>:]  <span class="comment"># allow the "self" keyword be passed</span></span><br><span class="line">        <span class="keyword">if</span> len(args) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'Too many positional arguments'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            mapping = kws</span><br><span class="line">        <span class="keyword">elif</span> kws:</span><br><span class="line">            mapping = _multimap(kws, args[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mapping = args[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Helper function for .sub()</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(mo)</span>:</span></span><br><span class="line">            <span class="comment"># Check the most common path first.</span></span><br><span class="line">            named = mo.group(<span class="string">'named'</span>) <span class="keyword">or</span> mo.group(<span class="string">'braced'</span>)</span><br><span class="line">            <span class="keyword">if</span> named <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                val = mapping[named]</span><br><span class="line">                <span class="comment"># We use this idiom instead of str() because the latter will</span></span><br><span class="line">                <span class="comment"># fail if val is a Unicode containing non-ASCII characters.</span></span><br><span class="line">                <span class="keyword">return</span> <span class="string">'%s'</span> % (val,)</span><br><span class="line">            <span class="keyword">if</span> mo.group(<span class="string">'escaped'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> self.delimiter</span><br><span class="line">            <span class="keyword">if</span> mo.group(<span class="string">'invalid'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                self._invalid(mo)</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Unrecognized named group in pattern'</span>,</span><br><span class="line">                             self.pattern)</span><br><span class="line">        <span class="keyword">return</span> self.pattern.sub(convert, self.template)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">safe_substitute</span><span class="params">(*args, **kws)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"descriptor 'safe_substitute' of 'Template' object "</span></span><br><span class="line">                            <span class="string">"needs an argument"</span>)</span><br><span class="line">        self, args = args[<span class="number">0</span>], args[<span class="number">1</span>:]  <span class="comment"># allow the "self" keyword be passed</span></span><br><span class="line">        <span class="keyword">if</span> len(args) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'Too many positional arguments'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            mapping = kws</span><br><span class="line">        <span class="keyword">elif</span> kws:</span><br><span class="line">            mapping = _multimap(kws, args[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mapping = args[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Helper function for .sub()</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(mo)</span>:</span></span><br><span class="line">            named = mo.group(<span class="string">'named'</span>) <span class="keyword">or</span> mo.group(<span class="string">'braced'</span>)</span><br><span class="line">            <span class="keyword">if</span> named <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="comment"># We use this idiom instead of str() because the latter</span></span><br><span class="line">                    <span class="comment"># will fail if val is a Unicode containing non-ASCII</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="string">'%s'</span> % (mapping[named],)</span><br><span class="line">                <span class="keyword">except</span> KeyError:</span><br><span class="line">                    <span class="keyword">return</span> mo.group()</span><br><span class="line">            <span class="keyword">if</span> mo.group(<span class="string">'escaped'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> self.delimiter</span><br><span class="line">            <span class="keyword">if</span> mo.group(<span class="string">'invalid'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> mo.group()</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Unrecognized named group in pattern'</span>,</span><br><span class="line">                             self.pattern)</span><br><span class="line">        <span class="keyword">return</span> self.pattern.sub(convert, self.template)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">####################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> Everything below here is deprecated.  Use string methods instead.</span></span><br><span class="line"><span class="comment"># This stuff will go away in Python 3.0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward compatible names for exceptions</span></span><br><span class="line">index_error = ValueError</span><br><span class="line">atoi_error = ValueError</span><br><span class="line">atof_error = ValueError</span><br><span class="line">atol_error = ValueError</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert UPPER CASE letters to lower case</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""lower(s) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s转换为小写的副本。</span></span><br><span class="line"><span class="string">    Return a copy of the string s converted to lowercase.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert lower case letters to UPPER CASE</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">upper</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""upper(s) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s转换为大写的副本。</span></span><br><span class="line"><span class="string">    Return a copy of the string s converted to uppercase.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.upper()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Swap lower case letters and UPPER CASE</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">swapcase</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""swapcase(s) -&gt; string</span></span><br><span class="line"><span class="string">    返回的字符串s的副本,大写字符转换为小写，反之亦然。</span></span><br><span class="line"><span class="string">    Return a copy of the string s with upper case characters</span></span><br><span class="line"><span class="string">    converted to lowercase and vice versa.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.swapcase()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Strip leading and trailing tabs and spaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strip</span><span class="params">(s, chars=None)</span>:</span></span><br><span class="line">    <span class="string">"""strip(s [,chars]) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s的副本，开头和结尾的空格去掉。</span></span><br><span class="line"><span class="string">    如果chars给出，删除s开头和结尾的chars字符串，如string.strip("as1asdgas","as"),输出为：1asdg</span></span><br><span class="line"><span class="string">    Return a copy of the string s with leading and trailing</span></span><br><span class="line"><span class="string">    whitespace removed.</span></span><br><span class="line"><span class="string">    If chars is given and not None, remove characters in chars instead.</span></span><br><span class="line"><span class="string">    If chars is unicode, S will be converted to unicode before stripping.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.strip(chars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Strip leading tabs and spaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstrip</span><span class="params">(s, chars=None)</span>:</span></span><br><span class="line">    <span class="string">"""lstrip(s [,chars]) -&gt; string</span></span><br><span class="line"><span class="string">    返回的字符串s的副本,开头空格删除。</span></span><br><span class="line"><span class="string">    如果字符给出，而不是None，删除s开头的chars字符串。如string.lstrip("as1asdgas","as"),输出为：1asdgas</span></span><br><span class="line"><span class="string">    Return a copy of the string s with leading whitespace removed.</span></span><br><span class="line"><span class="string">    If chars is given and not None, remove characters in chars instead.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.lstrip(chars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Strip trailing tabs and spaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rstrip</span><span class="params">(s, chars=None)</span>:</span></span><br><span class="line">    <span class="string">"""rstrip(s [,chars]) -&gt; string</span></span><br><span class="line"><span class="string">    返回的字符串s的副本,结尾空格删除。</span></span><br><span class="line"><span class="string">    如果字符给出，而不是None，删除s结尾的chars字符串。如string.rstrip("as1asdgas","as"),输出为：as1asdg</span></span><br><span class="line"><span class="string">    Return a copy of the string s with trailing whitespace removed.</span></span><br><span class="line"><span class="string">    If chars is given and not None, remove characters in chars instead.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rstrip(chars)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split a string into a list of space/tab-separated words</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(s, sep=None, maxsplit=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""split(s [,sep [,maxsplit]]) -&gt; list of strings</span></span><br><span class="line"><span class="string">    返回字符串s中单词的列表，用sep作为字符串的分隔符。如果maxsplit给出，分割在不超过第</span></span><br><span class="line"><span class="string">    maxsplit个分隔符的位置（结果至多为maxsplit+ 1个单词）。如果sep未指定或为None，以空白字符串作为分隔符。</span></span><br><span class="line"><span class="string">    如string.split("a  ds sd"),输出为['a','ds','sd']</span></span><br><span class="line"><span class="string">    string.split("a  ds sd",maxsplit=1),输出为['a','ds sd']</span></span><br><span class="line"><span class="string">    Return a list of the words in the string s, using sep as the</span></span><br><span class="line"><span class="string">    delimiter string.  If maxsplit is given, splits at no more than</span></span><br><span class="line"><span class="string">    maxsplit places (resulting in at most maxsplit+1 words).  If sep</span></span><br><span class="line"><span class="string">    is not specified or is None, any whitespace string is a separator.</span></span><br><span class="line"><span class="string">    (split and splitfields are synonymous)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.split(sep, maxsplit)</span><br><span class="line">splitfields = split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split a string into a list of space/tab-separated words</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rsplit</span><span class="params">(s, sep=None, maxsplit=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""rsplit(s [,sep [,maxsplit]]) -&gt; list of strings</span></span><br><span class="line"><span class="string">    返回字符串s中单词的列表，用sep作为字符串的分隔符,分割符从字符串s尾部开始算起。如果maxsplit给出，分割在不超过第</span></span><br><span class="line"><span class="string">    maxsplit个分隔符的位置（结果至多为maxsplit+ 1个单词）。如果sep未指定或为None，以空白字符串作为分隔符。</span></span><br><span class="line"><span class="string">    如string.rsplit("a  ds sd"),输出为['a','ds','sd']</span></span><br><span class="line"><span class="string">    string.rsplit("a  ds sd",maxsplit=1),输出为['a ds','sd']</span></span><br><span class="line"><span class="string">    Return a list of the words in the string s, using sep as the</span></span><br><span class="line"><span class="string">    delimiter string, starting at the end of the string and working</span></span><br><span class="line"><span class="string">    to the front.  If maxsplit is given, at most maxsplit splits are</span></span><br><span class="line"><span class="string">    done. If sep is not specified or is None, any whitespace string</span></span><br><span class="line"><span class="string">    is a separator.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rsplit(sep, maxsplit)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Join fields with optional separator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span><span class="params">(words, sep = <span class="string">' '</span>)</span>:</span></span><br><span class="line">    <span class="string">"""join(list [,sep]) -&gt; string</span></span><br><span class="line"><span class="string">    返回列表中的单词所组成字符串，以sep相串联。默认的分隔符是一个空格。</span></span><br><span class="line"><span class="string">    如string.join(["nice","to", "meet","you"])，输出为：nice to meet you</span></span><br><span class="line"><span class="string">    Return a string composed of the words in list, with</span></span><br><span class="line"><span class="string">    intervening occurrences of sep.  The default separator is a</span></span><br><span class="line"><span class="string">    single space.</span></span><br><span class="line"><span class="string">    (joinfields and join are synonymous)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> sep.join(words)</span><br><span class="line">joinfields = join</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find substring, raise exception if not found</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""index(s, sub [,start [,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    类似find，但是没有找到子串时引发ValueError错误。返回子串出现的首位置，否则报错。</span></span><br><span class="line"><span class="string">    如string.index("a sd fg a","a"),输出为：0</span></span><br><span class="line"><span class="string">    如string.index("a sd fg a fg","sdsdfg"[4:5]),输出为:5</span></span><br><span class="line"><span class="string">    Like find but raises ValueError when the substring is not found.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.index(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find last substring, raise exception if not found</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rindex</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""rindex(s, sub [,start [,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    类似rfind，但是没有找到子串时引发ValueError错误。返回子串出现的最后位置，否则报错。</span></span><br><span class="line"><span class="string">    如string.rindex("a sd fg a","a"),输出为：8</span></span><br><span class="line"><span class="string">    如string.rindex("a sd fg a fg","sdsdfg"[4:5]),输出为:10</span></span><br><span class="line"><span class="string">    Like rfind but raises ValueError when the substring is not found.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rindex(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Count non-overlapping occurrences of substring</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""count(s, sub[, start[,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    返回字符串s[start:end]中的子串sub出现的次数.可选参数start和end都解释为片符号。</span></span><br><span class="line"><span class="string">    如string.count("a sd fg a","a"),输出为2</span></span><br><span class="line"><span class="string">    如string.count("a sd fg a fg","sdsdfg"[2:3]),输出为:1</span></span><br><span class="line"><span class="string">    Return the number of occurrences of substring sub in string</span></span><br><span class="line"><span class="string">    s[start:end].  Optional arguments start and end are</span></span><br><span class="line"><span class="string">    interpreted as in slice notation.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.count(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find substring, return -1 if not found</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""find(s, sub [,start [,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    返回子串出现的首位置，子串包含在s[start,end]中(即子串sub长度不超过s).可选参数start和end都解释为片符号。不存在是返回-1</span></span><br><span class="line"><span class="string">    如string.find("a sd fg a","a"),输出为：0</span></span><br><span class="line"><span class="string">    如string.find("a sd fg a fg","sdsdfg"[4:5]),输出为:5</span></span><br><span class="line"><span class="string">    Return the lowest index in s where substring sub is found,</span></span><br><span class="line"><span class="string">    such that sub is contained within s[start,end].  Optional</span></span><br><span class="line"><span class="string">    arguments start and end are interpreted as in slice notation.</span></span><br><span class="line"><span class="string">    Return -1 on failure.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.find(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find last substring, return -1 if not found</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rfind</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""rfind(s, sub [,start [,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    返回子串出现的最后位置，子串包含在s[start,end]中(即子串sub长度不超过s).可选参数start和end都解释为片符号。不存在是返回-1</span></span><br><span class="line"><span class="string">    如string.rfind("a sd fg a","a"),输出为：8</span></span><br><span class="line"><span class="string">    如string.rfind("a sd fg a fg","sdsdfg"[4:5]),输出为:10</span></span><br><span class="line"><span class="string">    Return the highest index in s where substring sub is found,</span></span><br><span class="line"><span class="string">    such that sub is contained within s[start,end].  Optional</span></span><br><span class="line"><span class="string">    arguments start and end are interpreted as in slice notation.</span></span><br><span class="line"><span class="string">    Return -1 on failure.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rfind(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for a bit of speed</span></span><br><span class="line">_float = float</span><br><span class="line">_int = int</span><br><span class="line">_long = long</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert string to float</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atof</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""atof(s) -&gt; float</span></span><br><span class="line"><span class="string">    Return the floating point number represented by the string s.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _float(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert string to integer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atoi</span><span class="params">(s , base=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">"""atoi(s [,base]) -&gt; int</span></span><br><span class="line"><span class="string">    Return the integer represented by the string s in the given</span></span><br><span class="line"><span class="string">    base, which defaults to 10.  The string s must consist of one</span></span><br><span class="line"><span class="string">    or more digits, possibly preceded by a sign.  If base is 0, it</span></span><br><span class="line"><span class="string">    is chosen from the leading characters of s, 0 for octal, 0x or</span></span><br><span class="line"><span class="string">    0X for hexadecimal.  If base is 16, a preceding 0x or 0X is</span></span><br><span class="line"><span class="string">    accepted.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _int(s, base)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert string to long integer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atol</span><span class="params">(s, base=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">"""atol(s [,base]) -&gt; long</span></span><br><span class="line"><span class="string">    Return the long integer represented by the string s in the</span></span><br><span class="line"><span class="string">    given base, which defaults to 10.  The string s must consist</span></span><br><span class="line"><span class="string">    of one or more digits, possibly preceded by a sign.  If base</span></span><br><span class="line"><span class="string">    is 0, it is chosen from the leading characters of s, 0 for</span></span><br><span class="line"><span class="string">    octal, 0x or 0X for hexadecimal.  If base is 16, a preceding</span></span><br><span class="line"><span class="string">    0x or 0X is accepted.  A trailing L or l is not accepted,</span></span><br><span class="line"><span class="string">    unless base is 0.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _long(s, base)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Left-justify a string</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ljust</span><span class="params">(s, width, *args)</span>:</span></span><br><span class="line">    <span class="string">"""ljust(s, width[, fillchar]) -&gt; string</span></span><br><span class="line"><span class="string">    返回s的左对齐的版本，在该场指定宽度，可以根据需要用空格填充。该字符串不会被截断。如果指定了fillchar,以此代替空格。</span></span><br><span class="line"><span class="string">    如string.ljust(" adf", 8,"s"),输出为： adfssss</span></span><br><span class="line"><span class="string">    如string.ljust(" adfsfsfsfa", 3,"s"),(体现出不被截断)输出为 adfsfsfsfa</span></span><br><span class="line"><span class="string">    Return a left-justified version of s, in a field of the</span></span><br><span class="line"><span class="string">    specified width, padded with spaces as needed.  The string is</span></span><br><span class="line"><span class="string">    never truncated.  If specified the fillchar is used instead of spaces.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.ljust(width, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Right-justify a string</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rjust</span><span class="params">(s, width, *args)</span>:</span></span><br><span class="line">    <span class="string">"""rjust(s, width[, fillchar]) -&gt; string</span></span><br><span class="line"><span class="string">    返回s的右对齐的版本，在该场指定宽度，可以根据需要用空格填充。该字符串不会被截断。如果指定了fillchar,以此代替空格。</span></span><br><span class="line"><span class="string">    如string.rjust(" adf", 8,"s"),输出为：ssss adf</span></span><br><span class="line"><span class="string">    如string.rjust(" adfsfsfsfa", 3,"s"),(体现出不被截断)输出为 adfsfsfsfa</span></span><br><span class="line"><span class="string">    Return a right-justified version of s, in a field of the</span></span><br><span class="line"><span class="string">    specified width, padded with spaces as needed.  The string is</span></span><br><span class="line"><span class="string">    never truncated.  If specified the fillchar is used instead of spaces.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rjust(width, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Center a string</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">center</span><span class="params">(s, width, *args)</span>:</span></span><br><span class="line">    <span class="string">"""center(s, width[, fillchar]) -&gt; string</span></span><br><span class="line"><span class="string">    返回s的中心对齐的版本，在该场指定宽度，可以根据需要用空格填充。该字符串不会被截断。如果指定了fillchar,以此代替空格。</span></span><br><span class="line"><span class="string">    如string.center(" adf", 9,"s"),输出为：sss adfss</span></span><br><span class="line"><span class="string">    如string.center(" adfsfsfsfa", 3,"s"),(体现出不被截断)输出为 adfsfsfsfa</span></span><br><span class="line"><span class="string">    Return a center version of s, in a field of the specified</span></span><br><span class="line"><span class="string">    width. padded with spaces as needed.  The string is never</span></span><br><span class="line"><span class="string">    truncated.  If specified the fillchar is used instead of spaces.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.center(width, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Zero-fill a number, e.g., (12, 3) --&gt; '012' and (-3, 3) --&gt; '-03'</span></span><br><span class="line"><span class="comment"># Decadent feature: the argument may be a string or a number</span></span><br><span class="line"><span class="comment"># (Use of this is deprecated; it should be a string as with ljust c.s.)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zfill</span><span class="params">(x, width)</span>:</span></span><br><span class="line">    <span class="string">"""zfill(x, width) -&gt; string</span></span><br><span class="line"><span class="string">    在字符串x左边，填充0达到指定的宽度。该字符串x不会被截断。</span></span><br><span class="line"><span class="string">    如string.zfill(" adf", 9),输出为：00000 adf</span></span><br><span class="line"><span class="string">    Pad a numeric string x with zeros on the left, to fill a field</span></span><br><span class="line"><span class="string">    of the specified width.  The string x is never truncated.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(x, basestring):</span><br><span class="line">        x = repr(x)</span><br><span class="line">    <span class="keyword">return</span> x.zfill(width)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Expand tabs in a string.</span></span><br><span class="line"><span class="comment"># Doesn't take non-printing chars into account, but does understand \n.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expandtabs</span><span class="params">(s, tabsize=<span class="number">8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""expandtabs(s [,tabsize]) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s的副本，所有的制表符(tab)由适当数量的空格替代，取决于当前列和制表符大小（默认为8）</span></span><br><span class="line"><span class="string">    Return a copy of the string s with all tab characters replaced</span></span><br><span class="line"><span class="string">    by the appropriate number of spaces, depending on the current</span></span><br><span class="line"><span class="string">    column, and the tabsize (default 8).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.expandtabs(tabsize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Character translation through look-up table.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(s, table, deletions=<span class="string">""</span>)</span>:</span></span><br><span class="line">    <span class="string">"""translate(s,table [,deletions]) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s的副本，可选参数deletions出现的所有字符被删除，剩下的字符通过给定的转换表来映射，</span></span><br><span class="line"><span class="string">    转换表必须是长度为256的字符串.deletions不允许Unicode字符串</span></span><br><span class="line"><span class="string">    如t=string.maketrans('abc','ABC'),string.translate("abc123",t,'12'),输出为：ABC3</span></span><br><span class="line"><span class="string">    Return a copy of the string s, where all characters occurring</span></span><br><span class="line"><span class="string">    in the optional argument deletions are removed, and the</span></span><br><span class="line"><span class="string">    remaining characters have been mapped through the given</span></span><br><span class="line"><span class="string">    translation table, which must be a string of length 256.  The</span></span><br><span class="line"><span class="string">    deletions argument is not allowed for Unicode strings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> deletions <span class="keyword">or</span> table <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> s.translate(table, deletions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Add s[:0] so that if s is Unicode and table is an 8-bit string,</span></span><br><span class="line">        <span class="comment"># table is converted to Unicode.  This means that table *cannot*</span></span><br><span class="line">        <span class="comment"># be a dictionary -- for that feature, use u.translate() directly.</span></span><br><span class="line">        <span class="keyword">return</span> s.translate(table + s[:<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Capitalize a string, e.g. "aBc  dEf" -&gt; "Abc  def".</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">capitalize</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""capitalize(s) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s的副本，只有首字符大写</span></span><br><span class="line"><span class="string">    Return a copy of the string s with only its first character</span></span><br><span class="line"><span class="string">    capitalized.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.capitalize()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Substring replacement (global)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace</span><span class="params">(s, old, new, maxreplace=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""replace (str, old, new[, maxreplace]) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串str的副本，以子串new代替所有出现的子串old。如果可选参数maxreplace给出，只有第一个maxreplace出现的地方被替换</span></span><br><span class="line"><span class="string">    如string.replace("old ffa old fsda old", "old", "new",2),输出为：new ffa new fsda old</span></span><br><span class="line"><span class="string">    Return a copy of string str with all occurrences of substring</span></span><br><span class="line"><span class="string">    old replaced by new. If the optional argument maxreplace is</span></span><br><span class="line"><span class="string">    given, only the first maxreplace occurrences are replaced.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.replace(old, new, maxreplace)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Try importing optional built-in module "strop" -- if it exists,</span></span><br><span class="line"><span class="comment"># it redefines some string operations that are 100-1000 times faster.</span></span><br><span class="line"><span class="comment"># It also defines values for whitespace, lowercase and uppercase</span></span><br><span class="line"><span class="comment"># that match &lt;ctype.h&gt;'s definitions.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> strop <span class="keyword">import</span> maketrans, lowercase, uppercase, whitespace</span><br><span class="line">    letters = lowercase + uppercase</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">pass</span>                                          <span class="comment"># Use the original versions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########################################################################</span></span><br><span class="line"><span class="comment"># the Formatter class</span></span><br><span class="line"><span class="comment"># see PEP 3101 for details and purpose of this class</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The hard parts are reused from the C implementation.  They're exposed as "_"</span></span><br><span class="line"><span class="comment"># prefixed methods of str and unicode.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The overall parser is implemented in str._formatter_parser.</span></span><br><span class="line"><span class="comment"># The field name parser is implemented in str._formatter_field_name_split</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Formatter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">format</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"descriptor 'format' of 'Formatter' object "</span></span><br><span class="line">                            <span class="string">"needs an argument"</span>)</span><br><span class="line">        self, args = args[<span class="number">0</span>], args[<span class="number">1</span>:]  <span class="comment"># allow the "self" keyword be passed</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            format_string, args = args[<span class="number">0</span>], args[<span class="number">1</span>:] <span class="comment"># allow the "format_string" keyword be passed</span></span><br><span class="line">        <span class="keyword">except</span> IndexError:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'format_string'</span> <span class="keyword">in</span> kwargs:</span><br><span class="line">                format_string = kwargs.pop(<span class="string">'format_string'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> TypeError(<span class="string">"format() missing 1 required positional "</span></span><br><span class="line">                                <span class="string">"argument: 'format_string'"</span>)</span><br><span class="line">        <span class="keyword">return</span> self.vformat(format_string, args, kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vformat</span><span class="params">(self, format_string, args, kwargs)</span>:</span></span><br><span class="line">        used_args = set()</span><br><span class="line">        result = self._vformat(format_string, args, kwargs, used_args, <span class="number">2</span>)</span><br><span class="line">        self.check_unused_args(used_args, args, kwargs)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_vformat</span><span class="params">(self, format_string, args, kwargs, used_args, recursion_depth)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> recursion_depth &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Max string recursion exceeded'</span>)</span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> literal_text, field_name, format_spec, conversion <span class="keyword">in</span> \</span><br><span class="line">                self.parse(format_string):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># output the literal text</span></span><br><span class="line">            <span class="keyword">if</span> literal_text:</span><br><span class="line">                result.append(literal_text)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># if there's a field, output it</span></span><br><span class="line">            <span class="keyword">if</span> field_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="comment"># this is some markup, find the object and do</span></span><br><span class="line">                <span class="comment">#  the formatting</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># given the field_name, find the object it references</span></span><br><span class="line">                <span class="comment">#  and the argument it came from</span></span><br><span class="line">                obj, arg_used = self.get_field(field_name, args, kwargs)</span><br><span class="line">                used_args.add(arg_used)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># do any conversion on the resulting object</span></span><br><span class="line">                obj = self.convert_field(obj, conversion)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># expand the format spec, if needed</span></span><br><span class="line">                format_spec = self._vformat(format_spec, args, kwargs,</span><br><span class="line">                                            used_args, recursion_depth<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># format the object and append to the result</span></span><br><span class="line">                result.append(self.format_field(obj, format_spec))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span><span class="params">(self, key, args, kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(key, (int, long)):</span><br><span class="line">            <span class="keyword">return</span> args[key]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> kwargs[key]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_unused_args</span><span class="params">(self, used_args, args, kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">format_field</span><span class="params">(self, value, format_spec)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> format(value, format_spec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_field</span><span class="params">(self, value, conversion)</span>:</span></span><br><span class="line">        <span class="comment"># do any conversion on the resulting object</span></span><br><span class="line">        <span class="keyword">if</span> conversion <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> value</span><br><span class="line">        <span class="keyword">elif</span> conversion == <span class="string">'s'</span>:</span><br><span class="line">            <span class="keyword">return</span> str(value)</span><br><span class="line">        <span class="keyword">elif</span> conversion == <span class="string">'r'</span>:</span><br><span class="line">            <span class="keyword">return</span> repr(value)</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Unknown conversion specifier &#123;0!s&#125;"</span>.format(conversion))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># returns an iterable that contains tuples of the form:</span></span><br><span class="line">    <span class="comment"># (literal_text, field_name, format_spec, conversion)</span></span><br><span class="line">    <span class="comment"># literal_text can be zero length</span></span><br><span class="line">    <span class="comment"># field_name can be None, in which case there's no</span></span><br><span class="line">    <span class="comment">#  object to format and output</span></span><br><span class="line">    <span class="comment"># if field_name is not None, it is looked up, formatted</span></span><br><span class="line">    <span class="comment">#  with format_spec and conversion and then used</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, format_string)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> format_string._formatter_parser()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># given a field_name, find the object it references.</span></span><br><span class="line">    <span class="comment">#  field_name:   the field being looked up, e.g. "0.name"</span></span><br><span class="line">    <span class="comment">#                 or "lookup[3]"</span></span><br><span class="line">    <span class="comment">#  used_args:    a set of which args have been used</span></span><br><span class="line">    <span class="comment">#  args, kwargs: as passed in to vformat</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_field</span><span class="params">(self, field_name, args, kwargs)</span>:</span></span><br><span class="line">        first, rest = field_name._formatter_field_name_split()</span><br><span class="line"></span><br><span class="line">        obj = self.get_value(first, args, kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loop through the rest of the field_name, doing</span></span><br><span class="line">        <span class="comment">#  getattr or getitem as needed</span></span><br><span class="line">        <span class="keyword">for</span> is_attr, i <span class="keyword">in</span> rest:</span><br><span class="line">            <span class="keyword">if</span> is_attr:</span><br><span class="line">                obj = getattr(obj, i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                obj = obj[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> obj, first</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/12/Python/Python_shutil/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/12/Python/Python_shutil/" class="post-title-link" itemprop="url">python之Shutil</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-12 10:48:11" itemprop="dateCreated datePublished" datetime="2018-03-12T10:48:11+08:00">2018-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-10-12 15:08:00" itemprop="dateModified" datetime="2018-10-12T15:08:00+08:00">2018-10-12</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>#python之Shutil文件、文件夹高级操作</p>
<h2 id="复制操作"><a href="#复制操作" class="headerlink" title="复制操作"></a>复制操作</h2><h3 id="copyfileobj"><a href="#copyfileobj" class="headerlink" title="copyfileobj"></a>copyfileobj</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.copyfileobj(fsrc, fdst[, length=<span class="number">16</span>\*<span class="number">1024</span>])</span><br></pre></td></tr></table></figure>
<p>copy文件内容到另一个文件，可以copy指定大小的内容。这个方法是shutil模块中其它拷贝方法的基础，其它方法在本质上都是调用这个方法。</p>
<p>让我们看一下它的源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copyfileobj</span><span class="params">(fsrc, fdst, length=<span class="number">16</span>*<span class="number">1024</span>)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">        buf = fsrc.read(length)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> buf:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        fdst.write(buf)</span><br></pre></td></tr></table></figure>
<p>代码很简单，一看就懂。但是要注意，其中的fsrc，fdst都是使用open()方法打开后的文件对象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line">s =open(<span class="string">'fsrc.txt'</span>,<span class="string">'r'</span>)</span><br><span class="line">d=open(<span class="string">'fdst.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line">shutil.copyfileobj(s,d,length=<span class="number">16</span>*<span class="number">1024</span>)</span><br></pre></td></tr></table></figure>
<h3 id="copyfile"><a href="#copyfile" class="headerlink" title="copyfile"></a>copyfile</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.copyfile(src, dst)</span><br></pre></td></tr></table></figure>
<p>拷贝整个文件。同样看下它的源码，忽略前面一些检测用的代码，该方法的核心在最后几行，我们可以很清楚地看到<code>copyfile()</code>方法对<code>copyfileobj()</code>进行了调用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">copyfile</span><span class="params">(src, dst, *, follow_symlinks=True)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> _samefile(src, dst):</span><br><span class="line">        <span class="keyword">raise</span> SameFileError(<span class="string">"&#123;!r&#125; and &#123;!r&#125; are the same file"</span>.format(src, dst))</span><br><span class="line">    <span class="keyword">for</span> fn <span class="keyword">in</span> [src, dst]:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            st = os.stat(fn)</span><br><span class="line">        <span class="keyword">except</span> OSError:      </span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> stat.S_ISFIFO(st.st_mode):</span><br><span class="line">                <span class="keyword">raise</span> SpecialFileError(<span class="string">"`%s` is a named pipe"</span> % fn)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> follow_symlinks <span class="keyword">and</span> os.path.islink(src):</span><br><span class="line">        os.symlink(os.readlink(src), dst)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">with</span> open(src, <span class="string">'rb'</span>) <span class="keyword">as</span> fsrc:</span><br><span class="line">            <span class="keyword">with</span> open(dst, <span class="string">'wb'</span>) <span class="keyword">as</span> fdst:</span><br><span class="line">                copyfileobj(fsrc, fdst)</span><br><span class="line">    <span class="keyword">return</span> dst</span><br></pre></td></tr></table></figure>
<h3 id="copymode"><a href="#copymode" class="headerlink" title="copymode"></a>copymode</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.copymode(src, dst)</span><br></pre></td></tr></table></figure>
<p>仅拷贝权限。内容、组、用户均不变。</p>
<h3 id="copystat"><a href="#copystat" class="headerlink" title="copystat"></a>copystat</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.copystat(src, dst)</span><br></pre></td></tr></table></figure>
<p>仅复制所有的状态信息，包括权限，组，用户，时间等。</p>
<h3 id="copy"><a href="#copy" class="headerlink" title="copy"></a>copy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.copy(src,dst)</span><br></pre></td></tr></table></figure>
<p>同时复制文件的内容以及权限，也就是先copyfile()然后copymode()。</p>
<h3 id="copy2"><a href="#copy2" class="headerlink" title="copy2"></a>copy2</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.copy2(src, dst)</span><br></pre></td></tr></table></figure>
<p>同时复制文件的内容以及文件的所有状态信息。先copyfile()后copystat()。</p>
<h3 id="ignore-patterns"><a href="#ignore-patterns" class="headerlink" title="ignore_patterns"></a>ignore_patterns</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.ignore_patterns(*patterns)</span><br></pre></td></tr></table></figure>
<p>忽略指定的文件。通常配合下面的copytree()方法使用。</p>
<h3 id="copytree"><a href="#copytree" class="headerlink" title="copytree"></a>copytree</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">shutil.copytree(src, dst, symlinks=<span class="keyword">False</span>,ignore=<span class="keyword">None</span>,copy_function=copy2,</span><br><span class="line">                ignore_dangling_symlinks=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>递归地复制目录及其子目录的文件和状态信息</p>
<ul>
<li>symlinks：指定是否复制软链接。小心陷入死循环。</li>
<li>ignore：指定不参与复制的文件，其值应该是一个ignore_patterns()方法。</li>
<li>copy_function：指定复制的模式</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 典型用法</span></span><br><span class="line"><span class="keyword">from</span> shutil <span class="keyword">import</span> copytree, ignore_patterns</span><br><span class="line">copytree(source, destination, ignore=ignore_patterns(<span class="string">'*.pyc'</span>, <span class="string">'tmp*'</span>))</span><br><span class="line">copytree(<span class="string">'folder1'</span>, <span class="string">'folder2'</span>, ignore=ignore_patterns(<span class="string">'*.pyc'</span>, <span class="string">'tmp*'</span>))</span><br><span class="line">copytree(<span class="string">'f1'</span>, <span class="string">'f2'</span>, symlinks=<span class="keyword">True</span>, ignore=ignore_patterns(<span class="string">'*.pyc'</span>, <span class="string">'tmp*'</span>))</span><br></pre></td></tr></table></figure>
<h2 id="移动删除操作"><a href="#移动删除操作" class="headerlink" title="移动删除操作"></a>移动删除操作</h2><h3 id="rmtree"><a href="#rmtree" class="headerlink" title="rmtree"></a>rmtree</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.rmtree(path[, ignore_errors[, onerror]])</span><br></pre></td></tr></table></figure>
<p>递归地删除目录及子目录内的文件。注意！<strong>该方法不会询问yes或no，被删除的文件也不会出现在回收站里，请务必小心！</strong>此函数与类似<code>os.removedirs</code> ，但是<code>os.removedirs</code> 不能删除非空文件夹</p>
<p>下面的例子在碰到只读文件时，尝试清除只读属性，然后再删除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, stat</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_readonly</span><span class="params">(func, path, _)</span>:</span></span><br><span class="line">    <span class="string">"去除文件的只读属性，尝试再次删除"</span></span><br><span class="line">    os.chmod(path, stat.S_IWRITE)</span><br><span class="line">    func(path)</span><br><span class="line">shutil.rmtree(directory, onerror=remove_readonly)</span><br></pre></td></tr></table></figure>
<h3 id="move"><a href="#move" class="headerlink" title="move"></a>move</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.move(src, dst)</span><br></pre></td></tr></table></figure>
<p>递归地移动文件，类似mv命令，其实就是重命名。</p>
<h2 id="which"><a href="#which" class="headerlink" title="which"></a>which</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.which(cmd)</span><br></pre></td></tr></table></figure>
<p>类似linux的<code>which</code>命令，返回执行该命令的程序路径。Python3.3新增</p>
<h2 id="disk-usage"><a href="#disk-usage" class="headerlink" title="disk_usage"></a>disk_usage</h2><p>检测磁盘使用信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.disk_usage(&quot;d:/&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="解压缩"><a href="#解压缩" class="headerlink" title="解压缩"></a>解压缩</h2><h3 id="get-archive-formats"><a href="#get-archive-formats" class="headerlink" title="get_archive_formats"></a>get_archive_formats</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.get_archive_formats()</span><br></pre></td></tr></table></figure>
<p>获取当前系统已注册的归档文件格式（后缀）</p>
<blockquote>
<p>[(‘bztar’, “bzip2’ed tar-file”), </p>
<p>(‘gztar’, “gzip’ed tar-file”), </p>
<p>(‘tar’, ‘uncompressed tar file’),</p>
<p> (‘xztar’, “xz’ed tar-file”), </p>
<p>(‘zip’, ‘ZIP file’)]</p>
</blockquote>
<h3 id="make-archive"><a href="#make-archive" class="headerlink" title="make_archive"></a>make_archive</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.make_archive(base_name, format[, root_dir[, base_dir[, verbose[, dry_run[, owner[, group[, logger]]]]]]])</span><br></pre></td></tr></table></figure>
<p>创建归档或压缩文件。</p>
<ul>
<li><code>base_name</code>：压缩后的文件名。如果不指定绝对路径，则压缩文件保存在当前目录下。这个参数必须指定。</li>
<li><code>format</code>：压缩格式，可以是“zip”, “tar”, “bztar” ，“gztar”，“xztar”中的一种。这个参数也必须指定。</li>
<li><code>root_dir</code>：设置压缩包里的根目录，一般使用默认值，不特别指定。</li>
<li><code>base_dir</code>：要进行压缩的源文件或目录。</li>
<li><code>owner</code>：用户，默认当前用户。</li>
<li><code>group</code>：组，默认当前组。</li>
<li><code>logger</code>：用于记录日志，通常是<code>logging.Logger</code>对象。</li>
</ul>
<p>范例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line">shutil.make_archive(<span class="string">"d:\\3"</span>, <span class="string">"zip"</span>,  base_dir=<span class="string">"d:\\1.txt"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="get-unpack-formats"><a href="#get-unpack-formats" class="headerlink" title="get_unpack_formats"></a>get_unpack_formats</h3><p>获取当前系统已经注册的解包文件格式(后缀)</p>
<blockquote>
<p>[(‘bztar’, [‘.tar.bz2’, ‘.tbz2’], “bzip2’ed tar-file”), </p>
<p>(‘gztar’, [‘.tar.gz’, ‘.tgz’], “gzip’ed tar-file”), </p>
<p>(‘tar’, [‘.tar’], ‘uncompressed tar file’),</p>
<p> (‘xztar’, [‘.tar.xz’, ‘.txz’], “xz’ed tar-file”),</p>
<p> (‘zip’, [‘.zip’], ‘ZIP file’)]</p>
</blockquote>
<h3 id="unpack-archive"><a href="#unpack-archive" class="headerlink" title="unpack_archive"></a>unpack_archive</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutil.unpack_archive(filename[, extract_dir[, format]])</span><br></pre></td></tr></table></figure>
<p>解压缩或解包源文件。</p>
<ul>
<li>filename是压缩文档的完整路径</li>
<li>extract_dir是解压缩路径，默认为当前目录。</li>
<li>format是压缩格式。默认使用文件后缀名代码的压缩格式。</li>
</ul>
<p>范例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line">shutil.unpack_archive(<span class="string">"d:\\3.zip"</span>, <span class="string">"f:\\3"</span>, <span class="string">'zip'</span>)</span><br></pre></td></tr></table></figure>
<p>shutil模块的压缩和解压功能，在后台是通过调用zipfile和tarfile两个模块来进行的。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/12/Python/Python_webbrowser/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/12/Python/Python_webbrowser/" class="post-title-link" itemprop="url">python之webbrowser</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-12 10:48:11" itemprop="dateCreated datePublished" datetime="2018-03-12T10:48:11+08:00">2018-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-10-12 17:13:24" itemprop="dateModified" datetime="2018-10-12T17:13:24+08:00">2018-10-12</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="python之webbrowser打开浏览器"><a href="#python之webbrowser打开浏览器" class="headerlink" title="python之webbrowser打开浏览器"></a>python之webbrowser打开浏览器</h1><h2 id="使用默认浏览器显示网址"><a href="#使用默认浏览器显示网址" class="headerlink" title="使用默认浏览器显示网址"></a>使用默认浏览器显示网址</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.open(url, new=<span class="number">0</span>, autoraise=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="在默认浏览器的新窗口中打开URL"><a href="#在默认浏览器的新窗口中打开URL" class="headerlink" title="在默认浏览器的新窗口中打开URL"></a>在默认浏览器的新窗口中打开URL</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.open_new(url)</span><br></pre></td></tr></table></figure>
<h2 id="在默认浏览器的新页面（“标签”）中打开网址"><a href="#在默认浏览器的新页面（“标签”）中打开网址" class="headerlink" title="在默认浏览器的新页面（“标签”）中打开网址"></a>在默认浏览器的新页面（“标签”）中打开网址</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.open_new_tab(url)</span><br></pre></td></tr></table></figure>
<h2 id="指定浏览器打开"><a href="#指定浏览器打开" class="headerlink" title="指定浏览器打开"></a>指定浏览器打开</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.get(<span class="string">'chrome'</span>)</span><br></pre></td></tr></table></figure>
<p>如果name为空，则返回适用于调用者环境的默认浏览器的constructor</p>
<h2 id="注册浏览器类型名称。"><a href="#注册浏览器类型名称。" class="headerlink" title="注册浏览器类型名称。"></a>注册浏览器类型名称。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.register(name, constructor[, instance])</span><br></pre></td></tr></table></figure>
<p>注册浏览器类型后，get()函数可以返回该浏览器类型的constructor。如果未提供instance，或者为None，在需要时将在不使用参数的情况下调用constructor来创建实例。如果提供了实例，则永远不会调用constructor，并且可能为None</p>
<h2 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> webbrowser</span><br><span class="line"></span><br><span class="line">chromePath = <span class="string">r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"</span></span><br><span class="line">webbrowser.register(<span class="string">'chrome1'</span>, <span class="keyword">None</span>, webbrowser.BackgroundBrowser(chromePath))</span><br><span class="line"></span><br><span class="line">browser = webbrowser.get(<span class="string">'chrome1'</span>)</span><br><span class="line">browser.open(<span class="string">'https://heroinlin.github.io/'</span>)</span><br><span class="line">browser.open_new(<span class="string">'https://heroinlin.github.io/'</span>)</span><br><span class="line">browser.open_new_tab(<span class="string">'https://heroinlin.github.io/'</span>)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/12/Pytorch/pytorch_Tensor/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/12/Pytorch/pytorch_Tensor/" class="post-title-link" itemprop="url">Tensor的数学运算</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-12 10:48:11" itemprop="dateCreated datePublished" datetime="2018-03-12T10:48:11+08:00">2018-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-08-06 09:25:00" itemprop="dateModified" datetime="2018-08-06T09:25:00+08:00">2018-08-06</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>#<strong>Tensor的数学运算</strong></p>
<p>总结的方法包括：</p>
<p>Tensor求和以及按索引求和：<strong>torch.sum()         torch.Tensor.indexadd()</strong></p>
<p>Tensor元素乘积<strong>：torch.prod(input)</strong></p>
<p>对Tensor求均值、方差、极值：</p>
<p><strong>torch.mean()      torch.var()</strong></p>
<p><strong>torch.max()         torch.min()</strong></p>
<p>最后还有在NLP领域经常用到的：</p>
<p>求Tensor的平方根倒数、线性插值、双曲正切</p>
<p><strong>torch.rsqrt(input)     torch.lerp(star,end,weight)</strong></p>
<p><strong>torch.tanh(input, out=None)</strong></p>
<h2 id="元素求和"><a href="#元素求和" class="headerlink" title="元素求和"></a><strong>元素求和</strong></h2><p><strong>torch.sum(input)</strong> → float</p>
<p>返回输入向量input中所有元素的和。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_sum</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.sum(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.6491 -0.1617  0.7009<br>[torch.FloatTensor of size 1x3]</p>
<p>1.1884211152791977</p>
</blockquote>
<p><strong>torch.sum(input, dim, keepdim=False, out=None)</strong> → Tensor</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的和。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_sum_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.sum(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.8452 -0.2816  0.2672<br> 1.0685  2.4003 -0.6541<br>-0.1700 -0.4373 -0.2217<br>-1.2500  1.1798 -0.6842<br>[torch.FloatTensor of size 4x3]</p>
<p>-1.1968  2.8613 -1.2928<br>[torch.FloatTensor of size 1x3]</p>
</blockquote>
<h2 id="元素乘积"><a href="#元素乘积" class="headerlink" title="元素乘积"></a><strong>元素乘积</strong></h2><p><strong>torch.prod(input)</strong> → float</p>
<p>返回输入张量input所有元素的乘积。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_prod</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.prod(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.3751  0.3082 -0.7879<br>[torch.FloatTensor of size 1x3]</p>
<p>0.09109678113290456</p>
</blockquote>
<p><strong>torch.prod(input, dim, keepdim=False, out=None)</strong> → Tensor</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的乘积。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_prod_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.prod(a, <span class="number">1</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.2707  0.4322<br>-2.0925  0.3860<br>-1.1050  1.2551<br>-0.9644 -0.8771<br>[torch.FloatTensor of size 4x2]</p>
<p> 0.1170<br>-0.8077<br>-1.3869<br> 0.8459<br>[torch.FloatTensor of size 4x1]</p>
</blockquote>
<h2 id="按索引求和"><a href="#按索引求和" class="headerlink" title="按索引求和"></a><strong>按索引求和</strong></h2><p><strong>torch.Tensor.indexadd(dim, index, tensor)</strong> → Tensor</p>
<p>按索引参数index中所确定的顺序，将参数张量tensor中的元素与执行本方法的张量的元素逐个相加。参数tensor的尺寸必须严格地与执行方法的张量匹配，否则会发生错误。</p>
<p><strong>参数：</strong></p>
<ul>
<li>dim (int) - 索引index所指向的维度</li>
<li>index (LongTensor) - 包含索引数的张量</li>
<li>tensor (Tensor) - 含有相加元素的张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_index_add</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     x = torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">&gt;     print(x)</span><br><span class="line">&gt;     t = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">&gt;     index = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">&gt;     x.index_add_(<span class="number">0</span>, index, t)</span><br><span class="line">&gt;     print(x)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>Output:</p>
<p>1  1  1<br> 1  1  1<br> 1  1  1<br>[torch.FloatTensor of size 3x3]</p>
<p>  2   3   4<br>  8   9  10<br>  5   6   7<br>[torch.FloatTensor of size 3x3]</p>
</blockquote>
<h2 id="平均数"><a href="#平均数" class="headerlink" title="平均数"></a><strong>平均数</strong></h2><p><strong>torch.mean(input)</strong></p>
<p>返回输入张量input中每个元素的平均值。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_mean</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.mean(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.9257 -0.1373  1.5762<br>[torch.FloatTensor of size 1x3]</p>
<p>0.788198247551918</p>
</blockquote>
<p><strong>torch.mean(input, dim, keepdim=False, out=None)</strong></p>
<p>返回新的张量，其中包含输入张量input指定维度dim中每行的平均值。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
<li>dim (int) - 指定进行均值计算的维度</li>
<li>keepdim (bool, optional) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (Tensor) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_mean_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.mean(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.3839 -1.2157  0.0210  1.1199  0.0319<br>-1.3452 -1.0125 -1.2500  1.0597 -0.9030<br> 0.1642  0.9110  0.8520  0.0481 -0.5234<br>[torch.FloatTensor of size 3x5]</p>
<p>-0.2657 -0.4391 -0.1257  0.7425 -0.4648<br>[torch.FloatTensor of size 1x5]</p>
</blockquote>
<h2 id="方差"><a href="#方差" class="headerlink" title="方差"></a><strong>方差</strong></h2><p><strong>torch.var(input, unbiased=True)</strong> → float</p>
<p>返回输入向量input中所有元素的方差。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
<li>unbiased (bool) - 是否使用基于修正贝塞尔函数的无偏估计</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_var</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.var(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.7808  0.1883  0.7654<br>[torch.FloatTensor of size 1x3]</p>
<p>0.6105006681458913</p>
</blockquote>
<p><strong>torch.var(input, dim, keepdim=False, unbiased=True, out=None)</strong> → Tensor</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的方差。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>unbiased (bool) - 是否使用基于修正贝塞尔函数的无偏估计</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_var_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.var(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.2789 -1.6603  1.1928  0.9614  1.2953<br> 0.9082  0.4015  1.1001 -0.9432  0.9254<br>-1.0593 -0.2636 -0.9274  0.0006  1.3933<br>[torch.FloatTensor of size 3x5]</p>
<p> 0.9815  1.1074  1.4358  0.9069  0.0609<br>[torch.FloatTensor of size 1x5]</p>
</blockquote>
<h2 id="最大值"><a href="#最大值" class="headerlink" title="最大值"></a><strong>最大值</strong></h2><p><strong>torch.max(input)</strong> → float</p>
<p>返回输入张量所有元素的最大值。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_max</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.max(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.8944  0.8395 -0.8867<br>[torch.FloatTensor of size 1x3]</p>
<p>0.8944145441055298</p>
</blockquote>
<p><strong>torch.max(input, dim, keepdim=False, out=None)</strong> -&gt; (Tensor, LongTensor)</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的最大值，同时返回每个最大值的位置索引。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (tuple,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_max_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.max(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.5306 -0.8915  2.7820  0.1723 -0.5061<br> 2.0535  0.2018 -3.1085 -0.7618  0.5924<br> 0.9906 -0.3212  0.1849  1.7002  0.8102<br>[torch.FloatTensor of size 3x5]</p>
<p>(<br> 2.0535  0.2018  2.7820  1.7002  0.8102<br>[torch.FloatTensor of size 1x5]<br>,<br> 1  1  0  2  2<br>[torch.LongTensor of size 1x5]<br>)</p>
</blockquote>
<p><strong>torch.max(input, other, out=None)</strong> → Tensor</p>
<p>逐个元素比较张量input与张量other，将比较出的最大值保存到输出张量中。<br>两个张量尺寸不需要完全相同，但需要支持自动扩展法则。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>other (Tensor) - 另一个输入的Tensor</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_max_2tensor</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">4</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.randn(<span class="number">1</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;     print(torch.max(a, b))</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>0.5369<br>-1.5926<br>-1.3574<br>-1.6009<br>[torch.FloatTensor of size 4]</p>
<p>-0.1394<br>[torch.FloatTensor of size 1]</p>
<p> 0.5369<br>-0.1394<br>-0.1394<br>-0.1394<br>[torch.FloatTensor of size 4]</p>
</blockquote>
<h2 id="最小值"><a href="#最小值" class="headerlink" title="最小值"></a><strong>最小值</strong></h2><p><strong>torch.min(input)</strong> → float</p>
<p>返回输入张量所有元素的最小值。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_min</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.min(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.8926  0.2079 -0.6790<br>[torch.FloatTensor of size 1x3]</p>
<p>-0.8925955295562744</p>
</blockquote>
<p><strong>torch.min(input, dim, keepdim=False, out=None)</strong> -&gt; (Tensor, LongTensor)</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的最小值，同时返回每个最小值的位置索引。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (tuple,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_min_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.min(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.8179  1.1834 -0.2989  0.6051 -0.1072<br>-1.1543  0.0666 -0.7919  0.2359  1.1995<br>-0.8094  0.5873  0.5116 -0.6181  0.9788<br>[torch.FloatTensor of size 3x5]</p>
<p>(<br>-1.1543  0.0666 -0.7919 -0.6181 -0.1072<br>[torch.FloatTensor of size 1x5]<br>,<br> 1  1  1  2  0<br>[torch.LongTensor of size 1x5]<br>)</p>
</blockquote>
<p><strong>torch.min(input, other, out=None)</strong> → Tensor</p>
<p>逐个元素比较张量input与张量other，将比较出的最小值保存到输出张量中。<br>两个张量尺寸不需要完全相同，但需要支持自动扩展法则。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>other (Tensor) - 另一个输入的Tensor</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_min_2tensor</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.randn(<span class="number">1</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;     print(torch.min(a, b))</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.3494  0.2155 -0.0723  0.8322<br>[torch.FloatTensor of size 1x4]</p>
<p> 0.2635<br>[torch.FloatTensor of size 1]</p>
<p> 0.2635  0.2155 -0.0723  0.2635<br>[torch.FloatTensor of size 1x4]</p>
</blockquote>
<h2 id="平方根倒数"><a href="#平方根倒数" class="headerlink" title="平方根倒数"></a><strong>平方根倒数</strong></h2><p><strong>torch.rsqrt(input)</strong> → Tensor</p>
<p>返回新的张量，其中包含input张量每个元素平方根的倒数。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_rsqrt</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.rsqrt(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.1615  0.3116 -0.3093 -1.5020<br>[torch.FloatTensor of size 1x4]</p>
<p> 2.4884  1.7915     nan     nan<br>[torch.FloatTensor of size 1x4]</p>
</blockquote>
<h2 id="线性插值"><a href="#线性插值" class="headerlink" title="线性插值"></a><strong>线性插值</strong></h2><p><strong>torch.lerp(start,end,weight)</strong> → Tensor</p>
<p>基于weight对输入的两个张量start与end逐个元素计算线性插值，结果返回至输出张量。</p>
<p>返回结果是： $outs_i=start_i+weight*(end_i-start_i)$</p>
<p><strong>参数：</strong></p>
<ul>
<li>start (Tensor) – 起始点张量</li>
<li>end (Tensor) – 终止点张量</li>
<li>weight (float) – 插入公式的 weight</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_lerp</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     start = torch.arange(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(start)</span><br><span class="line">&gt;     end = torch.Tensor(<span class="number">4</span>).fill_(<span class="number">10</span>)</span><br><span class="line">&gt;     print(end)</span><br><span class="line">&gt;     outs = torch.lerp(start, end, <span class="number">0.4</span>)</span><br><span class="line">&gt;     print(outs)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 1<br> 2<br> 3<br> 4<br>[torch.FloatTensor of size 4]</p>
<p> 10<br> 10<br> 10<br> 10<br>[torch.FloatTensor of size 4]</p>
<p> 4.6000<br> 5.2000<br> 5.8000<br> 6.4000<br>[torch.FloatTensor of size 4]</p>
</blockquote>
<h2 id="双曲正切"><a href="#双曲正切" class="headerlink" title="双曲正切"></a><strong>双曲正切</strong></h2><p><strong>torch.tanh(input, out=None)</strong> → Tensor</p>
<p>返回新的张量，其中包括输入张量input中每个元素的双曲正切。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_tanh</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     outs = torch.tanh(a)</span><br><span class="line">&gt;     print(outs)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.4298  0.0992  0.1322  1.4975  0.8817<br>[torch.FloatTensor of size 1x5]</p>
<p>-0.4051  0.0989  0.1314  0.9047  0.7073<br>[torch.FloatTensor of size 1x5]</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/07/C&C++/Debug_error_solution/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/07/C&C++/Debug_error_solution/" class="post-title-link" itemprop="url">C及C++编译时候出现的一些问题与解决方案</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-03-07 15:54:11" itemprop="dateCreated datePublished" datetime="2018-03-07T15:54:11+08:00">2018-03-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-03-12 10:54:18" itemprop="dateModified" datetime="2018-03-12T10:54:18+08:00">2018-03-12</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/C-C/" itemprop="url" rel="index"><span itemprop="name">C&C++</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="C及C-编译时候出现的一些问题与解决方案"><a href="#C及C-编译时候出现的一些问题与解决方案" class="headerlink" title="C及C++编译时候出现的一些问题与解决方案"></a>C及C++编译时候出现的一些问题与解决方案</h1><h2 id="字符串问题"><a href="#字符串问题" class="headerlink" title="字符串问题"></a>字符串问题</h2><ul>
<li><p>[no matching function for call to ‘std::basic_ofstream &gt;::basic_ofstream(std::string&amp;)’]<br>编译时候报错为no matching function for call to std::basic_ofstream&lt;char, std::char_traits<char> &gt;::basic_ofstream(std::string&amp;)</char></p>
<p>原因是C++的string类与C的字符串存在不同，一些函数无法将string类作为参数使用。</p>
<blockquote>
<p><a href="http://en.cppreference.com/w/cpp/io/basic_ofstream" target="_blank" rel="noopener"><code>std::ofstream</code></a> can only be constructed with a <code>std::string</code> if you have C++11 or higher. Typically that is done with <code>-std=c++11</code> (gcc, clang). If you do not have access to c++11 then you can use the <code>c_str()</code> function of <code>std::string</code> to pass a <code>const char *</code> to the <code>ofstream</code> constructor.</p>
</blockquote>
<p>解决方案：转换为C的字符串</p>
<p>1.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> filename[<span class="number">10</span>];  </span><br><span class="line"><span class="built_in">strcpy</span>(filename, <span class="string">"1.txt"</span>);  </span><br><span class="line">ifstream fin;  </span><br><span class="line">fin.open(filename);</span><br></pre></td></tr></table></figure>
<p>2.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="built_in">string</span> asegurado;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Nombre a agregar: "</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; asegurado;</span><br><span class="line"></span><br><span class="line">    <span class="function">ofstream <span class="title">entrada</span><span class="params">(asegurado,<span class="string">""</span>)</span></span>;<span class="comment">/*编译报错*/</span></span><br><span class="line">    <span class="comment">//ofstream entrada(asegurado); // C++11 or higher</span></span><br><span class="line">	<span class="comment">//ofstream entrada(asegurado.c_str());  // C++03 or below</span></span><br><span class="line">    <span class="keyword">if</span> (entrada.fail())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"El archivo no se creo correctamente"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Optimizer/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Optimizer/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：最优化笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-03-26 08:54:02" itemprop="dateModified" datetime="2018-03-26T08:54:02+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：最优化笔记"><a href="#CS231n课程笔记翻译：最优化笔记" class="headerlink" title="CS231n课程笔记翻译：最优化笔记"></a>CS231n课程笔记翻译：最优化笔记</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li><p>简介</p>
</li>
<li><p>损失函数可视化</p>
</li>
<li><p>最优化</p>
<ul>
<li>策略#1：随机搜索</li>
</ul>
</li>
</ul>
<ul>
<li>策略#2：随机局部搜索</li>
<li>策略#3：跟随梯度 </li>
</ul>
<ul>
<li><p>梯度计算</p>
<ul>
<li>使用有限差值进行数值计算</li>
</ul>
</li>
</ul>
<ul>
<li>微分计算梯度</li>
</ul>
<ul>
<li><p>梯度下降</p>
</li>
<li><p>小结</p>
</li>
</ul>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在上一节中，我们介绍了图像分类任务中的两个关键部分：</p>
<ol>
<li>基于参数的<strong>评分函数。</strong>该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。</li>
<li><strong>损失函数</strong>。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。</li>
</ol>
<p>上节中，线性函数的形式是$f(x_i, W)=Wx_i$，而SVM实现的公式是：</p>
<center>$$L=\displaystyle\frac{1}{N}\sum_i\sum_{j\not= y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)]+\alpha R(W)$$</center>

<p>对于图像数据$x_i$，如果基于参数集$W$做出的分类预测与真实情况比较一致，那么计算出来的损失值$L$)就很低。现在介绍第三个，也是最后一个关键部分：<strong>最优化Optimization</strong>。最优化是寻找能使得损失函数值最小化的参数$W$的过程。</p>
<p><strong>铺垫</strong>：一旦理解了这三个部分是如何相互运作的，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化过程这两个部分将会保持相对稳定。</p>
<h2 id="损失函数可视化"><a href="#损失函数可视化" class="headerlink" title="损失函数可视化"></a>损失函数可视化</h2><p>本课中讨论的损失函数一般都是定义在高维度的空间中（比如，在CIFAR-10中一个线性分类器的权重矩阵大小是[10x3073]，就有30730个参数），这样要将其可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，就能得到一些直观感受。例如，随机生成一个权重矩阵$W$，该矩阵就与高维空间中的一个点对应。然后沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向$W_1$并且沿着此方向计算损失值，计算方法是根据不同的$a$值来计算$L(W+aW_1)$。这个过程将生成一个图表，其x轴是$a$值，y轴是损失函数值。同样的方法还可以用在两个维度上，通过改变$a,b$来计算损失值$L(W+aW_1+bW_2)$，从而给出二维的图像。在图像中，$a,b$可以分别用x和y轴表示，而损失函数的值可以用颜色变化表示：</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_0.jpg?raw=true" width="350"></center>

<p>一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的100个数据。<strong>左</strong>：$a$值变化在某个维度方向上对应的的损失值变化。<strong>中和右</strong>：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。</p>
<p>—————————————————————————————————————————</p>
<p>我们可以通过数学公式来解释损失函数的分段线性结构。对于一个单独的数据，有损失函数的计算公式如下：</p>
<center>$$Li=\sum_{j\not=y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+1)]$$</center>

<p>通过公式可见，每个样本的数据损失值是以$W$为参数的线性函数的总和（零阈值来源于$max(0,-)$函数）。$W$的每一行（即$w_j$），有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有3个只有1个维度的点，数据集数据点有3个类别。那么完整的无正则化SVM的损失值计算如下：</p>
<center>$$L_0=max(0,w^T_1x_0-w^T_0x_0+1)+max(0,w^T_2x_0-w^T_0x_0+1)$$</center><br><center>$$L_1=max(0,w^T_0x_1-w^T_1x_1+1)+max(0,w^T_2x_1-w^T_1x_1+1)$$</center><br><center>$$L_2=max(0,w^T_0x_2-w^T_2x_2+1)+max(0,w^T_1x_2-w^T_2x_2+1)$$</center><br><center>$$L=(L_0+L_1+L_2)/3$$</center>

<p>因为这些例子都是一维的，所以数据$x_i$和权重$w_j$都是数字。观察$w_0$，可以看到上面的式子中一些项是$w_0$的线性函数，且每一项都会与0比较，取两者的最大值。可作图如下：——————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_1.png?raw=true" width="350"></center>

<p>从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。完整的SVM数据损失就是这个形状的30730维版本。</p>
<p>——————————————————————————————————————</p>
<p>需要多说一句的是，你可能根据SVM的损失函数的碗状外观猜出它是一个<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Convex_function" target="_blank" rel="noopener"><strong>凸函数</strong></a>。关于如何高效地最小化凸函数的论文有很多，你也可以学习斯坦福大学关于（<a href="http://link.zhihu.com/?target=http%3A//stanford.edu/%7Eboyd/cvxbook/" target="_blank" rel="noopener"><strong>凸函数最优化</strong></a>）的课程。但是一旦我们将<img src="http://www.zhihu.com/equation?tex=f" alt="f">函数扩展到神经网络，目标函数就就不再是凸函数了，图像也不会像上面那样是个碗状，而是凹凸不平的复杂地形形状。</p>
<p><em>不可导的损失函数。</em>作为一个技术笔记，你要注意到：由于max操作，损失函数中存在一些<em>不可导点（kinks），</em>这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Subderivative" target="_blank" rel="noopener"><strong>次梯度（subgradient）</strong></a>依然存在且常常被使用。在本课中，我们将交换使用<em>次梯度</em>和<em>梯度</em>两个术语。</p>
<h2 id="最优化-Optimization"><a href="#最优化-Optimization" class="headerlink" title="最优化 Optimization"></a>最优化 Optimization</h2><p>重申一下：损失函数可以量化某个具体权重集<strong>W</strong>的质量。而最优化的目标就是找到能够最小化损失函数值的<strong>W</strong> 。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于有一些经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM 损失函数）是一个凸函数问题。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。</p>
<h3 id="策略-1：一个差劲的初始方案：随机搜索"><a href="#策略-1：一个差劲的初始方案：随机搜索" class="headerlink" title="策略#1：一个差劲的初始方案：随机搜索"></a><strong>策略#1：一个差劲的初始方案：随机搜索</strong></h3><p>既然确认参数集<strong>W</strong>的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）</span></span><br><span class="line"><span class="comment"># 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）</span></span><br><span class="line"><span class="comment"># 假设函数L对损失函数进行评价</span></span><br><span class="line"></span><br><span class="line">bestloss = float(<span class="string">"inf"</span>) <span class="comment"># Python assigns the highest possible float value</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.0001</span> <span class="comment"># generate random parameters</span></span><br><span class="line">  loss = L(X_train, Y_train, W) <span class="comment"># get the loss over the entire training set</span></span><br><span class="line">  <span class="keyword">if</span> loss &lt; bestloss: <span class="comment"># keep track of the best solution</span></span><br><span class="line">    bestloss = loss</span><br><span class="line">    bestW = W</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'in attempt %d the loss was %f, best %f'</span> % (num, loss, bestloss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># in attempt 0 the loss was 9.401632, best 9.401632</span></span><br><span class="line"><span class="comment"># in attempt 1 the loss was 8.959668, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 2 the loss was 9.044034, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 3 the loss was 9.278948, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 4 the loss was 8.857370, best 8.857370</span></span><br><span class="line"><span class="comment"># in attempt 5 the loss was 8.943151, best 8.857370</span></span><br><span class="line"><span class="comment"># in attempt 6 the loss was 8.605604, best 8.605604</span></span><br><span class="line"><span class="comment"># ... (trunctated: continues for 1000 lines)</span></span><br></pre></td></tr></table></figure>
<p>在上面的代码中，我们尝试了若干随机生成的权重矩阵<strong>W</strong>，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重<strong>W</strong>取出，然后去跑测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]</span></span><br><span class="line">scores = Wbest.dot(Xte_cols) <span class="comment"># 10 x 10000, the class scores for all test examples</span></span><br><span class="line"><span class="comment"># 找到在每列中评分值最大的索引（即预测的分类）</span></span><br><span class="line">Yte_predict = np.argmax(scores, axis = <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 以及计算准确率</span></span><br><span class="line">np.mean(Yte_predict == Yte)</span><br><span class="line"><span class="comment"># 返回 0.1555</span></span><br></pre></td></tr></table></figure>
<p>验证集上表现最好的权重<strong>W</strong>跑测试集的准确率是<strong>15.5%，</strong>而完全随机猜的准确率是10%，如此看来，这个准确率对于这样一个不经过大脑的策略来说，还算不错嘛！</p>
<p><strong>核心思路：迭代优化</strong>。当然，我们肯定能做得更好些。核心思路是：虽然找到最优的权重<strong>W</strong>非常困难，甚至是不可能的（尤其当<strong>W</strong>中存的是整个神经网络的权重的时候），但如果问题转化为：对一个权重矩阵集<strong>W</strong>取优，使其损失值稍微减少。那么问题的难度就大大降低了。换句话说，我们的方法从一个随机的<strong>W</strong>开始，然后对其迭代取优，每次都让它的损失值变得更小一点。</p>
<blockquote>
<p>我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。</p>
</blockquote>
<p><strong>蒙眼徒步者的比喻</strong>：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为<strong>W</strong>是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。</p>
<h3 id="策略-2：随机本地搜索"><a href="#策略-2：随机本地搜索" class="headerlink" title="策略#2：随机本地搜索"></a><strong>策略#2：随机本地搜索</strong></h3><p>第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个随机的扰动$\delta W$ ，只有当$W+\delta W$的损失值变低，我们才会更新。这个过程的具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 生成随机初始W</span></span><br><span class="line">bestloss = float(<span class="string">"inf"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  step_size = <span class="number">0.0001</span></span><br><span class="line">  Wtry = W + np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * step_size</span><br><span class="line">  loss = L(Xtr_cols, Ytr, Wtry)</span><br><span class="line">  <span class="keyword">if</span> loss &lt; bestloss:</span><br><span class="line">    W = Wtry</span><br><span class="line">    bestloss = loss</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'iter %d loss is %f'</span> % (i, bestloss)</span><br></pre></td></tr></table></figure>
<p>使用同样的数据（1000），这个方法可以得到<strong>21.4%</strong>的分类准确率。这个比策略一好，但是依然过于浪费计算资源。</p>
<h3 id="策略-3：跟随梯度"><a href="#策略-3：跟随梯度" class="headerlink" title="策略#3：跟随梯度"></a><strong>策略#3：跟随梯度</strong></h3><p>前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的<strong>梯度（gradient）</strong>。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。</p>
<p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数<strong>derivatives</strong>）。对一维函数的求导公式如下：</p>
<center>$$\displaystyle\frac{df(x)}{dx}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$</center>

<p>当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p>
<h2 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h2><p>计算梯度有两种方法：一个是缓慢的近似方法（<strong>数值梯度法</strong>），但实现相对简单。另一个方法（<strong>分析梯度法</strong>）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对两种方法进行介绍：</p>
<h3 id="利用有限差值计算梯度"><a href="#利用有限差值计算梯度" class="headerlink" title="利用有限差值计算梯度"></a><strong>利用有限差值计算梯度</strong></h3><p>上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数<strong>f</strong>和向量<strong>x，</strong>计算<strong>f</strong>的梯度的通用函数，它返回函数<strong>f</strong>在点<strong>x处</strong>的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_numerical_gradient</span><span class="params">(f, x)</span>:</span></span><br><span class="line">  <span class="string">"""  </span></span><br><span class="line"><span class="string">  一个f在x处的数值梯度法的简单实现</span></span><br><span class="line"><span class="string">  - f是只有一个参数的函数</span></span><br><span class="line"><span class="string">  - x是计算梯度的点</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line"></span><br><span class="line">  fx = f(x) <span class="comment"># 在原点计算函数值</span></span><br><span class="line">  grad = np.zeros(x.shape)</span><br><span class="line">  h = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对x中所有的索引进行迭代</span></span><br><span class="line">  it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算x+h处的函数值</span></span><br><span class="line">    ix = it.multi_index</span><br><span class="line">    old_value = x[ix]</span><br><span class="line">    x[ix] = old_value + h <span class="comment"># 增加h</span></span><br><span class="line">    fxh = f(x) <span class="comment"># 计算f(x + h)</span></span><br><span class="line">    x[ix] = old_value <span class="comment"># 存到前一个值中 (非常重要)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算偏导数</span></span><br><span class="line">    grad[ix] = (fxh - fx) / h <span class="comment"># 坡度</span></span><br><span class="line">    it.iternext() <span class="comment"># 到下个维度</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<p>根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化h，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量<strong>grad</strong>中。</p>
<p><strong>实践考量</strong>：注意在数学公式中，<strong>h</strong>的取值是趋近于0的，然而在实际中，用一个很小的数值（比如例子中的1e-5）就足够了。在不产生数值计算出错的理想前提下，你会使用尽可能小的h。还有，实际中用<strong>中心差值公式（centered difference formula)</strong> $[f(x+h)-f(x-h)]/2h$ 效果较好。细节可查看<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Numerical_differentiation" target="_blank" rel="noopener"><strong>wiki</strong></a>。</p>
<p>可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要使用上面的代码我们需要一个只有一个参数的函数</span></span><br><span class="line"><span class="comment"># (在这里参数就是权重)所以也包含了X_train和Y_train</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CIFAR10_loss_fun</span><span class="params">(W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> L(X_train, Y_train, W)</span><br><span class="line"></span><br><span class="line">W = np.random.rand(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 随机权重向量</span></span><br><span class="line">df = eval_numerical_gradient(CIFAR10_loss_fun, W) <span class="comment"># 得到梯度</span></span><br></pre></td></tr></table></figure>
<p>梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">loss_original = CIFAR10_loss_fun(W) <span class="comment"># 初始损失值</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'original loss: %f'</span> % (loss_original, )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同步长的效果</span></span><br><span class="line"><span class="keyword">for</span> step_size_log <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">-9</span>, <span class="number">-8</span>, <span class="number">-7</span>, <span class="number">-6</span>, <span class="number">-5</span>,<span class="number">-4</span>,<span class="number">-3</span>,<span class="number">-2</span>,<span class="number">-1</span>]:</span><br><span class="line">  step_size = <span class="number">10</span> ** step_size_log</span><br><span class="line">  W_new = W - step_size * df <span class="comment"># 权重空间中的新位置</span></span><br><span class="line">  loss_new = CIFAR10_loss_fun(W_new)</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'for step size %f new loss: %f'</span> % (step_size, loss_new)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># original loss: 2.200718</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-10 new loss: 2.200652</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-09 new loss: 2.200057</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-08 new loss: 2.194116</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-07 new loss: 2.135493</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-06 new loss: 1.647802</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-05 new loss: 2.844355</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-04 new loss: 25.558142</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-03 new loss: 254.086573</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-02 new loss: 2539.370888</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-01 new loss: 25392.214036</span></span><br></pre></td></tr></table></figure>
<p><strong>在梯度负方向上更新</strong>：在上面的代码中，为了计算<strong>W_new</strong>，要注意我们是向着梯度<strong>df</strong>的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。</p>
<p><strong>步长的影响</strong>：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。在后续的课程中可以看到，选择步长（也叫作<em>学习率</em>）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。还是用蒙眼徒步者下山的比喻，这就好比我们可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多长的步长呢？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在上面的代码中就能看见反例，在某些点如果步长过大，反而可能越过最低点导致更高的损失值。</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_2.jpg?raw=true" width="250"></center>

<p>将步长效果视觉化的图例。从某个具体的点W开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为<strong>学习率</strong>）将会是我们在调参中最重要的超参数之一。</p>
<p>————————————————————————————————————————</p>
<p><strong>效率问题</strong>：你可能已经注意到，计算数值梯度的复杂性和参数的量线性相关。在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度。现代神经网络很容易就有上千万的参数，因此这个问题只会越发严峻。显然这个策略不适合大规模数据，我们需要更好的策略。</p>
<h3 id="微分分析计算梯度"><a href="#微分分析计算梯度" class="headerlink" title="微分分析计算梯度"></a>微分分析计算梯度</h3><p>使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于<em>h</em>值是选取了一个很小的数值，但真正的梯度定义中<em>h</em>趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做<strong>梯度检查</strong>。</p>
<p>用SVM的损失函数在某个数据点上的计算来举例：</p>
<p><cneter>$$L_i=\displaystyle\sum_{j\not =y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)]$$</cneter></p>
<p>可以对函数进行微分。比如，对$w_{y_i}$进行微分得到：</p>
<center>$$ \nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$</center>

<p><strong>译者注：原公式中1为空心字体，尝试\mathbb{}等多种方法仍无法实现，请知友指点。</strong></p>
<p>其中$\mathbb{1}​$是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以$x_i​$就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些$j\not =y_i​$行的梯度是：</p>
<center>$$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i$$</center>

<p>一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为<em>梯度下降</em>，他的<strong>普通</strong>版本是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 进行梯度更新</span></span><br></pre></td></tr></table></figure>
<p>这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。</p>
<p><strong>小批量数据梯度下降（**</strong>Mini-batch gradient descent<strong>**）</strong>：在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的<strong>小批量（batches）</strong>数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是多少呢？一百二十万个。这个小批量数据就用来实现一个参数更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的小批量数据梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  data_batch = sample_training_data(data, <span class="number">256</span>) <span class="comment"># 256个数据</span></span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 参数更新</span></span><br></pre></td></tr></table></figure>
<p>这个方法之所以效果不错，是因为训练集中的数据都是相关的。要理解这一点，可以想象一个极端情况：在ILSVRC中的120万个图像是1000张不同图片的复制（每个类别1张图片，每张图片有1200张复制）。那么显然计算这1200张复制图像的梯度就应该是一样的。对比120万张图片的数据损失的均值与只计算1000张的子集的数据损失均值时，结果应该是一样的。实际情况中，数据集肯定不会包含重复图像，那么小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。</p>
<p>小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为<strong>随机梯度下降（Stochastic Gradient Descent 简称SGD）</strong>，有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_3.jpg?raw=true" width="300"></center>

<p>信息流的总结图例。数据集中的(x,y)是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量<strong>f</strong>中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分f和实际标签y之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。</p>
<p>—————————————————————————————————————————</p>
<p>在本节课中：</p>
<ul>
<li>将损失函数比作了一个<strong>高维度的最优化地形</strong>，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。</li>
<li>提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。</li>
<li>函数的<strong>梯度</strong>给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是<em>h</em>，用来计算数值梯度）。</li>
<li>参数更新需要有技巧地设置<strong>步长</strong>。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。</li>
<li>讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用<strong>梯度检查</strong>来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。</li>
<li>介绍了<strong>梯度下降</strong>算法，它在循环中迭代地计算梯度并更新参数。</li>
</ul>
<p><strong>预告</strong>：这节课的核心内容是：理解并能计算损失函数关于权重的梯度，是设计、训练和理解神经网络的核心能力。下节中，将介绍如何使用链式法则来高效地计算梯度，也就是通常所说的<strong>反向传播（backpropagation）机制</strong>。该机制能够对包含卷积神经网络在内的几乎所有类型的神经网络的损失函数进行高效的最优化。</p>
<p><strong>最优化笔记全文翻译完</strong>。</p>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/optimization-1/" target="_blank" rel="noopener"><strong>Optimization Note</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/li-yi-ying-73" target="_blank" rel="noopener">李艺颖</a>和<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改</p>
<p>知乎地址：（上，下）</p>
<p><a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_BP/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_BP/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：反向传播笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-03-26 08:53:14" itemprop="dateModified" datetime="2018-03-26T08:53:14+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：反向传播笔记"><a href="#CS231n课程笔记翻译：反向传播笔记" class="headerlink" title="CS231n课程笔记翻译：反向传播笔记"></a>CS231n课程笔记翻译：反向传播笔记</h1><h2 id="原文如下："><a href="#原文如下：" class="headerlink" title="原文如下："></a>原文如下：</h2><p>内容列表：</p>
<ul>
<li>简介</li>
<li>简单表达式和理解梯度</li>
<li>复合表达式，链式法则，反向传播</li>
<li>直观理解反向传播</li>
<li>模块：Sigmoid例子</li>
<li>反向传播实践：分段计算</li>
<li>回传流中的模式</li>
<li>用户向量化操作的梯度</li>
<li>小结</li>
</ul>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>目标</strong>：本节将帮助读者对<strong>反向传播</strong>形成直观而专业的理解。反向传播是利用<strong>链式法则</strong>递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常<strong>关键</strong>。</p>
<p><strong>问题陈述</strong>：这节的核心问题是：给定函数$f(x)$，其中$x$是输入数据的向量，需要计算函数$f$关于$x$的梯度，也就是$\nabla f(x)$。</p>
<p><strong>目标</strong>：之所以关注上述问题，是因为在神经网络中$f(x)$对应的是损失函数（$L$），输入$x$里面包含训练数据和神经网络的权重。举个例子，损失函数可以是SVM的损失函数，输入则包含了训练数据$(x_i,y_i),i=1…N$、权重$W$和偏差$b$。注意训练集是给定的（在机器学习中通常都是这样），而权重是可以控制的变量。因此，即使能用反向传播计算输入数据$x_i$ 上的梯度，但在实践为了进行参数更新，通常也只计算参数（比如$W,b$）的梯度。然而$x_i$ 的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。</p>
<p>如果读者之前对于利用链式法则计算偏微分已经很熟练，仍然建议浏览本篇笔记。因为它呈现了一个相对成熟的反向传播视角，在该视角中能看见基于实数值回路的反向传播过程，而对其细节的理解和收获将帮助读者更好地通过本课程。</p>
<h2 id="简单表达式和理解梯度"><a href="#简单表达式和理解梯度" class="headerlink" title="简单表达式和理解梯度"></a>简单表达式和理解梯度</h2><p>从简单表达式入手可以为复杂表达式打好符号和规则基础。先考虑一个简单的二元乘法函数$f(x,y)=xy$。对两个输入变量分别求偏导数还是很简单的：</p>
<center>$$\displaystyle f(x,y)=xy \to \frac {df}{dx}=y \quad \frac {df}{dy}=x$$</center>

<p><strong>解释</strong>：牢记这些导数的意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。</p>
<p>注意等号左边的分号和等号右边的分号不同，不是代表分数。相反，这个符号表示操作符$\frac{d}{dx}$被应用于函数$f$，并返回一个不同的函数（导数）。对于上述公式，可以认为$h$值非常小，函数可以被一条直线近似，而导数就是这条直线的斜率。换句话说，每个变量的导数指明了整个表达式对于该变量的值的敏感程度。比如，若$x=4,y=-3$，则$f(x,y)=-12$，$x$的导数$\frac{\partial f}{\partial x}=-3$。这就说明如果将变量$x$的值变大一点，整个表达式的值就会变小（原因在于负号），而且变小的量是$x$变大的量的三倍。通过重新排列公式可以看到这一点（$f(x+h)=f(x)+h \frac{df(x)}{dx}$）。同样，因为$\frac{\partial f}{\partial y}=4$，可以知道如果将$y$的值增加$h$，那么函数的输出也将增加（原因在于正号），且增加量是$4h$。</p>
<blockquote>
<p>函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度。</p>
</blockquote>
<p>如上所述，梯度$\nabla f$是偏导数的向量，所以有$\nabla f(x)=[\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]=[y,x]$。即使是梯度实际上是一个向量，仍然通常使用类似“<em>x上的梯度</em>”的术语，而不是使用如“<em>x的偏导数</em>”的正确说法，原因是因为前者说起来简单。</p>
<p>我们也可以对加法操作求导：</p>
<center>$$\displaystyle f(x,y)=x+y \to \frac {df}{dx}=1\quad\frac {df}{dy}=1$$</center>

<p>这就是说，无论其值如何，$x,y$的导数均为1。这是有道理的，因为无论增加$x,y$中任一个的值，函数$f$的值都会增加，并且增加的变化率独立于$x,y$的具体值（情况和乘法操作不同）。取最大值操作也是常常使用的：</p>
<center>$$\displaystyle f(x,y)=max(x,y) \to \frac {df}{dx}=1 (x&gt;=y) \quad\frac {df}{dy}=1 (y&gt;=x)$$</center>

<p>上式是说，如果该变量比另一个变量大，那么梯度是1，反之为0。例如，若$x=4,y=2$，那么max是4，所以函数对于$y$就不敏感。也就是说，在$y$上增加$h$，函数还是输出为4，所以梯度是0：因为对于函数输出是没有效果的。当然，如果给$y$增加一个很大的量，比如大于2，那么函数$f$的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：$lim_{h\to 0}$。</p>
<h2 id="使用链式法则计算复合表达式"><a href="#使用链式法则计算复合表达式" class="headerlink" title="使用链式法则计算复合表达式"></a>使用链式法则计算复合表达式</h2><p>现在考虑更复杂的包含多个函数的复合函数，比如$f(x,y,z)=(x+y)z$。虽然这个表达足够简单，可以直接微分，但是在此使用一种有助于读者直观理解反向传播的方法。将公式分成两部分：$q=x+y$和$f=qz$。在前面已经介绍过如何对这分开的两个公式进行计算，因为$f$是$q$和$z$相乘，所以$\displaystyle\frac{\partial f}{\partial q}=z,\frac{\partial f}{\partial z}=q$，又因为$q$是$x$加$y$，所以$\displaystyle\frac{\partial q}{\partial x}=1,\frac{\partial q}{\partial y}=1$。然而，并不需要关心中间量$q$的梯度，因为$\frac{\partial f}{\partial q}$没有用。相反，函数$f$关于$x,y,z$的梯度才是需要关注的。<strong>链式法则</strong>指出将这些梯度表达式链接起来的正确方式是相乘，比如$\displaystyle\frac{\partial f}{\partial x}=\frac{\partial f}{\partial q}\frac{\partial q}{\partial x}$。在实际操作中，这只是简单地将两个梯度数值相乘，示例代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置输入值</span></span><br><span class="line">x = <span class="number">-2</span>; y = <span class="number">5</span>; z = <span class="number">-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行前向传播</span></span><br><span class="line">q = x + y <span class="comment"># q becomes 3</span></span><br><span class="line">f = q * z <span class="comment"># f becomes -12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行反向传播:</span></span><br><span class="line"><span class="comment"># 首先回传到 f = q * z</span></span><br><span class="line">dfdz = q <span class="comment"># df/dz = q, 所以关于z的梯度是3</span></span><br><span class="line">dfdq = z <span class="comment"># df/dq = z, 所以关于q的梯度是-4</span></span><br><span class="line"><span class="comment"># 现在回传到q = x + y</span></span><br><span class="line">dfdx = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dx = 1. 这里的乘法是因为链式法则</span></span><br><span class="line">dfdy = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dy = 1</span></span><br></pre></td></tr></table></figure>
<p>最后得到变量的梯度<strong>[dfdx, dfdy, dfdz]</strong>，它们告诉我们函数<strong>f</strong>对于变量<strong>[x, y, z]</strong>的敏感程度。这是一个最简单的反向传播。一般会使用一个更简洁的表达符号，这样就不用写<strong>df</strong>了。这就是说，用<strong>dq</strong>来代替<strong>dfdq</strong>，且总是假设梯度是关于最终输出的。</p>
<p>这次计算可以被可视化为如下计算线路图像：</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/backprop/backprop_0.jpg?raw=true" width="350"></center><br>上图的真实值计算线路展示了计算的视觉化过程。<strong>前向传播</strong>从输入计算到输出（绿色），<strong>反向传播</strong>从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。<br><br>————————————————————————————————————————<br><br>## 反向传播的直观理解<br><br>反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：1. 这个门的输出值，和2.其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。<br><br>&gt; 这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。<br><br>下面通过例子来对这一过程进行理解。加法门收到了输入[-2, 5]，计算输出是3。既然这个门是加法操作，那么对于两个输入的局部梯度都是+1。网络的其余部分计算出最终值为-12。在反向传播时将递归地使用链式法则，算到加法门（是乘法门的输入）的时候，知道加法门的输出的梯度是-4。如果网络如果想要输出值更高，那么可以认为它会想要加法门的输出更小一点（因为负号），而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以<strong>x</strong>和<strong>y</strong>的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到得到了想要的效果：如果<strong>x，y减小</strong>（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。<br><br>因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。<br><br>## 模块化：Sigmoid例子<br><br>上面介绍的门是相对随意的。任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需要将一个函数分拆成多个门。现在看看一个表达式：<br><br><center>$$\displaystyle f(w,x)=\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}$$</center>

<p>在后面的课程中可以看到，这个表达式描述了一个含输入<strong>x</strong>和权重<strong>w</strong>的2维的神经元，该神经元使用了<em>sigmoid激活</em>函数。但是现在只是看做是一个简单的输入为x和w，输出为一个数字的函数。这个函数是由多个门组成的。除了上文介绍的加法门，乘法门，取最大值门，还有下面这4种：</p>
<center>$$f(x) = \frac{1}{x} \hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = -1/x^2 $$<br>$$f_c(x) = c + x\hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = 1 $$<br>$$f(x) = e^x\hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = e^x$$<br>$$f_a(x) = ax\hspace{1in} \rightarrow \hspace{1in} \frac{df}{dx} = a$$ </center>

<p>其中，函数$f_c$使用对输入值进行了常量$c$将输入值扩大了常量$a$倍。它们是加法和乘法的特例，但是这里将其看做一元门单元，因为确实需要计算常量$c,a$的梯度。整个计算线路如下：</p>
<p>———————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/backprop/backprop_1.jpg?raw=true" width="400"></center>

<p>使用sigmoid激活函数的2维神经元的例子。输入是[x0, x1]，可学习的权重是[w0, w1, w2]。一会儿会看见，这个神经元对输入数据做点积运算，然后其激活数据被sigmoid函数挤压到0到1之间。</p>
<p>————————————————————————————————————————</p>
<p>在上面的例子中可以看见一个函数操作的长链条，链条上的门都对<strong>w</strong>和<strong>x</strong>的点积结果进行操作。该函数被称为sigmoid函数$\sigma (x)$。sigmoid函数关于其输入的求导是可以简化的(使用了在分子上先加后减1的技巧)：</p>
<center>$$\sigma(x) = \frac{1}{1+e^{-x}} \\\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)$$</center>

<p>可以看到梯度计算简单了很多。举个例子，sigmoid表达式输入为1.0，则在前向传播中计算出输出为0.73。根据上面的公式，局部梯度为(1-0.73)*0.73~=0.2，和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。因此，在实际的应用中将这些操作装进一个单独的门单元中将会非常有用。该神经元反向传播的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">w = [<span class="number">2</span>,<span class="number">-3</span>,<span class="number">-3</span>] <span class="comment"># 假设一些随机数据和权重</span></span><br><span class="line">x = [<span class="number">-1</span>, <span class="number">-2</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">dot = w[<span class="number">0</span>]*x[<span class="number">0</span>] + w[<span class="number">1</span>]*x[<span class="number">1</span>] + w[<span class="number">2</span>]</span><br><span class="line">f = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-dot)) <span class="comment"># sigmoid函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对神经元反向传播</span></span><br><span class="line">ddot = (<span class="number">1</span> - f) * f <span class="comment"># 点积变量的梯度, 使用sigmoid函数求导</span></span><br><span class="line">dx = [w[<span class="number">0</span>] * ddot, w[<span class="number">1</span>] * ddot] <span class="comment"># 回传到x</span></span><br><span class="line">dw = [x[<span class="number">0</span>] * ddot, x[<span class="number">1</span>] * ddot, <span class="number">1.0</span> * ddot] <span class="comment"># 回传到w</span></span><br><span class="line"><span class="comment"># 完成！得到输入的梯度</span></span><br></pre></td></tr></table></figure>
<p><strong>实现提示：分段反向传播</strong>。上面的代码展示了在实际操作中，为了使反向传播过程更加简洁，把向前传播分成不同的阶段将是很有帮助的。比如我们创建了一个中间变量<strong>dot</strong>，它装着<strong>w</strong>和<strong>x</strong>的点乘结果。在反向传播的时，就可以（反向地）计算出装着<strong>w</strong>和<strong>x</strong>等的梯度的对应的变量（比如<strong>ddot</strong>，<strong>dx</strong>和<strong>dw</strong>）。</p>
<p>本节的要点就是展示反向传播的细节过程，以及前向传播过程中，哪些函数可以被组合成门，从而可以进行简化。知道表达式中哪部分的局部梯度计算比较简洁非常有用，这样他们可以“链”在一起，让代码量更少，效率更高。</p>
<h2 id="反向传播实践：分段计算"><a href="#反向传播实践：分段计算" class="headerlink" title="反向传播实践：分段计算"></a>反向传播实践：分段计算</h2><p>看另一个例子。假设有如下函数：</p>
<center>$$f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}$$</center>

<p>首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对$x$或$y$进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">3</span> <span class="comment"># 例子数值</span></span><br><span class="line">y = <span class="number">-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">sigy = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-y)) <span class="comment"># 分子中的sigmoi          #(1)</span></span><br><span class="line">num = x + sigy <span class="comment"># 分子                                    #(2)</span></span><br><span class="line">sigx = <span class="number">1.0</span> / (<span class="number">1</span> + math.exp(-x)) <span class="comment"># 分母中的sigmoid         #(3)</span></span><br><span class="line">xpy = x + y                                              <span class="comment">#(4)</span></span><br><span class="line">xpysqr = xpy**<span class="number">2</span>                                          <span class="comment">#(5)</span></span><br><span class="line">den = sigx + xpysqr <span class="comment"># 分母                                #(6)</span></span><br><span class="line">invden = <span class="number">1.0</span> / den                                       <span class="comment">#(7)</span></span><br><span class="line">f = num * invden <span class="comment"># 搞定！                                 #(8)</span></span><br></pre></td></tr></table></figure>
<p>┗|｀O′|┛ 嗷~~，到了表达式的最后，就完成了前向传播。注意在构建代码s时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生每个变量(<strong>sigy, num, sigx, xpy, xpysqr, den, invden</strong>)进行回传。我们会有同样数量的变量，但是都以<strong>d</strong>开头，用来存储对应变量的梯度。注意在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据使用链式法则乘以上游梯度。对于每行代码，我们将指明其对应的是前向传播的哪部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回传 f = num * invden</span></span><br><span class="line">dnum = invden <span class="comment"># 分子的梯度                                         #(8)</span></span><br><span class="line">dinvden = num                                                     <span class="comment">#(8)</span></span><br><span class="line"><span class="comment"># 回传 invden = 1.0 / den </span></span><br><span class="line">dden = (<span class="number">-1.0</span> / (den**<span class="number">2</span>)) * dinvden                                <span class="comment">#(7)</span></span><br><span class="line"><span class="comment"># 回传 den = sigx + xpysqr</span></span><br><span class="line">dsigx = (<span class="number">1</span>) * dden                                                <span class="comment">#(6)</span></span><br><span class="line">dxpysqr = (<span class="number">1</span>) * dden                                              <span class="comment">#(6)</span></span><br><span class="line"><span class="comment"># 回传 xpysqr = xpy**2</span></span><br><span class="line">dxpy = (<span class="number">2</span> * xpy) * dxpysqr                                        <span class="comment">#(5)</span></span><br><span class="line"><span class="comment"># 回传 xpy = x + y</span></span><br><span class="line">dx = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line">dy = (<span class="number">1</span>) * dxpy                                                   <span class="comment">#(4)</span></span><br><span class="line"><span class="comment"># 回传 sigx = 1.0 / (1 + math.exp(-x))</span></span><br><span class="line">dx += ((<span class="number">1</span> - sigx) * sigx) * dsigx <span class="comment"># Notice += !! See notes below  #(3)</span></span><br><span class="line"><span class="comment"># 回传 num = x + sigy</span></span><br><span class="line">dx += (<span class="number">1</span>) * dnum                                                  <span class="comment">#(2)</span></span><br><span class="line">dsigy = (<span class="number">1</span>) * dnum                                                <span class="comment">#(2)</span></span><br><span class="line"><span class="comment"># 回传 sigy = 1.0 / (1 + math.exp(-y))</span></span><br><span class="line">dy += ((<span class="number">1</span> - sigy) * sigy) * dsigy                                 <span class="comment">#(1)</span></span><br><span class="line"><span class="comment"># 完成! 嗷~~</span></span><br></pre></td></tr></table></figure>
<p>需要注意的一些东西：</p>
<p><strong>对前向传播变量进行缓存</strong>：在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。</p>
<p><strong>在不同分支的梯度要相加</strong>：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用<strong>+=</strong>而不是<strong>=</strong>来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的<em>多元链式法则</em>，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。</p>
<h2 id="回传流中的模式"><a href="#回传流中的模式" class="headerlink" title="回传流中的模式"></a>回传流中的模式</h2><p>一个有趣的现象是在多数情况下，反向传播中的梯度可以被很直观地解释。例如神经网络中最常用的加法、乘法和取最大值这三个门单元，它们在反向传播过程中的行为都有非常简单的解释。先看下面这个例子：</p>
<p>——————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/backprop/backprop_2.jpg?raw=true" width="350"></center>

<p>一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。</p>
<p>——————————————————————————————————————————</p>
<p>从上例可知：</p>
<p><strong>加法门单元</strong>把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。</p>
<p><strong>取最大值门单元</strong>对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了<strong>z</strong>变量，因为<strong>z</strong>的值比<strong>w</strong>高，于是<strong>w</strong>的梯度保持为0。</p>
<p><strong>乘法门单元</strong>相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，<strong>x</strong>的梯度是-4.00x2.00=-8.00。</p>
<p><em>非直观影响及其结果</em>。注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，那么乘法门的操作将会不是那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积$w^Tx_i$，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本$x_i$乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助读者调试网络。</p>
<h2 id="用向量化操作计算梯度"><a href="#用向量化操作计算梯度" class="headerlink" title="用向量化操作计算梯度"></a>用向量化操作计算梯度</h2><p>上述内容考虑的都是单个变量情况，但是所有概念都适用于矩阵和向量操作。然而，在操作的时候要注意关注维度和转置操作。</p>
<p><strong>矩阵相乘的梯度</strong>：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">W = np.random.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">X = np.random.randn(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">D = W.dot(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们得到了D的梯度</span></span><br><span class="line">dD = np.random.randn(*D.shape) <span class="comment"># 和D一样的尺寸</span></span><br><span class="line">dW = dD.dot(X.T) <span class="comment">#.T就是对矩阵进行转置</span></span><br><span class="line">dX = W.T.dot(dD)</span><br></pre></td></tr></table></figure>
<p><em>提示：要分析维度！</em>注意不需要去记忆<strong>dW</strong>和<strong>dX</strong>的表达，因为它们很容易通过维度推导出来。例如，权重的梯度dW的尺寸肯定和权重矩阵W的尺寸是一样的，而这又是由<strong>X</strong>和<strong>dD</strong>的矩阵乘法决定的（在上面的例子中<strong>X</strong>和<strong>W</strong>都是数字不是矩阵）。总有一个方式是能够让维度之间能够对的上的。例如，<strong>X</strong>的尺寸是[10x3]，<strong>dD</strong>的尺寸是[5x3]，如果你想要dW和W的尺寸是[5x10]，那就要<strong>dD.dot(X.T)</strong>。</p>
<p><strong>使用小而具体的例子</strong>：有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。</li>
<li>讨论了<strong>分段计算</strong>在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。</li>
</ul>
<p>在下节课中，将会开始定义神经网络，而反向传播使我们能高效计算神经网络各个节点关于损失函数的梯度。换句话说，我们现在已经准备好训练神经网络了，本课程最困难的部分已经过去了！ConvNets相比只是向前走了一小步。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.05767" target="_blank" rel="noopener"><strong>Automatic differentiation in machine learning: a survey</strong></a></li>
</ul>
<p><strong>反向传播笔记全文翻译完</strong>。</p>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/optimization-2/" target="_blank" rel="noopener"><strong>Backprop Note</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>和<a href="https://www.zhihu.com/people/hmonkey" target="_blank" rel="noopener">巩子嘉</a>进行校对修改。</p>
<p>知乎地址：<a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/hello-world/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-03-02 16:54:52" itemprop="dateModified" datetime="2018-03-02T16:54:52+08:00">2018-03-02</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Linear_Classify/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Linear_Classify/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：线性分类笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-03-26 08:53:54" itemprop="dateModified" datetime="2018-03-26T08:53:54+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：线性分类笔记"><a href="#CS231n课程笔记翻译：线性分类笔记" class="headerlink" title="CS231n课程笔记翻译：线性分类笔记"></a>CS231n课程笔记翻译：线性分类笔记</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li><p>线性分类器简介</p>
</li>
<li><p>线性评分函数</p>
</li>
<li><p>阐明线性分类器</p>
</li>
<li><p>损失函数</p>
<ul>
<li>多类SVM</li>
</ul>
</li>
</ul>
<ul>
<li>Softmax分类器</li>
<li>SVM和Softmax的比较</li>
</ul>
<ul>
<li><p>基于Web的可交互线性分类器原型</p>
</li>
<li><p>小结</p>
</li>
</ul>
<h2 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h2><p>上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。我们还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：</p>
<ul>
<li>分类器必须<em>记住</em>所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以GB计。</li>
<li>对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。</li>
</ul>
<p><strong>概述</strong>：我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是<strong>评分函数（score function）</strong>，它是原始图像数据到类别分值的映射。另一个是<strong>损失函数（loss function）</strong>，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。</p>
<h2 id="从图像到标签分值的参数化映射"><a href="#从图像到标签分值的参数化映射" class="headerlink" title="从图像到标签分值的参数化映射"></a>从图像到标签分值的参数化映射</h2><p>​        该方法的第一部分就是定义一个评分函数，这个函数将图像的像素值映射为各个分类类别的得分，得分高低代表图像属于该类别的可能性高低。下面会利用一个具体例子来展示该方法。现在假设有一个包含很多图像的训练集$x_i \in R^D$，每个图像都有一个对应的分类标签$y_i$。这里$i=1,2….N$并且$y_i \in 1….K$。这就是说，我们有<strong>N</strong>个图像样例，每个图像的维度是<strong>D</strong>，共有<strong>K</strong>种不同的分类。</p>
<p>举例来说，在CIFAR-10中，我们有一个<strong>N</strong>=50000的训练集，每个图像有<strong>D</strong>=32x32x3=3072个像素，而<strong>K</strong>=10，这是因为图片被分为10个不同的类别（狗，猫，汽车等）。我们现在定义评分函数为：$f:R^D \rightarrow R^K$，该函数是原始图像像素到分类分值的映射。</p>
<p><strong>线性分类器</strong>：在本模型中，我们从最简单的概率函数开始，一个线性映射：</p>
<center>$$f(x_i,W,b)=Wx_i+b$$</center>

<p>在上面的公式中，假设每个图像数据都被拉长为一个长度为D的列向量，大小为[D x 1]。其中大小为[K x D]的矩阵<strong>W</strong>和大小为[K x 1]列向量<strong>b</strong>为该函数的<strong>参数（parameters）</strong>。还是以CIFAR-10为例，$x_i$就包含了第$i$个图像的所有像素信息，这些信息被拉成为一个[3072 x 1]的列向量，W大小为[10x3072]，b的大小为[10x1]。因此，3072个数字（原始像素数值）输入函数，函数输出10个数字（不同分类得到的分值）。参数W被称为权重（weights）。b被称为偏差向量（bias vector），这是因为它影响输出数值，但是并不和原始数据产生关联。在实际情况中，人们常常混用权重和参数这两个术语。</p>
<p>需要注意的几点：</p>
<ul>
<li>首先，一个单独的矩阵乘法$Wx_i$就高效地并行评估10个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是W的一个行向量。</li>
<li>注意我们认为输入数据$(x_i,y_i)$是给定且不可改变的，但参数<strong>W</strong>和<strong>b</strong>是可控制改变的。我们的目标就是通过设置这些参数，使得计算出来的分类分值情况和训练集中图像数据的真实类别标签相符。在接下来的课程中，我们将详细介绍如何做到这一点，但是目前只需要直观地让正确分类的分值比错误分类的分值高即可。</li>
<li>该方法的一个优势是训练数据是用来学习到参数<strong>W</strong>和<strong>b</strong>的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。这是因为一个测试图像可以简单地输入函数，并基于计算出的分类分值来进行分类。</li>
<li>最后，注意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比k-NN中将测试图像和所有训练数据做比较的方法快多了。</li>
</ul>
<blockquote>
<p><em>预告：卷积神经网络映射图像像素值到分类分值的方法和上面一样，但是映射$(f)$就要复杂多了，其包含的参数也更多。</em></p>
</blockquote>
<h2 id="理解线性分类器"><a href="#理解线性分类器" class="headerlink" title="理解线性分类器"></a>理解线性分类器</h2><p>线性分类器计算图像中3个颜色通道中所有像素的值与权重的矩阵乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶（根据每个权重的符号而定）。举个例子，可以想象“船”分类就是被大量的蓝色所包围（对应的就是水）。那么“船”分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了“船”分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了“船”分类的分值）。</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_0.jpg?raw=true" width="300"></center>

<p>一个将图像映射到分类分值的例子。为了便于可视化，假设图像只有4个像素（都是黑白像素，这里不考虑RGB通道），有3个分类（红色代表猫，绿色代表狗，蓝色代表船，注意，这里的红、绿和蓝3种颜色仅代表分类，和RGB通道没有关系）。首先将图像像素拉伸为一个列向量，与W进行矩阵乘，然后得到各个分类的分值。需要注意的是，这个W一点也不好：猫分类的分值非常低。从上图来看，算法倒是觉得这个图像是一只狗。</p>
<p>————————————————————————————————————————</p>
<p><strong>将图像看做高维度的点</strong>：既然图像被伸展成为了一个高维度的列向量，那么我们可以把图像看做这个高维度空间中的一个点（即每张图像是3072维空间中的一个点）。整个数据集就是一个点的集合，每个点都带有1个分类标签。</p>
<p>既然定义每个分类类别的分值是权重和图像的矩阵乘，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化3072维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看看这些分类器在做什么了：</p>
<p>——————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_1.jpg?raw=true" width="350"></center>

<p>图像空间的示意图。其中每个图像是一个点，有3个分类器。以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。</p>
<p>—————————————————————————————————————————</p>
<p>从上面可以看到，<strong>W</strong>的每一行都是一个分类类别的分类器。对于这些数字的几何解释是：如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转。而偏差<strong>b</strong>，则允许分类器对应的直线平移。需要注意的是，如果没有偏差，无论权重如何，在$x_i=0$时分类分值始终为0。这样所有分类器的线都不得不穿过原点。</p>
<p><strong>将线性分类器看做模板匹配</strong>：关于权重<strong>W</strong>的另一个解释是<strong>它</strong>的每一行对应着一个分类的模板（有时候也叫作<em>原型</em>）。一张图像对应不同分类的得分，是通过使用内积（也叫<em>点积</em>）来比较图像和模板，然后找到和哪个模板最相似。从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用L1或者L2距离。</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_2.jpg?raw=true" width="500"></center>

<p>将课程进度快进一点。这里展示的是以CIFAR-10为训练集，学习结束后的权重的例子。注意，船的模板如期望的那样有很多蓝色像素。如果图像是一艘船行驶在大海上，那么这个模板利用内积计算图像将给出很高的分数。</p>
<p>————————————————————————————————————————</p>
<p>可以看到马的模板看起来似乎是两个头的马，这是因为训练集中的马的图像中马头朝向各有左右造成的。线性分类器将这两种情况融合到一起了。类似的，汽车的模板看起来也是将几个不同的模型融合到了一个模板中，并以此来分辨不同方向不同颜色的汽车。这个模板上的车是红色的，这是因为CIFAR-10中训练集的车大多是红色的。线性分类器对于不同颜色的车的分类能力是很弱的，但是后面可以看到神经网络是可以完成这一任务的。神经网络可以在它的隐藏层中实现中间神经元来探测不同种类的车（比如绿色车头向左，蓝色车头向前等）。而下一层的神经元通过计算不同的汽车探测器的权重和，将这些合并为一个更精确的汽车分类分值。</p>
<p><strong>偏差和权重的合并技巧</strong>：在进一步学习前，要提一下这个经常使用的技巧。它能够将我们常用的参数$W$和$b$合二为一。回忆一下，分类评分函数定义为：</p>
<center>$$f(x_i,W,b)=Wx_i+b$$</center>

<p>分开处理这两个参数（权重参数$W$和偏差参数$b$）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时$x_i$向量就要增加一个维度，这个维度的数值是常量1，这就是默认的偏差维度。这样新的公式就简化成下面这样：</p>
<center>$$f(x_i,W)=Wx_i$$</center>

<p>还是以CIFAR-10为例，那么$x_i$的大小就变成[3073x1]，而不是[3072x1]了，多出了包含常量1的1个维度）。$W$大小就是[10x3073]了。$W$中多出来的这一列对应的就是偏差值$b$，具体见下图：</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_3.jpg?raw=true" width="350"></center>

<p>偏差技巧的示意图。左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的。通过右边这样做，我们就只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵了。</p>
<p>—————————————————————————————————————————</p>
<p><strong>图像数据预处理</strong>：在上面的例子中，所有图像都是使用的原始像素值（从0到255）。在机器学习中，对于输入的特征做归一化（normalization）处理是常见的套路。而在图像分类的例子中，图像上的每个像素可以看做一个特征。在实践中，对每个特征减去平均值来<strong>中心化</strong>数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。下一个常见步骤是，让所有数值分布的区间变为[-1, 1]。<strong>零均值的中心化</strong>是很重要的，等我们理解了梯度下降后再来详细解释。</p>
<h2 id="损失函数-Loss-function"><a href="#损失函数-Loss-function" class="headerlink" title="损失函数 Loss function"></a>损失函数 Loss function</h2><p>在上一节定义了从图像像素值到所属类别的评分函数（score function），该函数的参数是权重矩阵$W$。在函数中，数据$(x_i,y_i)$是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p>
<p>回到之前那张猫的图像分类例子，它有针对“猫”，“狗”，“船”三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（-96.8），而狗（437.9）和船（61.95）比较高。我们将使用<strong>损失函数（Loss Function）</strong>（有时也叫<strong>代价函数Cost Function</strong>或<strong>目标函数Objective</strong>）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。</p>
<h3 id="多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss"><a href="#多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss" class="headerlink" title="多类支持向量机损失 Multiclass Support Vector Machine Loss"></a>多类支持向量机损失 Multiclass Support Vector Machine Loss</h3><p>损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值$\Delta$。我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。</p>
<p>让我们更精确一些。回忆一下，第i个数据中包含图像$x_i​$的像素和代表正确类别的标签$y_i​$。评分函数输入像素数据，然后通过公式$f(x_i,W)​$来计算不同分类类别的分值。这里我们将分值简写为$s​$。比如，针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)_j​$。针对第i个数据的多类SVM的损失函数定义如下：</p>
<center>$$L_i=\sum_{j\neq{y_i}}\max(0,s_j-s_{y_i}+\Delta)$$</center>

<p><strong>举例</strong>：用一个例子演示公式是如何计算的。假设有3个分类，并且得到了分值$s=[13,-7,11]$。其中第一个类别是正确类别，即$y_i=0$。同时假设$\Delta$是10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（$j\not=y_i$）加起来，所以我们得到两个部分：</p>
<p>可以看到第一个部分结果是0，这是因为[-7-13+10]得到的是负数，经过$max(0,-)$函数处理后得到0。这一对类别分数和标签的损失值是0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10。而SVM只关心差距至少要大于10，更大的差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。</p>
<p>那么在这次的模型中，我们面对的是线性评分函数（$f(x_i,W)=Wx_i$），所以我们可以将损失函数的公式稍微改写一下：</p>
<center>$$L_i=\sum_{j\neq{y_i}}\max(0,w_j^Tx_i-w_{y_i}^Tx_i+\Delta)$$</center>

<p>其中$w_j$是权重的$W$第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数$f$公式，这样做就不是必须的了。</p>
<p>在结束这一小节前，还必须提一下的属于是关于0的阀值：$max(0,-)$函数，它常被称为<strong>折叶损失（hinge loss）</strong>。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是$max(0,-)^2$，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。</p>
<blockquote>
<p>我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。</p>
</blockquote>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_4.jpg?raw=true" width="400"></center>

<p>多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出delta的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。</p>
<p>—————————————————————————————————————————</p>
<p><strong>正则化（Regularization）：</strong>上面损失函数有一个问题。假设有一个数据集和一个权重集<strong>W</strong>能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有$L_i=0$）。问题在于这个<strong>W</strong>并不唯一：可能有很多相似的<strong>W</strong>都能正确地分类所有的数据。一个简单的例子：如果<strong>W</strong>能够正确分类所有数据，即对于每个数据，损失值都是0。那么当$\lambda&gt;1$时，任何数乘$\lambda W$都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对<strong>W</strong>乘以2将使得差距变成30。</p>
<p>换句话说，我们希望能向某些特定的权重<strong>W</strong>添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个<strong>正则化惩罚(regularization penalty)</strong> $R(W)$部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：</p>
<center>$$\sum_k\sum_l{W_{k,l}^2}$$</center>

<p>上面的表达式中，将$W$中所有元素平方后求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：<strong>数据损失（data loss）</strong>，即所有样例的的平均损失$L_i$，以及<strong>正则化损失（regularization loss）</strong>。完整公式如下所示：</p>
<center>$$L=\displaystyle \underbrace{ \frac{1}{N}\sum_i L_i}<em>{data \  loss}+\underbrace{\lambda R(W)}</em>{regularization \ loss}$$</center>

<p>将其展开完整公式是：</p>
<center>$$L=\frac{1}{N}\sum_i \sum_{j\neq{y_i}}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta)]+\lambda\sum_k\sum_l{W_{k,l}^2}$$</center>

<p>其中，$N$是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。</p>
<p>除了上述理由外，引入正则化惩罚还带来很多良好的性质，这些性质大多会在后续章节介绍。比如引入了L2惩罚后，SVM们就有了<strong>最大边界（max margin）</strong>这一良好性质。（如果感兴趣，可以查看<a href="http://link.zhihu.com/?target=http%3A//cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="noopener"><strong>CS229课程</strong></a>）。</p>
<p>其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。举个例子，假设输入向量$x=[1,1,1,1]$，两个权重向量$w_1=[1,0,0,0]$，$w_2=[0.25,0.25,0.25,0.25]$。那么$w^T_1x=w^T_2=1$,两个权重向量都得到同样的内积，但是$w_1$的L2惩罚是1.0，而$w_2$的L2惩罚是0.25。因此，根据L2惩罚来看，$w_2$更好，因为它的正则化损失更小。从直观上来看，这是因为$w_2$的权重值更小且更分散。既然L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免<em>过拟合</em>。</p>
<p>需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重$W$正则化，而不正则化偏差$b$。在实际操作中，可发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当$W=0$的特殊情况下，才能得到损失值为0。</p>
<p><strong>代码</strong>：下面是一个无正则化部分的损失函数的Python实现，有非向量化和半向量化两个形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_i</span><span class="params">(x, y, W)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  unvectorized version. Compute the multiclass svm loss for a single example (x,y)</span></span><br><span class="line"><span class="string">  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)</span></span><br><span class="line"><span class="string">    with an appended bias dimension in the 3073-rd position (i.e. bias trick)</span></span><br><span class="line"><span class="string">  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)</span></span><br><span class="line"><span class="string">  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  delta = <span class="number">1.0</span> <span class="comment"># see notes about delta later in this section</span></span><br><span class="line">  scores = W.dot(x) <span class="comment"># scores becomes of size 10 x 1, the scores for each class</span></span><br><span class="line">  correct_class_score = scores[y]</span><br><span class="line">  D = W.shape[<span class="number">0</span>] <span class="comment"># number of classes, e.g. 10</span></span><br><span class="line">  loss_i = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> xrange(D): <span class="comment"># iterate over all wrong classes</span></span><br><span class="line">    <span class="keyword">if</span> j == y:</span><br><span class="line">      <span class="comment"># skip for the true class to only loop over incorrect classes</span></span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># accumulate loss for the i-th example</span></span><br><span class="line">    loss_i += max(<span class="number">0</span>, scores[j] - correct_class_score + delta)</span><br><span class="line">  <span class="keyword">return</span> loss_i</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_i_vectorized</span><span class="params">(x, y, W)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  A faster half-vectorized implementation. half-vectorized</span></span><br><span class="line"><span class="string">  refers to the fact that for a single example the implementation contains</span></span><br><span class="line"><span class="string">  no for loops, but there is still one loop over the examples (outside this function)</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  delta = <span class="number">1.0</span></span><br><span class="line">  scores = W.dot(x)</span><br><span class="line">  <span class="comment"># compute the margins for all classes in one vector operation</span></span><br><span class="line">  margins = np.maximum(<span class="number">0</span>, scores - scores[y] + delta)</span><br><span class="line">  <span class="comment"># on y-th position scores[y] - scores[y] canceled and gave delta. We want</span></span><br><span class="line">  <span class="comment"># to ignore the y-th position and only consider margin on max wrong class</span></span><br><span class="line">  margins[y] = <span class="number">0</span></span><br><span class="line">  loss_i = np.sum(margins)</span><br><span class="line">  <span class="keyword">return</span> loss_i</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L</span><span class="params">(X, y, W)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  fully-vectorized implementation :</span></span><br><span class="line"><span class="string">  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)</span></span><br><span class="line"><span class="string">  - y is array of integers specifying correct class (e.g. 50,000-D array)</span></span><br><span class="line"><span class="string">  - W are weights (e.g. 10 x 3073)</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># evaluate loss over all examples in X without using any for loops</span></span><br><span class="line">  <span class="comment"># left as exercise to reader in the assignment</span></span><br></pre></td></tr></table></figure>
<p>在本小节的学习中，一定要记得SVM损失采取了一种特殊的方法，使得能够衡量对于训练数据预测分类和实际分类标签的一致性。还有，对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。</p>
<blockquote>
<p>接下来要做的，就是找到能够使损失值最小化的权重了。</p>
</blockquote>
<h3 id="实际考虑"><a href="#实际考虑" class="headerlink" title="实际考虑"></a>实际考虑</h3><p><strong>设置Delta</strong>：你可能注意到上面的内容对超参数$\Delta$及其设置是一笔带过，那么它应该被设置成什么值？需要通过交叉验证来求得吗？现在看来，该超参数在绝大多数情况下设为$\Delta=1.0$都是安全的。超参数$\Delta$和$\lambda$看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重$W$的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将$W$中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如$\Delta=1$或$\Delta=100$）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度$\lambda$来控制）。</p>
<p><strong>与二元支持向量机（Binary Support Vector Machine）的关系</strong>：在学习本课程前，你可能对于二元支持向量机有些经验，它对于第i个数据的损失计算公式是：</p>
<p>其中，$C$是一个超参数，并且$y_i\in{-1,1}$。可以认为本章节介绍的SVM公式包含了上述公式，上述公式是多类支持向量机公式只有两个分类类别的特例。也就是说，如果我们要分类的类别只有两个，那么公式就化为二元SVM公式。这个公式中的$C$和多类SVM公式中的$\lambda$都控制着同样的权衡，而且它们之间的关系是$C\propto\frac{1}{\lambda}$</p>
<p><strong>备注：在初始形式中进行最优化</strong>。如果在本课程之前学习过SVM，那么对kernels，duals，SMO算法等将有所耳闻。在本课程（主要是神经网络相关）中，损失函数的最优化的始终在非限制初始形式下进行。很多这些损失函数从技术上来说是不可微的（比如当$x=y$时，$max(x,y)$函数就不可微分），但是在实际操作中并不存在问题，因为通常可以使用次梯度。</p>
<p><strong>备注：其他多类SVM公式</strong>。需要指出的是，本课中展示的多类SVM只是多种SVM公式中的一种。另一种常用的公式是<em>One-Vs-All</em>（OVA）SVM，它针对每个类和其他类训练一个独立的二元分类器。还有另一种更少用的叫做<em>All-Vs-All</em>（AVA）策略。我们的公式是按照<a href="http://link.zhihu.com/?target=https%3A//www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf" target="_blank" rel="noopener"><strong>Weston and Watkins 1999 (pdf)</strong></a>版本，比OVA性能更强（在构建有一个多类数据集的情况下，这个版本可以在损失值上取到0，而OVA就不行。感兴趣的话在论文中查阅细节）。最后一个需要知道的公式是Structured SVM，它将正确分类的分类分值和非正确分类中的最高分值的边界最大化。理解这些公式的差异超出了本课程的范围。本课程笔记介绍的版本可以在实践中安全使用，而被论证为最简单的OVA策略在实践中看起来也能工作的同样出色（在 Rikin等人2004年的论文<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf" target="_blank" rel="noopener"><strong>In Defense of One-Vs-All Classification (pdf)</strong></a>中可查）。</p>
<h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出$f(x_i,W)$作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射$f(x_i;W)=Wx_i$保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失（cross-entropy loss）</strong>。公式如下：</p>
<center>$\displaystyle Li=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})$ 或等价的 $L_i=-f_{y_i}+log(\sum_je^{f_j})$</center>

<p>在上式中，使用$f_j$来表示分类评分向量$f$中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值$L_i$之和。其中函数$f_j(z)=\frac{e^{z_j}}{\sum_ke^{z_k}}$被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（$z$中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p>
<p><strong>信息理论视角</strong>：在“真实”分布$p$和估计分布$q$之间的<em>交叉熵</em>定义如下：</p>
<p>因此，Softmax分类器所做的就是最小化在估计分类概率（就是上面的$e^{f_{y_i}}/\sum_je^{f_j}$）和“真实”分布之间的交叉熵，在这个解释中，“真实”分布就是所有概率密度都分布在正确的类别上（比如：$p=[0,…1,…,0]$中在$y_i$的位置就有一个单独的1）。还有，既然交叉熵可以写成熵和相对熵（Kullback-Leibler divergence）$H(p,q)=H(p)+D_{KL}(p||q)$，并且delta函数$p$的熵是0，那么就能等价的看做是对两个分布之间的相对熵做最小化操作。换句话说，交叉熵损失函数“想要”预测分布的所有概率密度都在正确分类上。</p>
<blockquote>
<p> <strong>*译者注</strong>：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。*</p>
</blockquote>
<p><strong>概率论解释</strong>：先看下面的公式：</p>
<p>可以解释为是给定图像数据$x_i$为参数，分配给正确分类标签$y_i$的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量$f$中的评分值解释为没有归一化的对数概率。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行<em>最大似然估计</em>（MLE）。该解释的另一个好处是，损失函数中的正则化部分$R(W)$可以被看做是权重矩阵$W$的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。提及这些解释只是为了让读者形成直观的印象，具体细节就超过本课程范围了。</p>
<p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项$e^{f_{y_i}}$和$\sum_j e^{f_j}$因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数$C$，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p>
<p>$C$的值可自由选择，不会影响计算结果，通过使用这个技巧可以提高计算中的数值稳定性。通常将$C$设为$logC=-max_jf_j$。该技巧简单地说，就是应该将向量$f$中的数值进行平移，使得最大值为0。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure>
<p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p>
<h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>下图有助于区分这 Softmax和SVM这两种分类器：</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_5.png?raw=true" width="350"></center>

<p>————————————————————————————————————————</p>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p>
<p>————————————————————————————————————————</p>
<p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p>
<p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（$\Delta =1$）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p>
<p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>
<h2 id="交互式的网页Demo"><a href="#交互式的网页Demo" class="headerlink" title="交互式的网页Demo"></a>交互式的网页Demo</h2><p>————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_6.jpg?raw=true" width="400"></center>我们实现了一个交互式的网页原型，来帮助读者直观地理解线性分类器。原型将损失函数进行可视化，画面表现的是对于2维数据的3种类别的分类。原型在课程进度上稍微超前，展现了最优化的内容，最优化将在下一节课讨论。</p>
<p>————————————————————————————————————————</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总结如下：</p>
<ul>
<li>定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重<strong>W</strong>和偏差<strong>b</strong>的线性函数。</li>
<li>与kNN分类器不同，<strong>参数方法</strong>的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重<strong>W</strong>进行一个矩阵乘法运算。</li>
<li>介绍了偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。</li>
<li>定义了损失函数（介绍了SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。</li>
</ul>
<p>现在我们知道了如何基于参数，将数据集中的图像映射成为分类的评分，也知道了两种不同的损失函数，它们都能用来衡量算法分类预测的质量。但是，如何高效地得到能够使损失值最小的参数呢？这个求得最优参数的过程被称为最优化，将在下节课中进行介绍。</p>
<h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><p>下面的内容读者可根据兴趣选择性阅读。</p>
<ul>
<li><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1306.0239" target="_blank" rel="noopener"><strong>Deep Learning using Linear Support Vector Machines</strong></a>一文的作者是Tang Charlie，论文写于2013年，展示了一些L2SVM比Softmax表现更出色的结果。</li>
</ul>
<p><strong>线性分类笔记全文翻译完毕</strong>。</p>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/linear-classify/" target="_blank" rel="noopener"><strong>Linear Classification Note</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改</p>
<p>知乎地址：（上，中，下）</p>
<p><a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit</a></p>
<p> <a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Neural_Network2/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Neural_Network2/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：神经网络笔记2</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2018-03-26 08:53:34" itemprop="dateModified" datetime="2018-03-26T08:53:34+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：神经网络笔记-2"><a href="#CS231n课程笔记翻译：神经网络笔记-2" class="headerlink" title="CS231n课程笔记翻译：神经网络笔记 2"></a>CS231n课程笔记翻译：神经网络笔记 2</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li>设置数据和模型<ul>
<li>数据预处理</li>
<li>权重初始化</li>
<li>批量归一化（Batch Normalization）</li>
<li>正则化（L2/L1/Maxnorm/Dropout）</li>
</ul>
</li>
<li>损失函数</li>
<li>小结</li>
</ul>
<h2 id="设置数据和模型"><a href="#设置数据和模型" class="headerlink" title="设置数据和模型"></a>设置数据和模型</h2><p>在上一节中介绍了神经元的模型，它在计算内积后进行非线性激活函数计算，神经网络将这些神经元组织成各个层。这些做法共同定义了<strong>评分函数（score function）</strong> 的新形式，该形式是从前面线性分类章节中的简单线性映射发展而来的。具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计选项，比如数据预处理，权重初始化和损失函数。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>关于数据预处理我们有3个常用的符号，数据矩阵<strong>X</strong>，假设其尺寸是 <strong>[N x D]</strong> （<strong>N</strong> 是数据样本的数量，<strong>D</strong> 是数据的维度）。</p>
<p><strong>均值减法（Mean subtraction）</strong> 是预处理最常用的形式。它对数据中每个独立<em>特征</em>减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码 <strong>X -= np.mean(X, axis=0)</strong> 实现。而对于图像，更常用的是对所有像素都减去一个值，可以用 <strong>X -= np.mean(X)</strong> 实现，也可以在3个颜色通道上分别操作。</p>
<p><strong>归一化（Normalization）</strong> 是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为 <strong>X /= np.std(X, axis=0)</strong> 。第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。</p>
<p>——————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_10.jpg?raw=true" width="400"></center><br>一般数据预处理流程： <strong>左边</strong> :原始的2维输入数据。 <strong>中间</strong> :在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。 <strong>右边</strong> : 每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。<br><br>——————————————————————————————————————————<br><br><strong>PCA和白化（Whitening）</strong> 是另一种预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设输入数据矩阵X的尺寸为[N x D]</span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># 对数据进行零中心化(重要)</span></span><br><span class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># 得到数据的协方差矩阵</span></span><br></pre></td></tr></table></figure><br><br>数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的<em>协方差</em>。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Positive-definite_matrix%23Negative-definite.2C_semidefinite_and_indefinite_matrices" target="_blank" rel="noopener"><strong>半正定</strong></a>的。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">U,S,V = np.linalg.svd(cov)</span><br></pre></td></tr></table></figure><br><br>U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xrot = np.dot(X,U) <span class="comment"># 对数据去相关性</span></span><br></pre></td></tr></table></figure><br><br>注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算<strong>Xrot</strong>的协方差矩阵，将会看到它是对角对称的。<strong>np.linalg.svd</strong>的一个良好性质是在它的返回值<strong>U</strong>中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有<strong>方差</strong>的维度。 这个操作也被称为主成分分析（ <a href="http://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener"><strong>Principal Component Analysis</strong></a> 简称PCA）降维：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># Xrot_reduced 变成 [N x 100]</span></span><br></pre></td></tr></table></figure><br><br>经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大<strong>方差</strong>的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。<br><br>最后一个在实践中会看见的变换是<strong>白化（whitening）</strong>。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据进行白化操作:</span></span><br><span class="line"><span class="comment"># 除以特征值 </span></span><br><span class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure><br><br><em>警告：夸大的噪声</em>。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。<br><br>——————————————————————————————————————————<br><br><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_11.jpg?raw=true" width="400"></center><br>PCA/白化。 <strong>左边</strong> 是二维的原始数据。 <strong>中间</strong> : 经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。 <strong>右边</strong> : 每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。<br><br>——————————————————————————————————————————<br><br>我们可以使用CIFAR-10数据将这些变化可视化出来。CIFAR-10训练集的大小是50000x3072，其中每张图片都可以拉伸为3072维的行向量。我们可以计算[3072 x 3072]的协方差矩阵然后进行奇异值分解（比较耗费计算性能），那么经过计算的特征向量看起来是什么样子呢？<br><br>—————————————————————————————————————————<br><br><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_12.jpg?raw=true" width="400"></center><br><strong>最左</strong>: 一个用于演示的集合，含49张图片。<strong>左二</strong>: 3072个特征值向量中的前144个。靠前面的特征向量解释了数据中大部分的方差，可以看见它们与图像中较低的频率相关。<strong>第三张</strong> 是49张经过了PCA降维处理的图片，展示了144个特征向量。这就是说，展示原始图像是每个图像用3072维的向量，向量中的元素是图片上某个位置的像素在某个颜色通道中的亮度值。而现在每张图片只使用了一个144维的向量，其中每个元素表示了特征向量对于组成这张图片的贡献度。为了让图片能够正常显示，需要将144维度重新变成基于像素基准的3072个数值。因为U是一个旋转，可以通过乘以U.transpose()[:144,:]来实现，然后将得到的3072个数值可视化。可以看见图像变得有点模糊了，这正好说明前面的特征向量获取了较低的频率。然而，大多数信息还是保留了下来。<strong>最右</strong>: 将“白化”后的数据进行显示。其中144个维度中的方差都被压缩到了相同的数值范围。然后144个白化后的数值通过乘以U.transpose()[:144,:]转换到图像像素基准上。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。<br><br>——————————————————————————————————————————<br><br><strong>实践操作:</strong>  在这个笔记中提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。<br><br><strong>常见错误:</strong> 进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。<strong>应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong><br><br><strong>译者注：此处确为初学者常见错误，请务必注意！</strong><br><br>### 权重初始化<br><br>我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。<br><br><strong>错误：全零初始化</strong>  让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。<br><br><strong>小随机数初始化</strong> 因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来<em>打破对称性</em>。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是： <strong>W = 0.01 * np.random.randn(D,H)</strong> 。其中 <strong>randn</strong> 函数是基于零均值和标准差的一个高斯分布（ <strong>译者注：国内教程一般习惯称均值参数为期望  $\mu$  </strong> ）来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。<br><br><strong>*警告</strong>   并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。<br><br><strong>使用1/sqrt(n)校准方差</strong>  上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为： <strong>w = np.random.randn(n) / sqrt(n)。</strong> 其中<strong>n</strong>是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。<br><br>上述结论的推导过程如下：假设权重$w$和输入$x$之间的内积为$s=\sum^n_iw_ix_i$，这是还没有进行非线性激活函数运算之前的原始数值。我们可以检查$s$的方差：<br><br><center>$$\begin{align}\text{Var}(s) &amp;= \text{Var}(\sum_i^n w_ix_i) \&amp;= \sum_i^n \text{Var}(w_ix_i) \&amp;= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + E[(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \&amp;= \sum_i^n \text{Var}(x_i)\text{Var}(w_i) \&amp;= \left( n \text{Var}(w) \right) \text{Var}(x)\end{align} $$</center>

<p>在前两步，使用了<a href="http://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Variance" target="_blank" rel="noopener"><strong>方差的性质</strong></a>。在第三步，因为假设输入和权重的平均值都是0，所以$E[x_i]=E[w_i]=0$。注意这并不是一般化情况，比如在ReLU单元中均值就为正。在最后一步，我们假设所有的$w_i,x_i$都服从同样的分布。从这个推导过程我们可以看见，如果想要$s$有和输入$x$一样的方差，那么在初始化的时候必须保证每个权重$w$的方差是$1/n$。又因为对于一个随机变量$X$和标量$a$，有$Var(aX)=a^2Var(X)$，这就说明可以基于一个标准高斯分布，然后乘以$a=\sqrt{1/n}$，使其方差为$1/n$)，于是得出：<strong>w = np.random.randn(n) / sqrt(n)</strong>。</p>
<p>Glorot等在论文<a href="http://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener"><strong>Understanding the difficulty of training deep feedforward neural networks</strong></a>中作出了类似的分析。在论文中，作者推荐初始化公式为$ ( \text{Var}(w) = 2/(n_{in} + n_{out}) ) $，其中$(n_{in}, n_{out})$是在前一层和后一层中单元的个数。这是基于妥协和对反向传播中梯度的分析得出的结论。该主题下最新的一篇论文是：<a href="http://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852" target="_blank" rel="noopener"><strong>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</strong></a>，作者是He等人。文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是$2.0/n$。代码为<strong>w = np.random.randn(n) * sqrt(2.0/n)</strong>。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。</p>
<p><strong>稀疏初始化（Sparse initialization）</strong> 另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。</p>
<p><strong>偏置（biases）的初始化</strong>  通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。</p>
<p><strong>实践</strong>  当前的推荐是使用ReLU激活函数，并且使用 <strong>w = np.random.randn(n) * sqrt(2.0/n)</strong> 来进行权重初始化，关于这一点，<a href="http://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852" target="_blank" rel="noopener"><strong>这篇文章</strong></a>有讨论。</p>
<p><strong>批量归一化（Batch Normalization）</strong> <a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.03167" target="_blank" rel="noopener"><strong>批量归一化</strong></a>是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛：），其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层，后续会讲）与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为上面的参考文献中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！</p>
<h3 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 Regularization"></a>正则化 Regularization</h3><p>有不少方法是通过控制神经网络的容量来防止其过拟合的：</p>
<p><strong>L2正则化</strong> 可能是最常用的正则化方法了。可以通过惩罚目标函数中所有参数的平方将其实现。即对于网络中的每个权重$w$，向目标函数中增加一个$\frac{1}{2}\lambda w^2$，其中$\lambda$是正则化强度。前面这个$\frac{1}{2}$很常见，是因为加上$\frac{1}{2}$后，该式子关于$w$梯度就是$\lambda w$而不是$2\lambda w$了。L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。在线性分类章节中讨论过，由于输入和权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以<strong>w += -lambda * W</strong>向着0线性下降。</p>
<p><strong>L1正则化</strong>  是另一个相对常用的正则化方法。对于每个$w$我们都向目标函数增加一个$\lambda|w|$。L1和L2正则化也可以进行组合：$\lambda_1|w|+\lambda_2w^2$，这也被称作<a href="http://link.zhihu.com/?target=http%3A//web.stanford.edu/%257Ehastie/Papers/B67.2%2520%25282005%2529%2520301-320%2520Zou%2520%26%2520Hastie.pdf" target="_blank" rel="noopener"><strong>Elastic net regularizaton</strong></a>。L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。</p>
<p><strong>最大范式约束（Max norm constraints）</strong> 另一种形式的正则化是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量$\overrightarrow{w}$必须满足$||\overrightarrow{w}||_2&lt;c$这一条件，一般$c$值为3或者4。有研究者发文称在使用这种正则化方法时效果更好。这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。</p>
<p><strong>随机失活（Dropout）</strong> 是一个简单又极其有效的正则化方法。该方法由Srivastava在论文<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%257Ersalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener"><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong></a>中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数$p$的概率被激活或者被设置为0。</p>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_13.jpg?raw=true" width="400"></center><br>图片来源自 <a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener"><strong>论文</strong></a>  ，展示其核心思路。在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。<br><br>—————————————————————————————————————————<br><br>一个3层神经网络的普通版随机失活可以用下面代码实现：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 普通版随机失活: 不推荐实现 (看下面笔记) """</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="string">""" X中是输入数据 """</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = np.random.rand(*H1.shape) &lt; p <span class="comment"># 第一个随机失活遮罩</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = np.random.rand(*H2.shape) &lt; p <span class="comment"># 第二个随机失活遮罩</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure><br><br>在上面的代码中， <strong>train_step</strong> 函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据 <strong>X创建</strong> 一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩 <strong>U1</strong> 和 <strong>U2</strong> 加入进去。<br><br>注意：在 <strong>predict</strong> 函数中不进行随机失活，但是对于两个隐层的输出都要乘以$p$，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以$p=0.5$为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。为了理解这点，先假设有一个神经元$x$的输出，那么进行随机失活的时候，该神经元的输出就是$px+(1-p)0$)，这是有$1-p$的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整$x\to px$来保持同样的预期输出。在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。<br><br>上述操作不好的性质是必须在测试时对激活数据要按照$p$进行数值范围调整。既然测试性能如此关键，实际更倾向使用 <strong>反向随机失活（inverted dropout）</strong> ，它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" </span></span><br><span class="line"><span class="string">反向随机失活: 推荐实现方式.</span></span><br><span class="line"><span class="string">在训练的时候drop和调整数值范围，测试时不做任何事.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 第一个随机失活遮罩. 注意/p!</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># 第二个随机失活遮罩. 注意/p!</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># 不用数值范围调整了</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure><br><br>在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献：<br><br>- <a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%257Ersalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener"><strong>Dropout paper</strong></a> by Srivastava et al. 2014.<br>- <a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf" target="_blank" rel="noopener"><strong>Dropout Training as Adaptive Regularization</strong></a>：“我们认为：在使用费希尔信息矩阵（<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Fisher_information_metric" target="_blank" rel="noopener"><strong>fisher information matrix</strong></a>）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。”<br><br><strong>前向传播中的噪音</strong> 在更一般化的分类上，随机失活属于网络在前向传播中有随机行为的方法。测试时，通过<em>分析法</em>（在使用随机失活的本例中就是乘以$p$）或<em>数值法</em>（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪音边缘化。在这个方向上的另一个研究是<a href="http://link.zhihu.com/?target=http%3A//cs.nyu.edu/%257Ewanli/dropc/" target="_blank" rel="noopener"><strong>DropConnect</strong></a>，它在前向传播的时候，一系列权重被随机设置为0。提前说一下，卷积神经网络同样会吸取这类方法的优点，比如随机汇合（stochastic pooling），分级汇合（fractional pooling），数据增长（data augmentation）。我们在后面会详细介绍。<br><br><strong>偏置正则化</strong> 在线性分类器的章节中介绍过，对于偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。<br><br><strong>每层正则化</strong> 对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。<br><br><strong>实践</strong> : 通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。$p$值一般默认设为0.5，也可能在验证集上调参。<br><br>## 损失函数<br><br>我们已经讨论过损失函数的正则化损失部分，它可以看做是对模型复杂程度的某种惩罚。损失函数的第二个部分是<em>数据损失</em>，它是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有样本的数据损失求平均。也就是说，$L=\frac{1}{N}\sum_iL_i$中，$N$是训练集数据的样本数。让我们把神经网络中输出层的激活函数简写为$f=f(x_i;W)$，在实际中你可能需要解决以下几类问题：<br><br><strong>分类问题</strong> 是我们一直讨论的。在该问题中，假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签（是固定分类标签之一）。在这类问题中，一个最常见的损失函数就是SVM（是Weston Watkins 公式）：<br><br><center>$$L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)$$</center>

<p>之前简要提起过，有些学者的论文中指出平方折叶损失（即使用$max(0,f_j-f_{y_i}+1)^2$）算法的结果会更好。第二个常用的损失函数是Softmax分类器，它使用交叉熵损失：</p>
<center>$$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)$$</center>

<p><strong>问题：类别数目巨大</strong> 当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用 <em>分层Softmax（ <strong>Hierarchical Softmax</strong> ）</em> 了（<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="noopener"><strong>参考文献</strong></a>）。分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。</p>
<p><strong>属性（Attribute）分类</strong> 上面两个损失公式的前提，都是假设每个样本只有一个正确的标签$y_i$。但是如果$y_i$是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥呢？比如在Instagram上的图片，就可以看成是被一个巨大的标签集合中的某个子集打上标签，一张图片上可能有多个标签。在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。例如，针对每个分类的二分类器会采用下面的公式：</p>
<center>$$L_i = \sum_j \max(0, 1 - y_{ij} f_j)$$</center>

<p>上式中，求和是对所有分类$j$，$y_{ij}$的值为1或者-1，具体根据第i个样本是否被第j个属性打标签而定，当该类别被正确预测并展示的时候，分值向量$f_j$为正，其余情况为负。可以发现，当一个正样本的得分小于+1，或者一个负样本得分大于-1的时候，算法就会累计损失值。</p>
<p>另一种方法是对每种属性训练一个独立的逻辑回归分类器。二分类的逻辑回归分类器只有两个分类（0，1），其中对于分类1的概率计算为：</p>
<center>$$P(y = 1 \mid x; w, b) = \frac{1}{1 + e^{-(w^Tx +b)}} = \sigma (w^Tx + b)$$</center>

<p>因为类别0和类别1的概率和为1，所以类别0的概率为：$\displaystyle P(y=0|x;w,b)=1-P(y=1|x;w,b)$。这样，如果$\sigma(w^Tx+b)&gt;0.5$或者$w^Tx+b&gt;0$，那么样本就要被分类成为正样本（y=1）。然后损失函数最大化这个对数似然函数，问题可以简化为：</p>
<center>$$L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))$$</center>

<p>上式中，假设标签$y_{ij}$非0即1，$\sigma(.)$就是sigmoid函数。上面的公式看起来吓人，但是$f$的梯度实际上非常简单：$\displaystyle \frac{\partial L_i}{\partial f_j}=y_{ij}-\sigma(f_j)$（你可以自己求导来验证）。</p>
<p><strong>回归问题</strong> 是预测实数的值的问题，比如预测房价，预测图片中某个东西的长度等。对于这种问题，通常是计算预测值和真实值之间的损失。然后用L2平方范式或L1范式度量差异。对于某个样本，L2范式计算如下：</p>
<center>$$L_i = \Vert f - y_i \Vert_2^2$$</center>

<p>之所以在目标函数中要进行平方，是因为梯度算起来更加简单。因为平方是一个单调运算，所以不用改变最优参数。L1范式则是要将每个维度上的绝对值加起来：</p>
<center>$$L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - (y_i)_j \mid$$</center>

<p>在上式中，如果有多个数量被预测了，就要对预测的所有维度的预测求和，即$\sum_j$。观察第i个样本的第j维，用$\delta_{ij}$表示预测值与真实值之间的差异。关于该维度的梯度（也就是$\partial L_i/\partial f_j$）能够轻松地通过被求导为L2范式的$\delta_{ij}$或$sign(\delta_{ij})$。这就是说，评分值的梯度要么与误差中的差值直接成比例，要么是固定的并从差值中继承sign。</p>
<p><em>注意</em>：L2损失比起较为稳定的Softmax损失来，其最优化过程要困难很多。直观而言，它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。而在Softmax中就不是这样，每个评分的准确值并不是那么重要：只有当它们量级适当的时候，才有意义。还有，L2损失鲁棒性不好，因为异常值可以导致很大的梯度。所以在面对一个回归问题时，先考虑将输出变成二值化是否真的不够用。例如，如果对一个产品的星级进行预测，使用5个独立的分类器来对1-5星进行打分的效果一般比使用一个回归损失要好很多。分类还有一个额外优点，就是能给出关于回归的输出的分布，而不是一个简单的毫无把握的输出值。如果确信分类不适用，那么使用L2损失吧，但是一定要谨慎：L2非常脆弱，在网络中使用随机失活（尤其是在L2损失层的上一层）不是好主意。</p>
<blockquote>
<p>当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。</p>
</blockquote>
<p><strong>结构化预测（structured prediction）</strong> 结构化损失是指标签可以是任意的结构，例如图表、树或者其他复杂物体的情况。通常这种情况还会假设结构空间非常巨大，不容易进行遍历。结构化SVM背后的基本思想就是在正确的结构$y_i$和得分最高的非正确结构之间画出一个边界。解决这类问题，并不是像解决一个简单无限制的最优化问题那样使用梯度下降就可以了，而是需要设计一些特殊的解决方案，这样可以有效利用对于结构空间的特殊简化假设。我们简要地提一下这个问题，但是详细内容就超出本课程范围。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>小结如下：</p>
<ul>
<li>推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1,1]范围之内。</li>
<li>使用标准差为$\sqrt{2/n}$的高斯分布来初始化权重，其中<img src="http://www.zhihu.com/equation?tex=n" alt="n">是输入的神经元数。例如用numpy可以写作：<strong>w = np.random.randn(n) * sqrt(2.0/n)</strong>。</li>
<li>使用L2正则化和随机失活的倒置版本。</li>
<li>使用批量归一化。</li>
<li>讨论了在实践中可能要面对的不同任务，以及每个任务对应的常用损失函数。</li>
</ul>
<p>现在，我们预处理了数据，初始化了模型。在下一节中，我们将讨论算法的学习过程及其运作特性。</p>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener"><strong>Neural Nets notes 2</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改</p>
<p>知乎地址：<a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/leaf.jpg"
                alt="Heroinlin"/>
            
              <p class="site-author-name" itemprop="name">Heroinlin</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">35</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">11</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">59</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/heroinlin" title="GitHub &rarr; https://github.com/heroinlin" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="/heroinlj@gmail.com" title="E-Mail &rarr; heroinlj@gmail.com"><i class="fa fa-fw fa-gmail"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://twitter.com/Heroin" title="Twitter &rarr; https://twitter.com/Heroin" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Heroinlin</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.6.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.1.2</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/affix.js?v=7.1.2"></script>

  <script src="/js/schemes/pisces.js?v=7.1.2"></script>



  

  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  



  




  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
