<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222"/>


















  
  
  <link rel="stylesheet" href="/lib/fancybox/source/jquery.fancybox.css"/>







<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0"/>

<link rel="stylesheet" href="/css/main.css?v=7.1.2"/>


  <link rel="apple-touch-icon" sizes="180x180" href="/images/leaf.jpg?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/leaf.ico?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/leaf1.ico?v=7.1.2">


  <link rel="mask-icon" href="/images/leaf.jpg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Heroinlin&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/3/index.html">
<meta property="og:site_name" content="Heroinlin&#39;s Blog">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Heroinlin&#39;s Blog">





  
  
  <link rel="canonical" href="http://yoursite.com/page/3/"/>



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Heroinlin's Blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Heroinlin's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br/>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br/>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br/>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br/>归档</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-schedule">

    
    
    
      
    

    

    <a href="/schedule/" rel="section"><i class="menu-item-icon fa fa-fw fa-calendar"></i> <br/>日程表</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/12/Python/Python_String/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/12/Python/Python_String/" class="post-title-link" itemprop="url">Python之String模块</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-12 10:48:11 / 修改时间：10:51:28" itemprop="dateCreated datePublished" datetime="2018-03-12T10:48:11+08:00">2018-03-12</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>#Python之String模块</p>
<p>本部分是对python中的string模块进行解析，添加函数中文说明与实例说明</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""A collection of string operations (most are no longer used).</span></span><br><span class="line"><span class="string">Warning: most of the code you see here isn't normally used nowadays.</span></span><br><span class="line"><span class="string">Beginning with Python 1.6, many of these functions are implemented as</span></span><br><span class="line"><span class="string">methods on the standard string object. They used to be implemented by</span></span><br><span class="line"><span class="string">a built-in module called strop, but strop is now obsolete itself.</span></span><br><span class="line"><span class="string">Public module variables:</span></span><br><span class="line"><span class="string">whitespace -- a string containing all characters considered whitespace</span></span><br><span class="line"><span class="string">lowercase -- a string containing all characters considered lowercase letters</span></span><br><span class="line"><span class="string">uppercase -- a string containing all characters considered uppercase letters</span></span><br><span class="line"><span class="string">letters -- a string containing all characters considered letters</span></span><br><span class="line"><span class="string">digits -- a string containing all characters considered decimal digits</span></span><br><span class="line"><span class="string">hexdigits -- a string containing all characters considered hexadecimal digits</span></span><br><span class="line"><span class="string">octdigits -- a string containing all characters considered octal digits</span></span><br><span class="line"><span class="string">punctuation -- a string containing all characters considered punctuation</span></span><br><span class="line"><span class="string">printable -- a string containing all characters considered printable</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Some strings for ctype-style character classification</span></span><br><span class="line">whitespace = <span class="string">' \t\n\r\v\f'</span></span><br><span class="line">lowercase = <span class="string">'abcdefghijklmnopqrstuvwxyz'</span></span><br><span class="line">uppercase = <span class="string">'ABCDEFGHIJKLMNOPQRSTUVWXYZ'</span></span><br><span class="line">letters = lowercase + uppercase</span><br><span class="line">ascii_lowercase = lowercase</span><br><span class="line">ascii_uppercase = uppercase</span><br><span class="line">ascii_letters = ascii_lowercase + ascii_uppercase</span><br><span class="line">digits = <span class="string">'0123456789'</span></span><br><span class="line">hexdigits = digits + <span class="string">'abcdef'</span> + <span class="string">'ABCDEF'</span></span><br><span class="line">octdigits = <span class="string">'01234567'</span></span><br><span class="line">punctuation = <span class="string">"""!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`&#123;|&#125;~"""</span></span><br><span class="line">printable = digits + letters + punctuation + whitespace</span><br><span class="line"></span><br><span class="line"><span class="comment"># Case conversion helpers</span></span><br><span class="line"><span class="comment"># Use str to convert Unicode literal in case of -U</span></span><br><span class="line">l = map(chr, xrange(<span class="number">256</span>))</span><br><span class="line">_idmap = str(<span class="string">''</span>).join(l)</span><br><span class="line"><span class="keyword">del</span> l</span><br><span class="line"></span><br><span class="line"><span class="comment"># Functions which aren't available as string methods.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Capitalize the words in a string, e.g. " aBc  dEf " -&gt; "Abc Def".</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">capwords</span><span class="params">(s, sep=None)</span>:</span></span><br><span class="line">    <span class="string">"""capwords(s [,sep]) -&gt; string</span></span><br><span class="line"><span class="string">    用split拆分参数转换为单词，利用capitalize使单词首字母大写，并且用join连接这些单词。</span></span><br><span class="line"><span class="string">    如果可选的第二个参数sep是缺省或无，以单个空格代替一串空白字符串，</span></span><br><span class="line"><span class="string">    开头和结尾的空格被删除，否则以sep为分隔符来分割和连接单词.</span></span><br><span class="line"><span class="string">    如string.capwords("   nice   to   meet  you     "),输出为：Nice To Meet You</span></span><br><span class="line"><span class="string">    如string.capwords(" niceto  meet  you ","e"),输出为： niceTo  meeT  you</span></span><br><span class="line"><span class="string">    Split the argument into words using split, capitalize each</span></span><br><span class="line"><span class="string">    word using capitalize, and join the capitalized words using</span></span><br><span class="line"><span class="string">    join.  If the optional second argument sep is absent or None,</span></span><br><span class="line"><span class="string">    runs of whitespace characters are replaced by a single space</span></span><br><span class="line"><span class="string">    and leading and trailing whitespace are removed, otherwise</span></span><br><span class="line"><span class="string">    sep is used to split and join the words.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> (sep <span class="keyword">or</span> <span class="string">' '</span>).join(x.capitalize() <span class="keyword">for</span> x <span class="keyword">in</span> s.split(sep))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Construct a translation string</span></span><br><span class="line">_idmapL = <span class="keyword">None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">maketrans</span><span class="params">(fromstr, tostr)</span>:</span></span><br><span class="line">    <span class="string">"""maketrans(frm, to) -&gt; string</span></span><br><span class="line"><span class="string">    返回一个转换表适用于string.translate使用（字符串长256字节）。字符串frm和to必须具有相同的长度</span></span><br><span class="line"><span class="string">    Return a translation table (a string of 256 bytes long)</span></span><br><span class="line"><span class="string">    suitable for use in string.translate.  The strings frm and to</span></span><br><span class="line"><span class="string">    must be of the same length.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> len(fromstr) != len(tostr):</span><br><span class="line">        <span class="keyword">raise</span> ValueError, <span class="string">"maketrans arguments must have same length"</span></span><br><span class="line">    <span class="keyword">global</span> _idmapL</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> _idmapL:</span><br><span class="line">        _idmapL = list(_idmap)</span><br><span class="line">    L = _idmapL[:]</span><br><span class="line">    fromstr = map(ord, fromstr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(fromstr)):</span><br><span class="line">        L[fromstr[i]] = tostr[i]</span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(L)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">####################################################################</span></span><br><span class="line"><span class="keyword">import</span> re <span class="keyword">as</span> _re</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_multimap</span>:</span></span><br><span class="line">    <span class="string">"""Helper class for combining multiple mappings.</span></span><br><span class="line"><span class="string">    Used by .&#123;safe_,&#125;substitute() to combine the mapping and keyword</span></span><br><span class="line"><span class="string">    arguments.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, primary, secondary)</span>:</span></span><br><span class="line">        self._primary = primary</span><br><span class="line">        self._secondary = secondary</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">return</span> self._primary[key]</span><br><span class="line">        <span class="keyword">except</span> KeyError:</span><br><span class="line">            <span class="keyword">return</span> self._secondary[key]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">_TemplateMetaclass</span><span class="params">(type)</span>:</span></span><br><span class="line">    pattern = <span class="string">r"""</span></span><br><span class="line"><span class="string">    %(delim)s(?:</span></span><br><span class="line"><span class="string">      (?P&lt;escaped&gt;%(delim)s) |   # Escape sequence of two delimiters</span></span><br><span class="line"><span class="string">      (?P&lt;named&gt;%(id)s)      |   # delimiter and a Python identifier</span></span><br><span class="line"><span class="string">      &#123;(?P&lt;braced&gt;%(id)s)&#125;   |   # delimiter and a braced identifier</span></span><br><span class="line"><span class="string">      (?P&lt;invalid&gt;)              # Other ill-formed delimiter exprs</span></span><br><span class="line"><span class="string">    )</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(cls, name, bases, dct)</span>:</span></span><br><span class="line">        super(_TemplateMetaclass, cls).__init__(name, bases, dct)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'pattern'</span> <span class="keyword">in</span> dct:</span><br><span class="line">            pattern = cls.pattern</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pattern = _TemplateMetaclass.pattern % &#123;</span><br><span class="line">                <span class="string">'delim'</span> : _re.escape(cls.delimiter),</span><br><span class="line">                <span class="string">'id'</span>    : cls.idpattern,</span><br><span class="line">                &#125;</span><br><span class="line">        cls.pattern = _re.compile(pattern, _re.IGNORECASE | _re.VERBOSE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Template</span>:</span></span><br><span class="line">    <span class="string">"""A string class for supporting $-substitutions."""</span></span><br><span class="line">    __metaclass__ = _TemplateMetaclass</span><br><span class="line"></span><br><span class="line">    delimiter = <span class="string">'$'</span></span><br><span class="line">    idpattern = <span class="string">r'[_a-z][_a-z0-9]*'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, template)</span>:</span></span><br><span class="line">        self.template = template</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Search for $$, $identifier, $&#123;identifier&#125;, and any bare $'s</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_invalid</span><span class="params">(self, mo)</span>:</span></span><br><span class="line">        i = mo.start(<span class="string">'invalid'</span>)</span><br><span class="line">        lines = self.template[:i].splitlines(<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> lines:</span><br><span class="line">            colno = <span class="number">1</span></span><br><span class="line">            lineno = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            colno = i - len(<span class="string">''</span>.join(lines[:<span class="number">-1</span>]))</span><br><span class="line">            lineno = len(lines)</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid placeholder in string: line %d, col %d'</span> %</span><br><span class="line">                         (lineno, colno))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">substitute</span><span class="params">(*args, **kws)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"descriptor 'substitute' of 'Template' object "</span></span><br><span class="line">                            <span class="string">"needs an argument"</span>)</span><br><span class="line">        self, args = args[<span class="number">0</span>], args[<span class="number">1</span>:]  <span class="comment"># allow the "self" keyword be passed</span></span><br><span class="line">        <span class="keyword">if</span> len(args) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'Too many positional arguments'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            mapping = kws</span><br><span class="line">        <span class="keyword">elif</span> kws:</span><br><span class="line">            mapping = _multimap(kws, args[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mapping = args[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Helper function for .sub()</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(mo)</span>:</span></span><br><span class="line">            <span class="comment"># Check the most common path first.</span></span><br><span class="line">            named = mo.group(<span class="string">'named'</span>) <span class="keyword">or</span> mo.group(<span class="string">'braced'</span>)</span><br><span class="line">            <span class="keyword">if</span> named <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                val = mapping[named]</span><br><span class="line">                <span class="comment"># We use this idiom instead of str() because the latter will</span></span><br><span class="line">                <span class="comment"># fail if val is a Unicode containing non-ASCII characters.</span></span><br><span class="line">                <span class="keyword">return</span> <span class="string">'%s'</span> % (val,)</span><br><span class="line">            <span class="keyword">if</span> mo.group(<span class="string">'escaped'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> self.delimiter</span><br><span class="line">            <span class="keyword">if</span> mo.group(<span class="string">'invalid'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                self._invalid(mo)</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Unrecognized named group in pattern'</span>,</span><br><span class="line">                             self.pattern)</span><br><span class="line">        <span class="keyword">return</span> self.pattern.sub(convert, self.template)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">safe_substitute</span><span class="params">(*args, **kws)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"descriptor 'safe_substitute' of 'Template' object "</span></span><br><span class="line">                            <span class="string">"needs an argument"</span>)</span><br><span class="line">        self, args = args[<span class="number">0</span>], args[<span class="number">1</span>:]  <span class="comment"># allow the "self" keyword be passed</span></span><br><span class="line">        <span class="keyword">if</span> len(args) &gt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'Too many positional arguments'</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            mapping = kws</span><br><span class="line">        <span class="keyword">elif</span> kws:</span><br><span class="line">            mapping = _multimap(kws, args[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            mapping = args[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># Helper function for .sub()</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(mo)</span>:</span></span><br><span class="line">            named = mo.group(<span class="string">'named'</span>) <span class="keyword">or</span> mo.group(<span class="string">'braced'</span>)</span><br><span class="line">            <span class="keyword">if</span> named <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="comment"># We use this idiom instead of str() because the latter</span></span><br><span class="line">                    <span class="comment"># will fail if val is a Unicode containing non-ASCII</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="string">'%s'</span> % (mapping[named],)</span><br><span class="line">                <span class="keyword">except</span> KeyError:</span><br><span class="line">                    <span class="keyword">return</span> mo.group()</span><br><span class="line">            <span class="keyword">if</span> mo.group(<span class="string">'escaped'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> self.delimiter</span><br><span class="line">            <span class="keyword">if</span> mo.group(<span class="string">'invalid'</span>) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> mo.group()</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Unrecognized named group in pattern'</span>,</span><br><span class="line">                             self.pattern)</span><br><span class="line">        <span class="keyword">return</span> self.pattern.sub(convert, self.template)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">####################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> Everything below here is deprecated.  Use string methods instead.</span></span><br><span class="line"><span class="comment"># This stuff will go away in Python 3.0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward compatible names for exceptions</span></span><br><span class="line">index_error = ValueError</span><br><span class="line">atoi_error = ValueError</span><br><span class="line">atof_error = ValueError</span><br><span class="line">atol_error = ValueError</span><br><span class="line"></span><br><span class="line"><span class="comment"># convert UPPER CASE letters to lower case</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lower</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""lower(s) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s转换为小写的副本。</span></span><br><span class="line"><span class="string">    Return a copy of the string s converted to lowercase.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.lower()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert lower case letters to UPPER CASE</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">upper</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""upper(s) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s转换为大写的副本。</span></span><br><span class="line"><span class="string">    Return a copy of the string s converted to uppercase.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.upper()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Swap lower case letters and UPPER CASE</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">swapcase</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""swapcase(s) -&gt; string</span></span><br><span class="line"><span class="string">    返回的字符串s的副本,大写字符转换为小写，反之亦然。</span></span><br><span class="line"><span class="string">    Return a copy of the string s with upper case characters</span></span><br><span class="line"><span class="string">    converted to lowercase and vice versa.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.swapcase()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Strip leading and trailing tabs and spaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strip</span><span class="params">(s, chars=None)</span>:</span></span><br><span class="line">    <span class="string">"""strip(s [,chars]) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s的副本，开头和结尾的空格去掉。</span></span><br><span class="line"><span class="string">    如果chars给出，删除s开头和结尾的chars字符串，如string.strip("as1asdgas","as"),输出为：1asdg</span></span><br><span class="line"><span class="string">    Return a copy of the string s with leading and trailing</span></span><br><span class="line"><span class="string">    whitespace removed.</span></span><br><span class="line"><span class="string">    If chars is given and not None, remove characters in chars instead.</span></span><br><span class="line"><span class="string">    If chars is unicode, S will be converted to unicode before stripping.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.strip(chars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Strip leading tabs and spaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstrip</span><span class="params">(s, chars=None)</span>:</span></span><br><span class="line">    <span class="string">"""lstrip(s [,chars]) -&gt; string</span></span><br><span class="line"><span class="string">    返回的字符串s的副本,开头空格删除。</span></span><br><span class="line"><span class="string">    如果字符给出，而不是None，删除s开头的chars字符串。如string.lstrip("as1asdgas","as"),输出为：1asdgas</span></span><br><span class="line"><span class="string">    Return a copy of the string s with leading whitespace removed.</span></span><br><span class="line"><span class="string">    If chars is given and not None, remove characters in chars instead.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.lstrip(chars)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Strip trailing tabs and spaces</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rstrip</span><span class="params">(s, chars=None)</span>:</span></span><br><span class="line">    <span class="string">"""rstrip(s [,chars]) -&gt; string</span></span><br><span class="line"><span class="string">    返回的字符串s的副本,结尾空格删除。</span></span><br><span class="line"><span class="string">    如果字符给出，而不是None，删除s结尾的chars字符串。如string.rstrip("as1asdgas","as"),输出为：as1asdg</span></span><br><span class="line"><span class="string">    Return a copy of the string s with trailing whitespace removed.</span></span><br><span class="line"><span class="string">    If chars is given and not None, remove characters in chars instead.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rstrip(chars)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Split a string into a list of space/tab-separated words</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(s, sep=None, maxsplit=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""split(s [,sep [,maxsplit]]) -&gt; list of strings</span></span><br><span class="line"><span class="string">    返回字符串s中单词的列表，用sep作为字符串的分隔符。如果maxsplit给出，分割在不超过第</span></span><br><span class="line"><span class="string">    maxsplit个分隔符的位置（结果至多为maxsplit+ 1个单词）。如果sep未指定或为None，以空白字符串作为分隔符。</span></span><br><span class="line"><span class="string">    如string.split("a  ds sd"),输出为['a','ds','sd']</span></span><br><span class="line"><span class="string">    string.split("a  ds sd",maxsplit=1),输出为['a','ds sd']</span></span><br><span class="line"><span class="string">    Return a list of the words in the string s, using sep as the</span></span><br><span class="line"><span class="string">    delimiter string.  If maxsplit is given, splits at no more than</span></span><br><span class="line"><span class="string">    maxsplit places (resulting in at most maxsplit+1 words).  If sep</span></span><br><span class="line"><span class="string">    is not specified or is None, any whitespace string is a separator.</span></span><br><span class="line"><span class="string">    (split and splitfields are synonymous)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.split(sep, maxsplit)</span><br><span class="line">splitfields = split</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split a string into a list of space/tab-separated words</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rsplit</span><span class="params">(s, sep=None, maxsplit=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""rsplit(s [,sep [,maxsplit]]) -&gt; list of strings</span></span><br><span class="line"><span class="string">    返回字符串s中单词的列表，用sep作为字符串的分隔符,分割符从字符串s尾部开始算起。如果maxsplit给出，分割在不超过第</span></span><br><span class="line"><span class="string">    maxsplit个分隔符的位置（结果至多为maxsplit+ 1个单词）。如果sep未指定或为None，以空白字符串作为分隔符。</span></span><br><span class="line"><span class="string">    如string.rsplit("a  ds sd"),输出为['a','ds','sd']</span></span><br><span class="line"><span class="string">    string.rsplit("a  ds sd",maxsplit=1),输出为['a ds','sd']</span></span><br><span class="line"><span class="string">    Return a list of the words in the string s, using sep as the</span></span><br><span class="line"><span class="string">    delimiter string, starting at the end of the string and working</span></span><br><span class="line"><span class="string">    to the front.  If maxsplit is given, at most maxsplit splits are</span></span><br><span class="line"><span class="string">    done. If sep is not specified or is None, any whitespace string</span></span><br><span class="line"><span class="string">    is a separator.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rsplit(sep, maxsplit)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Join fields with optional separator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span><span class="params">(words, sep = <span class="string">' '</span>)</span>:</span></span><br><span class="line">    <span class="string">"""join(list [,sep]) -&gt; string</span></span><br><span class="line"><span class="string">    返回列表中的单词所组成字符串，以sep相串联。默认的分隔符是一个空格。</span></span><br><span class="line"><span class="string">    如string.join(["nice","to", "meet","you"])，输出为：nice to meet you</span></span><br><span class="line"><span class="string">    Return a string composed of the words in list, with</span></span><br><span class="line"><span class="string">    intervening occurrences of sep.  The default separator is a</span></span><br><span class="line"><span class="string">    single space.</span></span><br><span class="line"><span class="string">    (joinfields and join are synonymous)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> sep.join(words)</span><br><span class="line">joinfields = join</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find substring, raise exception if not found</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">index</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""index(s, sub [,start [,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    类似find，但是没有找到子串时引发ValueError错误。返回子串出现的首位置，否则报错。</span></span><br><span class="line"><span class="string">    如string.index("a sd fg a","a"),输出为：0</span></span><br><span class="line"><span class="string">    如string.index("a sd fg a fg","sdsdfg"[4:5]),输出为:5</span></span><br><span class="line"><span class="string">    Like find but raises ValueError when the substring is not found.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.index(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find last substring, raise exception if not found</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rindex</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""rindex(s, sub [,start [,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    类似rfind，但是没有找到子串时引发ValueError错误。返回子串出现的最后位置，否则报错。</span></span><br><span class="line"><span class="string">    如string.rindex("a sd fg a","a"),输出为：8</span></span><br><span class="line"><span class="string">    如string.rindex("a sd fg a fg","sdsdfg"[4:5]),输出为:10</span></span><br><span class="line"><span class="string">    Like rfind but raises ValueError when the substring is not found.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rindex(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Count non-overlapping occurrences of substring</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""count(s, sub[, start[,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    返回字符串s[start:end]中的子串sub出现的次数.可选参数start和end都解释为片符号。</span></span><br><span class="line"><span class="string">    如string.count("a sd fg a","a"),输出为2</span></span><br><span class="line"><span class="string">    如string.count("a sd fg a fg","sdsdfg"[2:3]),输出为:1</span></span><br><span class="line"><span class="string">    Return the number of occurrences of substring sub in string</span></span><br><span class="line"><span class="string">    s[start:end].  Optional arguments start and end are</span></span><br><span class="line"><span class="string">    interpreted as in slice notation.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.count(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find substring, return -1 if not found</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""find(s, sub [,start [,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    返回子串出现的首位置，子串包含在s[start,end]中(即子串sub长度不超过s).可选参数start和end都解释为片符号。不存在是返回-1</span></span><br><span class="line"><span class="string">    如string.find("a sd fg a","a"),输出为：0</span></span><br><span class="line"><span class="string">    如string.find("a sd fg a fg","sdsdfg"[4:5]),输出为:5</span></span><br><span class="line"><span class="string">    Return the lowest index in s where substring sub is found,</span></span><br><span class="line"><span class="string">    such that sub is contained within s[start,end].  Optional</span></span><br><span class="line"><span class="string">    arguments start and end are interpreted as in slice notation.</span></span><br><span class="line"><span class="string">    Return -1 on failure.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.find(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Find last substring, return -1 if not found</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rfind</span><span class="params">(s, *args)</span>:</span></span><br><span class="line">    <span class="string">"""rfind(s, sub [,start [,end]]) -&gt; int</span></span><br><span class="line"><span class="string">    返回子串出现的最后位置，子串包含在s[start,end]中(即子串sub长度不超过s).可选参数start和end都解释为片符号。不存在是返回-1</span></span><br><span class="line"><span class="string">    如string.rfind("a sd fg a","a"),输出为：8</span></span><br><span class="line"><span class="string">    如string.rfind("a sd fg a fg","sdsdfg"[4:5]),输出为:10</span></span><br><span class="line"><span class="string">    Return the highest index in s where substring sub is found,</span></span><br><span class="line"><span class="string">    such that sub is contained within s[start,end].  Optional</span></span><br><span class="line"><span class="string">    arguments start and end are interpreted as in slice notation.</span></span><br><span class="line"><span class="string">    Return -1 on failure.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rfind(*args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># for a bit of speed</span></span><br><span class="line">_float = float</span><br><span class="line">_int = int</span><br><span class="line">_long = long</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert string to float</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atof</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""atof(s) -&gt; float</span></span><br><span class="line"><span class="string">    Return the floating point number represented by the string s.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _float(s)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert string to integer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atoi</span><span class="params">(s , base=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">"""atoi(s [,base]) -&gt; int</span></span><br><span class="line"><span class="string">    Return the integer represented by the string s in the given</span></span><br><span class="line"><span class="string">    base, which defaults to 10.  The string s must consist of one</span></span><br><span class="line"><span class="string">    or more digits, possibly preceded by a sign.  If base is 0, it</span></span><br><span class="line"><span class="string">    is chosen from the leading characters of s, 0 for octal, 0x or</span></span><br><span class="line"><span class="string">    0X for hexadecimal.  If base is 16, a preceding 0x or 0X is</span></span><br><span class="line"><span class="string">    accepted.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _int(s, base)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert string to long integer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atol</span><span class="params">(s, base=<span class="number">10</span>)</span>:</span></span><br><span class="line">    <span class="string">"""atol(s [,base]) -&gt; long</span></span><br><span class="line"><span class="string">    Return the long integer represented by the string s in the</span></span><br><span class="line"><span class="string">    given base, which defaults to 10.  The string s must consist</span></span><br><span class="line"><span class="string">    of one or more digits, possibly preceded by a sign.  If base</span></span><br><span class="line"><span class="string">    is 0, it is chosen from the leading characters of s, 0 for</span></span><br><span class="line"><span class="string">    octal, 0x or 0X for hexadecimal.  If base is 16, a preceding</span></span><br><span class="line"><span class="string">    0x or 0X is accepted.  A trailing L or l is not accepted,</span></span><br><span class="line"><span class="string">    unless base is 0.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> _long(s, base)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Left-justify a string</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ljust</span><span class="params">(s, width, *args)</span>:</span></span><br><span class="line">    <span class="string">"""ljust(s, width[, fillchar]) -&gt; string</span></span><br><span class="line"><span class="string">    返回s的左对齐的版本，在该场指定宽度，可以根据需要用空格填充。该字符串不会被截断。如果指定了fillchar,以此代替空格。</span></span><br><span class="line"><span class="string">    如string.ljust(" adf", 8,"s"),输出为： adfssss</span></span><br><span class="line"><span class="string">    如string.ljust(" adfsfsfsfa", 3,"s"),(体现出不被截断)输出为 adfsfsfsfa</span></span><br><span class="line"><span class="string">    Return a left-justified version of s, in a field of the</span></span><br><span class="line"><span class="string">    specified width, padded with spaces as needed.  The string is</span></span><br><span class="line"><span class="string">    never truncated.  If specified the fillchar is used instead of spaces.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.ljust(width, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Right-justify a string</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rjust</span><span class="params">(s, width, *args)</span>:</span></span><br><span class="line">    <span class="string">"""rjust(s, width[, fillchar]) -&gt; string</span></span><br><span class="line"><span class="string">    返回s的右对齐的版本，在该场指定宽度，可以根据需要用空格填充。该字符串不会被截断。如果指定了fillchar,以此代替空格。</span></span><br><span class="line"><span class="string">    如string.rjust(" adf", 8,"s"),输出为：ssss adf</span></span><br><span class="line"><span class="string">    如string.rjust(" adfsfsfsfa", 3,"s"),(体现出不被截断)输出为 adfsfsfsfa</span></span><br><span class="line"><span class="string">    Return a right-justified version of s, in a field of the</span></span><br><span class="line"><span class="string">    specified width, padded with spaces as needed.  The string is</span></span><br><span class="line"><span class="string">    never truncated.  If specified the fillchar is used instead of spaces.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.rjust(width, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Center a string</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">center</span><span class="params">(s, width, *args)</span>:</span></span><br><span class="line">    <span class="string">"""center(s, width[, fillchar]) -&gt; string</span></span><br><span class="line"><span class="string">    返回s的中心对齐的版本，在该场指定宽度，可以根据需要用空格填充。该字符串不会被截断。如果指定了fillchar,以此代替空格。</span></span><br><span class="line"><span class="string">    如string.center(" adf", 9,"s"),输出为：sss adfss</span></span><br><span class="line"><span class="string">    如string.center(" adfsfsfsfa", 3,"s"),(体现出不被截断)输出为 adfsfsfsfa</span></span><br><span class="line"><span class="string">    Return a center version of s, in a field of the specified</span></span><br><span class="line"><span class="string">    width. padded with spaces as needed.  The string is never</span></span><br><span class="line"><span class="string">    truncated.  If specified the fillchar is used instead of spaces.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.center(width, *args)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Zero-fill a number, e.g., (12, 3) --&gt; '012' and (-3, 3) --&gt; '-03'</span></span><br><span class="line"><span class="comment"># Decadent feature: the argument may be a string or a number</span></span><br><span class="line"><span class="comment"># (Use of this is deprecated; it should be a string as with ljust c.s.)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zfill</span><span class="params">(x, width)</span>:</span></span><br><span class="line">    <span class="string">"""zfill(x, width) -&gt; string</span></span><br><span class="line"><span class="string">    在字符串x左边，填充0达到指定的宽度。该字符串x不会被截断。</span></span><br><span class="line"><span class="string">    如string.zfill(" adf", 9),输出为：00000 adf</span></span><br><span class="line"><span class="string">    Pad a numeric string x with zeros on the left, to fill a field</span></span><br><span class="line"><span class="string">    of the specified width.  The string x is never truncated.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(x, basestring):</span><br><span class="line">        x = repr(x)</span><br><span class="line">    <span class="keyword">return</span> x.zfill(width)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Expand tabs in a string.</span></span><br><span class="line"><span class="comment"># Doesn't take non-printing chars into account, but does understand \n.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">expandtabs</span><span class="params">(s, tabsize=<span class="number">8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""expandtabs(s [,tabsize]) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s的副本，所有的制表符(tab)由适当数量的空格替代，取决于当前列和制表符大小（默认为8）</span></span><br><span class="line"><span class="string">    Return a copy of the string s with all tab characters replaced</span></span><br><span class="line"><span class="string">    by the appropriate number of spaces, depending on the current</span></span><br><span class="line"><span class="string">    column, and the tabsize (default 8).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.expandtabs(tabsize)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Character translation through look-up table.</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(s, table, deletions=<span class="string">""</span>)</span>:</span></span><br><span class="line">    <span class="string">"""translate(s,table [,deletions]) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s的副本，可选参数deletions出现的所有字符被删除，剩下的字符通过给定的转换表来映射，</span></span><br><span class="line"><span class="string">    转换表必须是长度为256的字符串.deletions不允许Unicode字符串</span></span><br><span class="line"><span class="string">    如t=string.maketrans('abc','ABC'),string.translate("abc123",t,'12'),输出为：ABC3</span></span><br><span class="line"><span class="string">    Return a copy of the string s, where all characters occurring</span></span><br><span class="line"><span class="string">    in the optional argument deletions are removed, and the</span></span><br><span class="line"><span class="string">    remaining characters have been mapped through the given</span></span><br><span class="line"><span class="string">    translation table, which must be a string of length 256.  The</span></span><br><span class="line"><span class="string">    deletions argument is not allowed for Unicode strings.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> deletions <span class="keyword">or</span> table <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">return</span> s.translate(table, deletions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># Add s[:0] so that if s is Unicode and table is an 8-bit string,</span></span><br><span class="line">        <span class="comment"># table is converted to Unicode.  This means that table *cannot*</span></span><br><span class="line">        <span class="comment"># be a dictionary -- for that feature, use u.translate() directly.</span></span><br><span class="line">        <span class="keyword">return</span> s.translate(table + s[:<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Capitalize a string, e.g. "aBc  dEf" -&gt; "Abc  def".</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">capitalize</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="string">"""capitalize(s) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串s的副本，只有首字符大写</span></span><br><span class="line"><span class="string">    Return a copy of the string s with only its first character</span></span><br><span class="line"><span class="string">    capitalized.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.capitalize()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Substring replacement (global)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace</span><span class="params">(s, old, new, maxreplace=<span class="number">-1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""replace (str, old, new[, maxreplace]) -&gt; string</span></span><br><span class="line"><span class="string">    返回字符串str的副本，以子串new代替所有出现的子串old。如果可选参数maxreplace给出，只有第一个maxreplace出现的地方被替换</span></span><br><span class="line"><span class="string">    如string.replace("old ffa old fsda old", "old", "new",2),输出为：new ffa new fsda old</span></span><br><span class="line"><span class="string">    Return a copy of string str with all occurrences of substring</span></span><br><span class="line"><span class="string">    old replaced by new. If the optional argument maxreplace is</span></span><br><span class="line"><span class="string">    given, only the first maxreplace occurrences are replaced.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> s.replace(old, new, maxreplace)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Try importing optional built-in module "strop" -- if it exists,</span></span><br><span class="line"><span class="comment"># it redefines some string operations that are 100-1000 times faster.</span></span><br><span class="line"><span class="comment"># It also defines values for whitespace, lowercase and uppercase</span></span><br><span class="line"><span class="comment"># that match &lt;ctype.h&gt;'s definitions.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">from</span> strop <span class="keyword">import</span> maketrans, lowercase, uppercase, whitespace</span><br><span class="line">    letters = lowercase + uppercase</span><br><span class="line"><span class="keyword">except</span> ImportError:</span><br><span class="line">    <span class="keyword">pass</span>                                          <span class="comment"># Use the original versions</span></span><br><span class="line"></span><br><span class="line"><span class="comment">########################################################################</span></span><br><span class="line"><span class="comment"># the Formatter class</span></span><br><span class="line"><span class="comment"># see PEP 3101 for details and purpose of this class</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The hard parts are reused from the C implementation.  They're exposed as "_"</span></span><br><span class="line"><span class="comment"># prefixed methods of str and unicode.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The overall parser is implemented in str._formatter_parser.</span></span><br><span class="line"><span class="comment"># The field name parser is implemented in str._formatter_field_name_split</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Formatter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">format</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> args:</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"descriptor 'format' of 'Formatter' object "</span></span><br><span class="line">                            <span class="string">"needs an argument"</span>)</span><br><span class="line">        self, args = args[<span class="number">0</span>], args[<span class="number">1</span>:]  <span class="comment"># allow the "self" keyword be passed</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            format_string, args = args[<span class="number">0</span>], args[<span class="number">1</span>:] <span class="comment"># allow the "format_string" keyword be passed</span></span><br><span class="line">        <span class="keyword">except</span> IndexError:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'format_string'</span> <span class="keyword">in</span> kwargs:</span><br><span class="line">                format_string = kwargs.pop(<span class="string">'format_string'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> TypeError(<span class="string">"format() missing 1 required positional "</span></span><br><span class="line">                                <span class="string">"argument: 'format_string'"</span>)</span><br><span class="line">        <span class="keyword">return</span> self.vformat(format_string, args, kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vformat</span><span class="params">(self, format_string, args, kwargs)</span>:</span></span><br><span class="line">        used_args = set()</span><br><span class="line">        result = self._vformat(format_string, args, kwargs, used_args, <span class="number">2</span>)</span><br><span class="line">        self.check_unused_args(used_args, args, kwargs)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_vformat</span><span class="params">(self, format_string, args, kwargs, used_args, recursion_depth)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> recursion_depth &lt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">'Max string recursion exceeded'</span>)</span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">for</span> literal_text, field_name, format_spec, conversion <span class="keyword">in</span> \</span><br><span class="line">                self.parse(format_string):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># output the literal text</span></span><br><span class="line">            <span class="keyword">if</span> literal_text:</span><br><span class="line">                result.append(literal_text)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># if there's a field, output it</span></span><br><span class="line">            <span class="keyword">if</span> field_name <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="comment"># this is some markup, find the object and do</span></span><br><span class="line">                <span class="comment">#  the formatting</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># given the field_name, find the object it references</span></span><br><span class="line">                <span class="comment">#  and the argument it came from</span></span><br><span class="line">                obj, arg_used = self.get_field(field_name, args, kwargs)</span><br><span class="line">                used_args.add(arg_used)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># do any conversion on the resulting object</span></span><br><span class="line">                obj = self.convert_field(obj, conversion)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># expand the format spec, if needed</span></span><br><span class="line">                format_spec = self._vformat(format_spec, args, kwargs,</span><br><span class="line">                                            used_args, recursion_depth<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># format the object and append to the result</span></span><br><span class="line">                result.append(self.format_field(obj, format_spec))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span>.join(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span><span class="params">(self, key, args, kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(key, (int, long)):</span><br><span class="line">            <span class="keyword">return</span> args[key]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> kwargs[key]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">check_unused_args</span><span class="params">(self, used_args, args, kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">format_field</span><span class="params">(self, value, format_spec)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> format(value, format_spec)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert_field</span><span class="params">(self, value, conversion)</span>:</span></span><br><span class="line">        <span class="comment"># do any conversion on the resulting object</span></span><br><span class="line">        <span class="keyword">if</span> conversion <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> value</span><br><span class="line">        <span class="keyword">elif</span> conversion == <span class="string">'s'</span>:</span><br><span class="line">            <span class="keyword">return</span> str(value)</span><br><span class="line">        <span class="keyword">elif</span> conversion == <span class="string">'r'</span>:</span><br><span class="line">            <span class="keyword">return</span> repr(value)</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Unknown conversion specifier &#123;0!s&#125;"</span>.format(conversion))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># returns an iterable that contains tuples of the form:</span></span><br><span class="line">    <span class="comment"># (literal_text, field_name, format_spec, conversion)</span></span><br><span class="line">    <span class="comment"># literal_text can be zero length</span></span><br><span class="line">    <span class="comment"># field_name can be None, in which case there's no</span></span><br><span class="line">    <span class="comment">#  object to format and output</span></span><br><span class="line">    <span class="comment"># if field_name is not None, it is looked up, formatted</span></span><br><span class="line">    <span class="comment">#  with format_spec and conversion and then used</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, format_string)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> format_string._formatter_parser()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># given a field_name, find the object it references.</span></span><br><span class="line">    <span class="comment">#  field_name:   the field being looked up, e.g. "0.name"</span></span><br><span class="line">    <span class="comment">#                 or "lookup[3]"</span></span><br><span class="line">    <span class="comment">#  used_args:    a set of which args have been used</span></span><br><span class="line">    <span class="comment">#  args, kwargs: as passed in to vformat</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_field</span><span class="params">(self, field_name, args, kwargs)</span>:</span></span><br><span class="line">        first, rest = field_name._formatter_field_name_split()</span><br><span class="line"></span><br><span class="line">        obj = self.get_value(first, args, kwargs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loop through the rest of the field_name, doing</span></span><br><span class="line">        <span class="comment">#  getattr or getitem as needed</span></span><br><span class="line">        <span class="keyword">for</span> is_attr, i <span class="keyword">in</span> rest:</span><br><span class="line">            <span class="keyword">if</span> is_attr:</span><br><span class="line">                obj = getattr(obj, i)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                obj = obj[i]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> obj, first</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/12/Python/Python_selenium/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/12/Python/Python_selenium/" class="post-title-link" itemprop="url">python之Selenium</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-12 10:48:11" itemprop="dateCreated datePublished" datetime="2018-03-12T10:48:11+08:00">2018-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-10-12 17:13:10" itemprop="dateModified" datetime="2018-10-12T17:13:10+08:00">2018-10-12</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="python之Selenium打开浏览器"><a href="#python之Selenium打开浏览器" class="headerlink" title="python之Selenium打开浏览器"></a>python之Selenium打开浏览器</h1><h2 id="函数及属性"><a href="#函数及属性" class="headerlink" title="函数及属性"></a>函数及属性</h2><p>driver = webdriver.Chrome(chromeDriver)</p>
<h3 id="返回当前会话中的cookies"><a href="#返回当前会话中的cookies" class="headerlink" title="返回当前会话中的cookies"></a>返回当前会话中的cookies</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.get_cookies()</span><br></pre></td></tr></table></figure>
<h3 id="根据cookies-name查找"><a href="#根据cookies-name查找" class="headerlink" title="根据cookies name查找"></a>根据cookies name查找</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.get_cookie(cookiename)</span><br></pre></td></tr></table></figure>
<h3 id="截取当前页面"><a href="#截取当前页面" class="headerlink" title="截取当前页面"></a>截取当前页面</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_screenshot_as_file(filename)</span><br></pre></td></tr></table></figure>
<p>如：get_screenshot_as_file(“D:\nm.bmp”)</p>
<h3 id="获取当前窗口的坐标"><a href="#获取当前窗口的坐标" class="headerlink" title="获取当前窗口的坐标"></a>获取当前窗口的坐标</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.get_window_position()</span><br></pre></td></tr></table></figure>
<h3 id="获取当前窗口的长和宽"><a href="#获取当前窗口的长和宽" class="headerlink" title="获取当前窗口的长和宽"></a>获取当前窗口的长和宽</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.get_window_size()</span><br></pre></td></tr></table></figure>
<h3 id="获取当前页面的URL"><a href="#获取当前页面的URL" class="headerlink" title="获取当前页面的URL"></a>获取当前页面的URL</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.current_url</span><br></pre></td></tr></table></figure>
<h3 id="获取当前页面的title"><a href="#获取当前页面的title" class="headerlink" title="获取当前页面的title"></a>获取当前页面的title</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.title</span><br></pre></td></tr></table></figure>
<h3 id="获取元素的坐标"><a href="#获取元素的坐标" class="headerlink" title="获取元素的坐标"></a>获取元素的坐标</h3><p>先获取到该元素，然后调用location属性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">　driver.find_element_by_xpath(<span class="string">"//*[@id='tablechart']/tbody/tr[14]/td[9]"</span>).location</span><br></pre></td></tr></table></figure>
<h3 id="获取css的属性值"><a href="#获取css的属性值" class="headerlink" title="获取css的属性值"></a>获取css的属性值</h3><p>value_of_css_property(css_name)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_css_selector(<span class="string">"input.btn"</span>).value_of_css_property(<span class="string">"input.btn"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="获取元素的属性值"><a href="#获取元素的属性值" class="headerlink" title="获取元素的属性值"></a>获取元素的属性值</h3><p>get_attribute(element_name)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"sellaiyuan"</span>).get_attribute(<span class="string">"sellaiyuan"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="判断元素是否被选中"><a href="#判断元素是否被选中" class="headerlink" title="判断元素是否被选中"></a>判断元素是否被选中</h3><p>is_selected()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"form1"</span>).is_selected()</span><br></pre></td></tr></table></figure>
<h3 id="返回元素的大小"><a href="#返回元素的大小" class="headerlink" title="返回元素的大小"></a>返回元素的大小</h3><p>size</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"iptPassword"</span>).size</span><br><span class="line">size``返回值：&#123;<span class="string">'width'</span>: <span class="number">250</span>, <span class="string">'height'</span>: <span class="number">30</span>&#125;</span><br></pre></td></tr></table></figure>
<h3 id="判断元素是否显示："><a href="#判断元素是否显示：" class="headerlink" title="判断元素是否显示："></a>判断元素是否显示：</h3><p>is_displayed()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"iptPassword"</span>).is_displayed()</span><br></pre></td></tr></table></figure>
<h3 id="判断元素是否被使用方法"><a href="#判断元素是否被使用方法" class="headerlink" title="判断元素是否被使用方法"></a>判断元素是否被使用方法</h3><p>is_enabled()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"iptPassword"</span>).is_enabled()</span><br></pre></td></tr></table></figure>
<h3 id="获取元素的文本值方法"><a href="#获取元素的文本值方法" class="headerlink" title="获取元素的文本值方法"></a>获取元素的文本值方法</h3><p>text</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"iptUsername"</span>).text</span><br></pre></td></tr></table></figure>
<h3 id="元素赋值方法"><a href="#元素赋值方法" class="headerlink" title="元素赋值方法"></a>元素赋值方法</h3><p>send_keys(*values)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"iptUsername"</span>).send_keys(<span class="string">'admin'</span>)</span><br><span class="line">PS：``注意如果是中文需要增加转义符u，eg.``driver.find_element_by_id(<span class="string">"iptUsername"</span>).send_keys(<span class="string">u'青春'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="返回元素的标签名称"><a href="#返回元素的标签名称" class="headerlink" title="返回元素的标签名称"></a>返回元素的标签名称</h3><p>tag_name</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_element_by_id(<span class="string">"iptUsername"</span>).tag_name</span><br></pre></td></tr></table></figure>
<h3 id="删除浏览器所有的cookies"><a href="#删除浏览器所有的cookies" class="headerlink" title="删除浏览器所有的cookies"></a>删除浏览器所有的cookies</h3><p>delete_all_cookies()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.delete_all_cookies()</span><br></pre></td></tr></table></figure>
<h3 id="删除指定的cookie"><a href="#删除指定的cookie" class="headerlink" title="删除指定的cookie"></a>删除指定的cookie</h3><p>delete_cookie(name)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.delete_cookie(<span class="string">"my_cookie_name"</span>)</span><br></pre></td></tr></table></figure>
<h3 id="关闭浏览器方法"><a href="#关闭浏览器方法" class="headerlink" title="关闭浏览器方法"></a>关闭浏览器方法</h3><p>close()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.close()</span><br></pre></td></tr></table></figure>
<h3 id="关闭浏览器并且退出驱动程序"><a href="#关闭浏览器并且退出驱动程序" class="headerlink" title="关闭浏览器并且退出驱动程序"></a>关闭浏览器并且退出驱动程序</h3><p>quit()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>
<h3 id="返回上一页"><a href="#返回上一页" class="headerlink" title="返回上一页"></a>返回上一页</h3><p>back()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.back()</span><br></pre></td></tr></table></figure>
<h3 id="设置等待超时的方法"><a href="#设置等待超时的方法" class="headerlink" title="设置等待超时的方法"></a>设置等待超时的方法</h3><p>implicitly_wait(wait_time)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.implicitly_wait(<span class="number">30</span>)</span><br></pre></td></tr></table></figure>
<h3 id="浏览器窗口最大化"><a href="#浏览器窗口最大化" class="headerlink" title="浏览器窗口最大化"></a>浏览器窗口最大化</h3><p>maximize_window()</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.maximize_window()</span><br></pre></td></tr></table></figure>
<h3 id="查看浏览器的名字"><a href="#查看浏览器的名字" class="headerlink" title="查看浏览器的名字"></a>查看浏览器的名字</h3><p>name</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drvier.name</span><br></pre></td></tr></table></figure>
<h2 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h2><h3 id="打开网址"><a href="#打开网址" class="headerlink" title="打开网址"></a>打开网址</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"></span><br><span class="line"><span class="comment"># chromeDriver = r".\chromedriver_win32\chromedriver.exe"</span></span><br><span class="line"><span class="comment"># browser = webdriver.Chrome(chromeDriver)</span></span><br><span class="line">browser = webdriver.Chrome()</span><br><span class="line">browser.get(<span class="string">'http://www.baidu.com/'</span>)</span><br></pre></td></tr></table></figure>
<p>如果程序执行错误，浏览器没有打开，那么应该是没有装 Chrome 浏览器或者 Chrome 驱动没有配置在环境变量里。下载驱动，然后将驱动文件路径配置在环境变量即可。</p>
<p><a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" target="_blank" rel="noopener">浏览器驱动下载</a></p>
<p>比如Mac OS，就把下载好的文件放在 /usr/bin 目录下就可以了。</p>
<h3 id="模拟提交"><a href="#模拟提交" class="headerlink" title="模拟提交"></a>模拟提交</h3><p>下面的代码实现了模拟提交提交搜索的功能，首先等页面加载完成，然后输入到搜索框文本，点击提交。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.keys <span class="keyword">import</span> Keys</span><br><span class="line"></span><br><span class="line">driver = webdriver.Firefox()</span><br><span class="line">driver.get(<span class="string">"http://www.python.org"</span>)</span><br><span class="line"><span class="keyword">assert</span> <span class="string">"Python"</span> <span class="keyword">in</span> driver.title</span><br><span class="line">elem = driver.find_element_by_name(<span class="string">"q"</span>)</span><br><span class="line">elem.clear()</span><br><span class="line">elem.send_keys(<span class="string">"pycon"</span>)</span><br><span class="line">elem.send_keys(Keys.RETURN)</span><br><span class="line">print(driver.page_source)</span><br><span class="line"><span class="keyword">assert</span> <span class="string">"No results found."</span> <span class="keyword">not</span> <span class="keyword">in</span> driver.page_source</span><br><span class="line">driver.close()</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cuiqingcai.com/2599.html" target="_blank" rel="noopener">Python爬虫利器五之Selenium的用法</a></p>
<p><a href="https://www.cnblogs.com/qiezizi/p/5788783.html" target="_blank" rel="noopener">selenium2中关于Python的常用函数</a></p>
<p><a href="https://selenium-python-zh.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Selenium-Python中文文档</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/12/Python/Python_webbrowser/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/12/Python/Python_webbrowser/" class="post-title-link" itemprop="url">python之webbrowser</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-12 10:48:11" itemprop="dateCreated datePublished" datetime="2018-03-12T10:48:11+08:00">2018-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-10-12 17:13:24" itemprop="dateModified" datetime="2018-10-12T17:13:24+08:00">2018-10-12</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="python之webbrowser打开浏览器"><a href="#python之webbrowser打开浏览器" class="headerlink" title="python之webbrowser打开浏览器"></a>python之webbrowser打开浏览器</h1><h2 id="使用默认浏览器显示网址"><a href="#使用默认浏览器显示网址" class="headerlink" title="使用默认浏览器显示网址"></a>使用默认浏览器显示网址</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.open(url, new=<span class="number">0</span>, autoraise=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="在默认浏览器的新窗口中打开URL"><a href="#在默认浏览器的新窗口中打开URL" class="headerlink" title="在默认浏览器的新窗口中打开URL"></a>在默认浏览器的新窗口中打开URL</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.open_new(url)</span><br></pre></td></tr></table></figure>
<h2 id="在默认浏览器的新页面（“标签”）中打开网址"><a href="#在默认浏览器的新页面（“标签”）中打开网址" class="headerlink" title="在默认浏览器的新页面（“标签”）中打开网址"></a>在默认浏览器的新页面（“标签”）中打开网址</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.open_new_tab(url)</span><br></pre></td></tr></table></figure>
<h2 id="指定浏览器打开"><a href="#指定浏览器打开" class="headerlink" title="指定浏览器打开"></a>指定浏览器打开</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.get(<span class="string">'chrome'</span>)</span><br></pre></td></tr></table></figure>
<p>如果name为空，则返回适用于调用者环境的默认浏览器的constructor</p>
<h2 id="注册浏览器类型名称。"><a href="#注册浏览器类型名称。" class="headerlink" title="注册浏览器类型名称。"></a>注册浏览器类型名称。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">webbrowser.register(name, constructor[, instance])</span><br></pre></td></tr></table></figure>
<p>注册浏览器类型后，get()函数可以返回该浏览器类型的constructor。如果未提供instance，或者为None，在需要时将在不使用参数的情况下调用constructor来创建实例。如果提供了实例，则永远不会调用constructor，并且可能为None</p>
<h2 id="实例代码"><a href="#实例代码" class="headerlink" title="实例代码"></a>实例代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> webbrowser</span><br><span class="line"></span><br><span class="line">chromePath = <span class="string">r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe"</span></span><br><span class="line">webbrowser.register(<span class="string">'chrome1'</span>, <span class="keyword">None</span>, webbrowser.BackgroundBrowser(chromePath))</span><br><span class="line"></span><br><span class="line">browser = webbrowser.get(<span class="string">'chrome1'</span>)</span><br><span class="line">browser.open(<span class="string">'https://heroinlin.github.io/'</span>)</span><br><span class="line">browser.open_new(<span class="string">'https://heroinlin.github.io/'</span>)</span><br><span class="line">browser.open_new_tab(<span class="string">'https://heroinlin.github.io/'</span>)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/12/Pytorch/pytorch_Tensor/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/12/Pytorch/pytorch_Tensor/" class="post-title-link" itemprop="url">Tensor的数学运算</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-12 10:48:11" itemprop="dateCreated datePublished" datetime="2018-03-12T10:48:11+08:00">2018-03-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-08-06 09:25:00" itemprop="dateModified" datetime="2018-08-06T09:25:00+08:00">2018-08-06</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Pytorch/" itemprop="url" rel="index"><span itemprop="name">Pytorch</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>#<strong>Tensor的数学运算</strong></p>
<p>总结的方法包括：</p>
<p>Tensor求和以及按索引求和：<strong>torch.sum()         torch.Tensor.indexadd()</strong></p>
<p>Tensor元素乘积<strong>：torch.prod(input)</strong></p>
<p>对Tensor求均值、方差、极值：</p>
<p><strong>torch.mean()      torch.var()</strong></p>
<p><strong>torch.max()         torch.min()</strong></p>
<p>最后还有在NLP领域经常用到的：</p>
<p>求Tensor的平方根倒数、线性插值、双曲正切</p>
<p><strong>torch.rsqrt(input)     torch.lerp(star,end,weight)</strong></p>
<p><strong>torch.tanh(input, out=None)</strong></p>
<h2 id="元素求和"><a href="#元素求和" class="headerlink" title="元素求和"></a><strong>元素求和</strong></h2><p><strong>torch.sum(input)</strong> → float</p>
<p>返回输入向量input中所有元素的和。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_sum</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.sum(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.6491 -0.1617  0.7009<br>[torch.FloatTensor of size 1x3]</p>
<p>1.1884211152791977</p>
</blockquote>
<p><strong>torch.sum(input, dim, keepdim=False, out=None)</strong> → Tensor</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的和。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_sum_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.sum(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.8452 -0.2816  0.2672<br> 1.0685  2.4003 -0.6541<br>-0.1700 -0.4373 -0.2217<br>-1.2500  1.1798 -0.6842<br>[torch.FloatTensor of size 4x3]</p>
<p>-1.1968  2.8613 -1.2928<br>[torch.FloatTensor of size 1x3]</p>
</blockquote>
<h2 id="元素乘积"><a href="#元素乘积" class="headerlink" title="元素乘积"></a><strong>元素乘积</strong></h2><p><strong>torch.prod(input)</strong> → float</p>
<p>返回输入张量input所有元素的乘积。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_prod</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.prod(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.3751  0.3082 -0.7879<br>[torch.FloatTensor of size 1x3]</p>
<p>0.09109678113290456</p>
</blockquote>
<p><strong>torch.prod(input, dim, keepdim=False, out=None)</strong> → Tensor</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的乘积。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_prod_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.prod(a, <span class="number">1</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.2707  0.4322<br>-2.0925  0.3860<br>-1.1050  1.2551<br>-0.9644 -0.8771<br>[torch.FloatTensor of size 4x2]</p>
<p> 0.1170<br>-0.8077<br>-1.3869<br> 0.8459<br>[torch.FloatTensor of size 4x1]</p>
</blockquote>
<h2 id="按索引求和"><a href="#按索引求和" class="headerlink" title="按索引求和"></a><strong>按索引求和</strong></h2><p><strong>torch.Tensor.indexadd(dim, index, tensor)</strong> → Tensor</p>
<p>按索引参数index中所确定的顺序，将参数张量tensor中的元素与执行本方法的张量的元素逐个相加。参数tensor的尺寸必须严格地与执行方法的张量匹配，否则会发生错误。</p>
<p><strong>参数：</strong></p>
<ul>
<li>dim (int) - 索引index所指向的维度</li>
<li>index (LongTensor) - 包含索引数的张量</li>
<li>tensor (Tensor) - 含有相加元素的张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_index_add</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     x = torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">&gt;     print(x)</span><br><span class="line">&gt;     t = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">&gt;     index = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line">&gt;     x.index_add_(<span class="number">0</span>, index, t)</span><br><span class="line">&gt;     print(x)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>Output:</p>
<p>1  1  1<br> 1  1  1<br> 1  1  1<br>[torch.FloatTensor of size 3x3]</p>
<p>  2   3   4<br>  8   9  10<br>  5   6   7<br>[torch.FloatTensor of size 3x3]</p>
</blockquote>
<h2 id="平均数"><a href="#平均数" class="headerlink" title="平均数"></a><strong>平均数</strong></h2><p><strong>torch.mean(input)</strong></p>
<p>返回输入张量input中每个元素的平均值。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) – 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_mean</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.mean(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.9257 -0.1373  1.5762<br>[torch.FloatTensor of size 1x3]</p>
<p>0.788198247551918</p>
</blockquote>
<p><strong>torch.mean(input, dim, keepdim=False, out=None)</strong></p>
<p>返回新的张量，其中包含输入张量input指定维度dim中每行的平均值。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
<li>dim (int) - 指定进行均值计算的维度</li>
<li>keepdim (bool, optional) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (Tensor) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_mean_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.mean(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.3839 -1.2157  0.0210  1.1199  0.0319<br>-1.3452 -1.0125 -1.2500  1.0597 -0.9030<br> 0.1642  0.9110  0.8520  0.0481 -0.5234<br>[torch.FloatTensor of size 3x5]</p>
<p>-0.2657 -0.4391 -0.1257  0.7425 -0.4648<br>[torch.FloatTensor of size 1x5]</p>
</blockquote>
<h2 id="方差"><a href="#方差" class="headerlink" title="方差"></a><strong>方差</strong></h2><p><strong>torch.var(input, unbiased=True)</strong> → float</p>
<p>返回输入向量input中所有元素的方差。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
<li>unbiased (bool) - 是否使用基于修正贝塞尔函数的无偏估计</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_var</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.var(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.7808  0.1883  0.7654<br>[torch.FloatTensor of size 1x3]</p>
<p>0.6105006681458913</p>
</blockquote>
<p><strong>torch.var(input, dim, keepdim=False, unbiased=True, out=None)</strong> → Tensor</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的方差。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>unbiased (bool) - 是否使用基于修正贝塞尔函数的无偏估计</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_var_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.var(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.2789 -1.6603  1.1928  0.9614  1.2953<br> 0.9082  0.4015  1.1001 -0.9432  0.9254<br>-1.0593 -0.2636 -0.9274  0.0006  1.3933<br>[torch.FloatTensor of size 3x5]</p>
<p> 0.9815  1.1074  1.4358  0.9069  0.0609<br>[torch.FloatTensor of size 1x5]</p>
</blockquote>
<h2 id="最大值"><a href="#最大值" class="headerlink" title="最大值"></a><strong>最大值</strong></h2><p><strong>torch.max(input)</strong> → float</p>
<p>返回输入张量所有元素的最大值。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_max</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.max(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.8944  0.8395 -0.8867<br>[torch.FloatTensor of size 1x3]</p>
<p>0.8944145441055298</p>
</blockquote>
<p><strong>torch.max(input, dim, keepdim=False, out=None)</strong> -&gt; (Tensor, LongTensor)</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的最大值，同时返回每个最大值的位置索引。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (tuple,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_max_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.max(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.5306 -0.8915  2.7820  0.1723 -0.5061<br> 2.0535  0.2018 -3.1085 -0.7618  0.5924<br> 0.9906 -0.3212  0.1849  1.7002  0.8102<br>[torch.FloatTensor of size 3x5]</p>
<p>(<br> 2.0535  0.2018  2.7820  1.7002  0.8102<br>[torch.FloatTensor of size 1x5]<br>,<br> 1  1  0  2  2<br>[torch.LongTensor of size 1x5]<br>)</p>
</blockquote>
<p><strong>torch.max(input, other, out=None)</strong> → Tensor</p>
<p>逐个元素比较张量input与张量other，将比较出的最大值保存到输出张量中。<br>两个张量尺寸不需要完全相同，但需要支持自动扩展法则。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>other (Tensor) - 另一个输入的Tensor</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_max_2tensor</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">4</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.randn(<span class="number">1</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;     print(torch.max(a, b))</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>0.5369<br>-1.5926<br>-1.3574<br>-1.6009<br>[torch.FloatTensor of size 4]</p>
<p>-0.1394<br>[torch.FloatTensor of size 1]</p>
<p> 0.5369<br>-0.1394<br>-0.1394<br>-0.1394<br>[torch.FloatTensor of size 4]</p>
</blockquote>
<h2 id="最小值"><a href="#最小值" class="headerlink" title="最小值"></a><strong>最小值</strong></h2><p><strong>torch.min(input)</strong> → float</p>
<p>返回输入张量所有元素的最小值。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_min</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.min(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.8926  0.2079 -0.6790<br>[torch.FloatTensor of size 1x3]</p>
<p>-0.8925955295562744</p>
</blockquote>
<p><strong>torch.min(input, dim, keepdim=False, out=None)</strong> -&gt; (Tensor, LongTensor)</p>
<p>返回新的张量，其中包括输入张量input中指定维度dim中每行的最小值，同时返回每个最小值的位置索引。</p>
<p>若keepdim值为True，则在输出张量中，除了被操作的dim维度值降为1，其它维度与输入张量input相同。否则，dim维度相当于被执行torch.squeeze()维度压缩操作，导致此维度消失，最终输出张量会比输入张量少一个维度。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>dim (int) - 指定维度</li>
<li>keepdim (bool) - 输出张量是否保持与输入张量有相同数量的维度</li>
<li>out (tuple,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_min_2d</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.min(a, <span class="number">0</span>, <span class="keyword">True</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.8179  1.1834 -0.2989  0.6051 -0.1072<br>-1.1543  0.0666 -0.7919  0.2359  1.1995<br>-0.8094  0.5873  0.5116 -0.6181  0.9788<br>[torch.FloatTensor of size 3x5]</p>
<p>(<br>-1.1543  0.0666 -0.7919 -0.6181 -0.1072<br>[torch.FloatTensor of size 1x5]<br>,<br> 1  1  1  2  0<br>[torch.LongTensor of size 1x5]<br>)</p>
</blockquote>
<p><strong>torch.min(input, other, out=None)</strong> → Tensor</p>
<p>逐个元素比较张量input与张量other，将比较出的最小值保存到输出张量中。<br>两个张量尺寸不需要完全相同，但需要支持自动扩展法则。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入Tensor</li>
<li>other (Tensor) - 另一个输入的Tensor</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_min_2tensor</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.randn(<span class="number">1</span>)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;     print(torch.min(a, b))</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.3494  0.2155 -0.0723  0.8322<br>[torch.FloatTensor of size 1x4]</p>
<p> 0.2635<br>[torch.FloatTensor of size 1]</p>
<p> 0.2635  0.2155 -0.0723  0.2635<br>[torch.FloatTensor of size 1x4]</p>
</blockquote>
<h2 id="平方根倒数"><a href="#平方根倒数" class="headerlink" title="平方根倒数"></a><strong>平方根倒数</strong></h2><p><strong>torch.rsqrt(input)</strong> → Tensor</p>
<p>返回新的张量，其中包含input张量每个元素平方根的倒数。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_rsqrt</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     b = torch.rsqrt(a)</span><br><span class="line">&gt;     print(b)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 0.1615  0.3116 -0.3093 -1.5020<br>[torch.FloatTensor of size 1x4]</p>
<p> 2.4884  1.7915     nan     nan<br>[torch.FloatTensor of size 1x4]</p>
</blockquote>
<h2 id="线性插值"><a href="#线性插值" class="headerlink" title="线性插值"></a><strong>线性插值</strong></h2><p><strong>torch.lerp(start,end,weight)</strong> → Tensor</p>
<p>基于weight对输入的两个张量start与end逐个元素计算线性插值，结果返回至输出张量。</p>
<p>返回结果是： $outs_i=start_i+weight*(end_i-start_i)$</p>
<p><strong>参数：</strong></p>
<ul>
<li>start (Tensor) – 起始点张量</li>
<li>end (Tensor) – 终止点张量</li>
<li>weight (float) – 插入公式的 weight</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_lerp</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     start = torch.arange(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(start)</span><br><span class="line">&gt;     end = torch.Tensor(<span class="number">4</span>).fill_(<span class="number">10</span>)</span><br><span class="line">&gt;     print(end)</span><br><span class="line">&gt;     outs = torch.lerp(start, end, <span class="number">0.4</span>)</span><br><span class="line">&gt;     print(outs)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p> 1<br> 2<br> 3<br> 4<br>[torch.FloatTensor of size 4]</p>
<p> 10<br> 10<br> 10<br> 10<br>[torch.FloatTensor of size 4]</p>
<p> 4.6000<br> 5.2000<br> 5.8000<br> 6.4000<br>[torch.FloatTensor of size 4]</p>
</blockquote>
<h2 id="双曲正切"><a href="#双曲正切" class="headerlink" title="双曲正切"></a><strong>双曲正切</strong></h2><p><strong>torch.tanh(input, out=None)</strong> → Tensor</p>
<p>返回新的张量，其中包括输入张量input中每个元素的双曲正切。</p>
<p><strong>参数：</strong></p>
<ul>
<li>input (Tensor) - 输入张量</li>
<li>out (Tensor,optional) - 结果张量</li>
</ul>
<p><strong>例子：</strong></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">def</span> <span class="title">tensor_tanh</span><span class="params">()</span>:</span></span><br><span class="line">&gt;     a = torch.randn(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">&gt;     print(a)</span><br><span class="line">&gt;     outs = torch.tanh(a)</span><br><span class="line">&gt;     print(outs)</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<blockquote>
<p>-0.4298  0.0992  0.1322  1.4975  0.8817<br>[torch.FloatTensor of size 1x5]</p>
<p>-0.4051  0.0989  0.1314  0.9047  0.7073<br>[torch.FloatTensor of size 1x5]</p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/03/07/C&C++/Debug_error_solution/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/03/07/C&C++/Debug_error_solution/" class="post-title-link" itemprop="url">C及C++编译时候出现的一些问题与解决方案</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-03-07 15:54:11" itemprop="dateCreated datePublished" datetime="2018-03-07T15:54:11+08:00">2018-03-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-12 10:54:18" itemprop="dateModified" datetime="2018-03-12T10:54:18+08:00">2018-03-12</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/C-C/" itemprop="url" rel="index"><span itemprop="name">C&C++</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="C及C-编译时候出现的一些问题与解决方案"><a href="#C及C-编译时候出现的一些问题与解决方案" class="headerlink" title="C及C++编译时候出现的一些问题与解决方案"></a>C及C++编译时候出现的一些问题与解决方案</h1><h2 id="字符串问题"><a href="#字符串问题" class="headerlink" title="字符串问题"></a>字符串问题</h2><ul>
<li><p>[no matching function for call to ‘std::basic_ofstream &gt;::basic_ofstream(std::string&amp;)’]<br>编译时候报错为no matching function for call to std::basic_ofstream&lt;char, std::char_traits<char> &gt;::basic_ofstream(std::string&amp;)</char></p>
<p>原因是C++的string类与C的字符串存在不同，一些函数无法将string类作为参数使用。</p>
<blockquote>
<p><a href="http://en.cppreference.com/w/cpp/io/basic_ofstream" target="_blank" rel="noopener"><code>std::ofstream</code></a> can only be constructed with a <code>std::string</code> if you have C++11 or higher. Typically that is done with <code>-std=c++11</code> (gcc, clang). If you do not have access to c++11 then you can use the <code>c_str()</code> function of <code>std::string</code> to pass a <code>const char *</code> to the <code>ofstream</code> constructor.</p>
</blockquote>
<p>解决方案：转换为C的字符串</p>
<p>1.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> filename[<span class="number">10</span>];  </span><br><span class="line"><span class="built_in">strcpy</span>(filename, <span class="string">"1.txt"</span>);  </span><br><span class="line">ifstream fin;  </span><br><span class="line">fin.open(filename);</span><br></pre></td></tr></table></figure>
<p>2.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="built_in">string</span> asegurado;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Nombre a agregar: "</span>;</span><br><span class="line">    <span class="built_in">cin</span> &gt;&gt; asegurado;</span><br><span class="line"></span><br><span class="line">    <span class="function">ofstream <span class="title">entrada</span><span class="params">(asegurado,<span class="string">""</span>)</span></span>;<span class="comment">/*编译报错*/</span></span><br><span class="line">    <span class="comment">//ofstream entrada(asegurado); // C++11 or higher</span></span><br><span class="line">	<span class="comment">//ofstream entrada(asegurado.c_str());  // C++03 or below</span></span><br><span class="line">    <span class="keyword">if</span> (entrada.fail())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"El archivo no se creo correctamente"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Optimizer/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Optimizer/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：最优化笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-26 08:54:02" itemprop="dateModified" datetime="2018-03-26T08:54:02+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：最优化笔记"><a href="#CS231n课程笔记翻译：最优化笔记" class="headerlink" title="CS231n课程笔记翻译：最优化笔记"></a>CS231n课程笔记翻译：最优化笔记</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li><p>简介</p>
</li>
<li><p>损失函数可视化</p>
</li>
<li><p>最优化</p>
<ul>
<li>策略#1：随机搜索</li>
</ul>
</li>
</ul>
<ul>
<li>策略#2：随机局部搜索</li>
<li>策略#3：跟随梯度 </li>
</ul>
<ul>
<li><p>梯度计算</p>
<ul>
<li>使用有限差值进行数值计算</li>
</ul>
</li>
</ul>
<ul>
<li>微分计算梯度</li>
</ul>
<ul>
<li><p>梯度下降</p>
</li>
<li><p>小结</p>
</li>
</ul>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在上一节中，我们介绍了图像分类任务中的两个关键部分：</p>
<ol>
<li>基于参数的<strong>评分函数。</strong>该函数将原始图像像素映射为分类评分值（例如：一个线性函数）。</li>
<li><strong>损失函数</strong>。该函数能够根据分类评分和训练集图像数据实际分类的一致性，衡量某个具体参数集的质量好坏。损失函数有多种版本和不同的实现方式（例如：Softmax或SVM）。</li>
</ol>
<p>上节中，线性函数的形式是$f(x_i, W)=Wx_i$，而SVM实现的公式是：</p>
<center>$$L=\displaystyle\frac{1}{N}\sum_i\sum_{j\not= y_i}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+1)]+\alpha R(W)$$</center>

<p>对于图像数据$x_i$，如果基于参数集$W$做出的分类预测与真实情况比较一致，那么计算出来的损失值$L$)就很低。现在介绍第三个，也是最后一个关键部分：<strong>最优化Optimization</strong>。最优化是寻找能使得损失函数值最小化的参数$W$的过程。</p>
<p><strong>铺垫</strong>：一旦理解了这三个部分是如何相互运作的，我们将会回到第一个部分（基于参数的函数映射），然后将其拓展为一个远比线性函数复杂的函数：首先是神经网络，然后是卷积神经网络。而损失函数和最优化过程这两个部分将会保持相对稳定。</p>
<h2 id="损失函数可视化"><a href="#损失函数可视化" class="headerlink" title="损失函数可视化"></a>损失函数可视化</h2><p>本课中讨论的损失函数一般都是定义在高维度的空间中（比如，在CIFAR-10中一个线性分类器的权重矩阵大小是[10x3073]，就有30730个参数），这样要将其可视化就很困难。然而办法还是有的，在1个维度或者2个维度的方向上对高维空间进行切片，就能得到一些直观感受。例如，随机生成一个权重矩阵$W$，该矩阵就与高维空间中的一个点对应。然后沿着某个维度方向前进的同时记录损失函数值的变化。换句话说，就是生成一个随机的方向$W_1$并且沿着此方向计算损失值，计算方法是根据不同的$a$值来计算$L(W+aW_1)$。这个过程将生成一个图表，其x轴是$a$值，y轴是损失函数值。同样的方法还可以用在两个维度上，通过改变$a,b$来计算损失值$L(W+aW_1+bW_2)$，从而给出二维的图像。在图像中，$a,b$可以分别用x和y轴表示，而损失函数的值可以用颜色变化表示：</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_0.jpg?raw=true" width="350"></center>

<p>一个无正则化的多类SVM的损失函数的图示。左边和中间只有一个样本数据，右边是CIFAR-10中的100个数据。<strong>左</strong>：$a$值变化在某个维度方向上对应的的损失值变化。<strong>中和右</strong>：两个维度方向上的损失值切片图，蓝色部分是低损失值区域，红色部分是高损失值区域。注意损失函数的分段线性结构。多个样本的损失值是总体的平均值，所以右边的碗状结构是很多的分段线性结构的平均（比如中间这个就是其中之一）。</p>
<p>—————————————————————————————————————————</p>
<p>我们可以通过数学公式来解释损失函数的分段线性结构。对于一个单独的数据，有损失函数的计算公式如下：</p>
<center>$$Li=\sum_{j\not=y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+1)]$$</center>

<p>通过公式可见，每个样本的数据损失值是以$W$为参数的线性函数的总和（零阈值来源于$max(0,-)$函数）。$W$的每一行（即$w_j$），有时候它前面是一个正号（比如当它对应错误分类的时候），有时候它前面是一个负号（比如当它是是正确分类的时候）。为进一步阐明，假设有一个简单的数据集，其中包含有3个只有1个维度的点，数据集数据点有3个类别。那么完整的无正则化SVM的损失值计算如下：</p>
<center>$$L_0=max(0,w^T_1x_0-w^T_0x_0+1)+max(0,w^T_2x_0-w^T_0x_0+1)$$</center><br><center>$$L_1=max(0,w^T_0x_1-w^T_1x_1+1)+max(0,w^T_2x_1-w^T_1x_1+1)$$</center><br><center>$$L_2=max(0,w^T_0x_2-w^T_2x_2+1)+max(0,w^T_1x_2-w^T_2x_2+1)$$</center><br><center>$$L=(L_0+L_1+L_2)/3$$</center>

<p>因为这些例子都是一维的，所以数据$x_i$和权重$w_j$都是数字。观察$w_0$，可以看到上面的式子中一些项是$w_0$的线性函数，且每一项都会与0比较，取两者的最大值。可作图如下：——————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_1.png?raw=true" width="350"></center>

<p>从一个维度方向上对数据损失值的展示。x轴方向就是一个权重，y轴就是损失值。数据损失是多个部分组合而成。其中每个部分要么是某个权重的独立部分，要么是该权重的线性函数与0阈值的比较。完整的SVM数据损失就是这个形状的30730维版本。</p>
<p>——————————————————————————————————————</p>
<p>需要多说一句的是，你可能根据SVM的损失函数的碗状外观猜出它是一个<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Convex_function" target="_blank" rel="noopener"><strong>凸函数</strong></a>。关于如何高效地最小化凸函数的论文有很多，你也可以学习斯坦福大学关于（<a href="http://link.zhihu.com/?target=http%3A//stanford.edu/%7Eboyd/cvxbook/" target="_blank" rel="noopener"><strong>凸函数最优化</strong></a>）的课程。但是一旦我们将<img src="http://www.zhihu.com/equation?tex=f" alt="f">函数扩展到神经网络，目标函数就就不再是凸函数了，图像也不会像上面那样是个碗状，而是凹凸不平的复杂地形形状。</p>
<p><em>不可导的损失函数。</em>作为一个技术笔记，你要注意到：由于max操作，损失函数中存在一些<em>不可导点（kinks），</em>这些点使得损失函数不可微，因为在这些不可导点，梯度是没有定义的。但是<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Subderivative" target="_blank" rel="noopener"><strong>次梯度（subgradient）</strong></a>依然存在且常常被使用。在本课中，我们将交换使用<em>次梯度</em>和<em>梯度</em>两个术语。</p>
<h2 id="最优化-Optimization"><a href="#最优化-Optimization" class="headerlink" title="最优化 Optimization"></a>最优化 Optimization</h2><p>重申一下：损失函数可以量化某个具体权重集<strong>W</strong>的质量。而最优化的目标就是找到能够最小化损失函数值的<strong>W</strong> 。我们现在就朝着这个目标前进，实现一个能够最优化损失函数的方法。对于有一些经验的同学，这节课看起来有点奇怪，因为使用的例子（SVM 损失函数）是一个凸函数问题。但是要记得，最终的目标是不仅仅对凸函数做最优化，而是能够最优化一个神经网络，而对于神经网络是不能简单的使用凸函数的最优化技巧的。</p>
<h3 id="策略-1：一个差劲的初始方案：随机搜索"><a href="#策略-1：一个差劲的初始方案：随机搜索" class="headerlink" title="策略#1：一个差劲的初始方案：随机搜索"></a><strong>策略#1：一个差劲的初始方案：随机搜索</strong></h3><p>既然确认参数集<strong>W</strong>的好坏蛮简单的，那第一个想到的（差劲）方法，就是可以随机尝试很多不同的权重，然后看其中哪个最好。过程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设X_train的每一列都是一个数据样本（比如3073 x 50000）</span></span><br><span class="line"><span class="comment"># 假设Y_train是数据样本的类别标签（比如一个长50000的一维数组）</span></span><br><span class="line"><span class="comment"># 假设函数L对损失函数进行评价</span></span><br><span class="line"></span><br><span class="line">bestloss = float(<span class="string">"inf"</span>) <span class="comment"># Python assigns the highest possible float value</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.0001</span> <span class="comment"># generate random parameters</span></span><br><span class="line">  loss = L(X_train, Y_train, W) <span class="comment"># get the loss over the entire training set</span></span><br><span class="line">  <span class="keyword">if</span> loss &lt; bestloss: <span class="comment"># keep track of the best solution</span></span><br><span class="line">    bestloss = loss</span><br><span class="line">    bestW = W</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'in attempt %d the loss was %f, best %f'</span> % (num, loss, bestloss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># in attempt 0 the loss was 9.401632, best 9.401632</span></span><br><span class="line"><span class="comment"># in attempt 1 the loss was 8.959668, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 2 the loss was 9.044034, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 3 the loss was 9.278948, best 8.959668</span></span><br><span class="line"><span class="comment"># in attempt 4 the loss was 8.857370, best 8.857370</span></span><br><span class="line"><span class="comment"># in attempt 5 the loss was 8.943151, best 8.857370</span></span><br><span class="line"><span class="comment"># in attempt 6 the loss was 8.605604, best 8.605604</span></span><br><span class="line"><span class="comment"># ... (trunctated: continues for 1000 lines)</span></span><br></pre></td></tr></table></figure>
<p>在上面的代码中，我们尝试了若干随机生成的权重矩阵<strong>W</strong>，其中某些的损失值较小，而另一些的损失值大些。我们可以把这次随机搜索中找到的最好的权重<strong>W</strong>取出，然后去跑测试集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设X_test尺寸是[3073 x 10000], Y_test尺寸是[10000 x 1]</span></span><br><span class="line">scores = Wbest.dot(Xte_cols) <span class="comment"># 10 x 10000, the class scores for all test examples</span></span><br><span class="line"><span class="comment"># 找到在每列中评分值最大的索引（即预测的分类）</span></span><br><span class="line">Yte_predict = np.argmax(scores, axis = <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 以及计算准确率</span></span><br><span class="line">np.mean(Yte_predict == Yte)</span><br><span class="line"><span class="comment"># 返回 0.1555</span></span><br></pre></td></tr></table></figure>
<p>验证集上表现最好的权重<strong>W</strong>跑测试集的准确率是<strong>15.5%，</strong>而完全随机猜的准确率是10%，如此看来，这个准确率对于这样一个不经过大脑的策略来说，还算不错嘛！</p>
<p><strong>核心思路：迭代优化</strong>。当然，我们肯定能做得更好些。核心思路是：虽然找到最优的权重<strong>W</strong>非常困难，甚至是不可能的（尤其当<strong>W</strong>中存的是整个神经网络的权重的时候），但如果问题转化为：对一个权重矩阵集<strong>W</strong>取优，使其损失值稍微减少。那么问题的难度就大大降低了。换句话说，我们的方法从一个随机的<strong>W</strong>开始，然后对其迭代取优，每次都让它的损失值变得更小一点。</p>
<blockquote>
<p>我们的策略是从随机权重开始，然后迭代取优，从而获得更低的损失值。</p>
</blockquote>
<p><strong>蒙眼徒步者的比喻</strong>：一个助于理解的比喻是把你自己想象成一个蒙着眼睛的徒步者，正走在山地地形上，目标是要慢慢走到山底。在CIFAR-10的例子中，这山是30730维的（因为<strong>W</strong>是3073x10）。我们在山上踩的每一点都对应一个的损失值，该损失值可以看做该点的海拔高度。</p>
<h3 id="策略-2：随机本地搜索"><a href="#策略-2：随机本地搜索" class="headerlink" title="策略#2：随机本地搜索"></a><strong>策略#2：随机本地搜索</strong></h3><p>第一个策略可以看做是每走一步都尝试几个随机方向，如果某个方向是向山下的，就向该方向走一步。这次我们从一个随机$W$开始，然后生成一个随机的扰动$\delta W$ ，只有当$W+\delta W$的损失值变低，我们才会更新。这个过程的具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 生成随机初始W</span></span><br><span class="line">bestloss = float(<span class="string">"inf"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">1000</span>):</span><br><span class="line">  step_size = <span class="number">0.0001</span></span><br><span class="line">  Wtry = W + np.random.randn(<span class="number">10</span>, <span class="number">3073</span>) * step_size</span><br><span class="line">  loss = L(Xtr_cols, Ytr, Wtry)</span><br><span class="line">  <span class="keyword">if</span> loss &lt; bestloss:</span><br><span class="line">    W = Wtry</span><br><span class="line">    bestloss = loss</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'iter %d loss is %f'</span> % (i, bestloss)</span><br></pre></td></tr></table></figure>
<p>使用同样的数据（1000），这个方法可以得到<strong>21.4%</strong>的分类准确率。这个比策略一好，但是依然过于浪费计算资源。</p>
<h3 id="策略-3：跟随梯度"><a href="#策略-3：跟随梯度" class="headerlink" title="策略#3：跟随梯度"></a><strong>策略#3：跟随梯度</strong></h3><p>前两个策略中，我们是尝试在权重空间中找到一个方向，沿着该方向能降低损失函数的损失值。其实不需要随机寻找方向，因为可以直接计算出最好的方向，这就是从数学上计算出最陡峭的方向。这个方向就是损失函数的<strong>梯度（gradient）</strong>。在蒙眼徒步者的比喻中，这个方法就好比是感受我们脚下山体的倾斜程度，然后向着最陡峭的下降方向下山。</p>
<p>在一维函数中，斜率是函数在某一点的瞬时变化率。梯度是函数的斜率的一般化表达，它不是一个值，而是一个向量。在输入空间中，梯度是各个维度的斜率组成的向量（或者称为导数<strong>derivatives</strong>）。对一维函数的求导公式如下：</p>
<center>$$\displaystyle\frac{df(x)}{dx}=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$</center>

<p>当函数有多个参数的时候，我们称导数为偏导数。而梯度就是在每个维度上偏导数所形成的向量。</p>
<h2 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h2><p>计算梯度有两种方法：一个是缓慢的近似方法（<strong>数值梯度法</strong>），但实现相对简单。另一个方法（<strong>分析梯度法</strong>）计算迅速，结果精确，但是实现时容易出错，且需要使用微分。现在对两种方法进行介绍：</p>
<h3 id="利用有限差值计算梯度"><a href="#利用有限差值计算梯度" class="headerlink" title="利用有限差值计算梯度"></a><strong>利用有限差值计算梯度</strong></h3><p>上节中的公式已经给出数值计算梯度的方法。下面代码是一个输入为函数<strong>f</strong>和向量<strong>x，</strong>计算<strong>f</strong>的梯度的通用函数，它返回函数<strong>f</strong>在点<strong>x处</strong>的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_numerical_gradient</span><span class="params">(f, x)</span>:</span></span><br><span class="line">  <span class="string">"""  </span></span><br><span class="line"><span class="string">  一个f在x处的数值梯度法的简单实现</span></span><br><span class="line"><span class="string">  - f是只有一个参数的函数</span></span><br><span class="line"><span class="string">  - x是计算梯度的点</span></span><br><span class="line"><span class="string">  """</span> </span><br><span class="line"></span><br><span class="line">  fx = f(x) <span class="comment"># 在原点计算函数值</span></span><br><span class="line">  grad = np.zeros(x.shape)</span><br><span class="line">  h = <span class="number">0.00001</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 对x中所有的索引进行迭代</span></span><br><span class="line">  it = np.nditer(x, flags=[<span class="string">'multi_index'</span>], op_flags=[<span class="string">'readwrite'</span>])</span><br><span class="line">  <span class="keyword">while</span> <span class="keyword">not</span> it.finished:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算x+h处的函数值</span></span><br><span class="line">    ix = it.multi_index</span><br><span class="line">    old_value = x[ix]</span><br><span class="line">    x[ix] = old_value + h <span class="comment"># 增加h</span></span><br><span class="line">    fxh = f(x) <span class="comment"># 计算f(x + h)</span></span><br><span class="line">    x[ix] = old_value <span class="comment"># 存到前一个值中 (非常重要)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算偏导数</span></span><br><span class="line">    grad[ix] = (fxh - fx) / h <span class="comment"># 坡度</span></span><br><span class="line">    it.iternext() <span class="comment"># 到下个维度</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<p>根据上面的梯度公式，代码对所有维度进行迭代，在每个维度上产生一个很小的变化h，通过观察函数值变化，计算函数在该维度上的偏导数。最后，所有的梯度存储在变量<strong>grad</strong>中。</p>
<p><strong>实践考量</strong>：注意在数学公式中，<strong>h</strong>的取值是趋近于0的，然而在实际中，用一个很小的数值（比如例子中的1e-5）就足够了。在不产生数值计算出错的理想前提下，你会使用尽可能小的h。还有，实际中用<strong>中心差值公式（centered difference formula)</strong> $[f(x+h)-f(x-h)]/2h$ 效果较好。细节可查看<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Numerical_differentiation" target="_blank" rel="noopener"><strong>wiki</strong></a>。</p>
<p>可以使用上面这个公式来计算任意函数在任意点上的梯度。下面计算权重空间中的某些随机点上，CIFAR-10损失函数的梯度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 要使用上面的代码我们需要一个只有一个参数的函数</span></span><br><span class="line"><span class="comment"># (在这里参数就是权重)所以也包含了X_train和Y_train</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CIFAR10_loss_fun</span><span class="params">(W)</span>:</span></span><br><span class="line">  <span class="keyword">return</span> L(X_train, Y_train, W)</span><br><span class="line"></span><br><span class="line">W = np.random.rand(<span class="number">10</span>, <span class="number">3073</span>) * <span class="number">0.001</span> <span class="comment"># 随机权重向量</span></span><br><span class="line">df = eval_numerical_gradient(CIFAR10_loss_fun, W) <span class="comment"># 得到梯度</span></span><br></pre></td></tr></table></figure>
<p>梯度告诉我们损失函数在每个维度上的斜率，以此来进行更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">loss_original = CIFAR10_loss_fun(W) <span class="comment"># 初始损失值</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'original loss: %f'</span> % (loss_original, )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看不同步长的效果</span></span><br><span class="line"><span class="keyword">for</span> step_size_log <span class="keyword">in</span> [<span class="number">-10</span>, <span class="number">-9</span>, <span class="number">-8</span>, <span class="number">-7</span>, <span class="number">-6</span>, <span class="number">-5</span>,<span class="number">-4</span>,<span class="number">-3</span>,<span class="number">-2</span>,<span class="number">-1</span>]:</span><br><span class="line">  step_size = <span class="number">10</span> ** step_size_log</span><br><span class="line">  W_new = W - step_size * df <span class="comment"># 权重空间中的新位置</span></span><br><span class="line">  loss_new = CIFAR10_loss_fun(W_new)</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'for step size %f new loss: %f'</span> % (step_size, loss_new)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># original loss: 2.200718</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-10 new loss: 2.200652</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-09 new loss: 2.200057</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-08 new loss: 2.194116</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-07 new loss: 2.135493</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-06 new loss: 1.647802</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-05 new loss: 2.844355</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-04 new loss: 25.558142</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-03 new loss: 254.086573</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-02 new loss: 2539.370888</span></span><br><span class="line"><span class="comment"># for step size 1.000000e-01 new loss: 25392.214036</span></span><br></pre></td></tr></table></figure>
<p><strong>在梯度负方向上更新</strong>：在上面的代码中，为了计算<strong>W_new</strong>，要注意我们是向着梯度<strong>df</strong>的负方向去更新，这是因为我们希望损失函数值是降低而不是升高。</p>
<p><strong>步长的影响</strong>：梯度指明了函数在哪个方向是变化率最大的，但是没有指明在这个方向上应该走多远。在后续的课程中可以看到，选择步长（也叫作<em>学习率</em>）将会是神经网络训练中最重要（也是最头痛）的超参数设定之一。还是用蒙眼徒步者下山的比喻，这就好比我们可以感觉到脚朝向的不同方向上，地形的倾斜程度不同。但是该跨出多长的步长呢？不确定。如果谨慎地小步走，情况可能比较稳定但是进展较慢（这就是步长较小的情况）。相反，如果想尽快下山，那就大步走吧，但结果也不一定尽如人意。在上面的代码中就能看见反例，在某些点如果步长过大，反而可能越过最低点导致更高的损失值。</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_2.jpg?raw=true" width="250"></center>

<p>将步长效果视觉化的图例。从某个具体的点W开始计算梯度（白箭头方向是负梯度方向），梯度告诉了我们损失函数下降最陡峭的方向。小步长下降稳定但进度慢，大步长进展快但是风险更大。采取大步长可能导致错过最优点，让损失值上升。步长（后面会称其为<strong>学习率</strong>）将会是我们在调参中最重要的超参数之一。</p>
<p>————————————————————————————————————————</p>
<p><strong>效率问题</strong>：你可能已经注意到，计算数值梯度的复杂性和参数的量线性相关。在本例中有30730个参数，所以损失函数每走一步就需要计算30731次损失函数的梯度。现代神经网络很容易就有上千万的参数，因此这个问题只会越发严峻。显然这个策略不适合大规模数据，我们需要更好的策略。</p>
<h3 id="微分分析计算梯度"><a href="#微分分析计算梯度" class="headerlink" title="微分分析计算梯度"></a>微分分析计算梯度</h3><p>使用有限差值近似计算梯度比较简单，但缺点在于终究只是近似（因为我们对于<em>h</em>值是选取了一个很小的数值，但真正的梯度定义中<em>h</em>趋向0的极限），且耗费计算资源太多。第二个梯度计算方法是利用微分来分析，能得到计算梯度的公式（不是近似），用公式计算梯度速度很快，唯一不好的就是实现的时候容易出错。为了解决这个问题，在实际操作时常常将分析梯度法的结果和数值梯度法的结果作比较，以此来检查其实现的正确性，这个步骤叫做<strong>梯度检查</strong>。</p>
<p>用SVM的损失函数在某个数据点上的计算来举例：</p>
<p><cneter>$$L_i=\displaystyle\sum_{j\not =y_i}[max(0,w^T_jx_i-w^T_{y_i}x_i+\Delta)]$$</cneter></p>
<p>可以对函数进行微分。比如，对$w_{y_i}$进行微分得到：</p>
<center>$$ \nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i$$</center>

<p><strong>译者注：原公式中1为空心字体，尝试\mathbb{}等多种方法仍无法实现，请知友指点。</strong></p>
<p>其中$\mathbb{1}​$是一个示性函数，如果括号中的条件为真，那么函数值为1，如果为假，则函数值为0。虽然上述公式看起来复杂，但在代码实现的时候比较简单：只需要计算没有满足边界值的分类的数量（因此对损失函数产生了贡献），然后乘以$x_i​$就是梯度了。注意，这个梯度只是对应正确分类的W的行向量的梯度，那些$j\not =y_i​$行的梯度是：</p>
<center>$$\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i$$</center>

<p>一旦将梯度的公式微分出来，代码实现公式并用于梯度更新就比较顺畅了。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>现在可以计算损失函数的梯度了，程序重复地计算梯度然后对参数进行更新，这一过程称为<em>梯度下降</em>，他的<strong>普通</strong>版本是这样的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 进行梯度更新</span></span><br></pre></td></tr></table></figure>
<p>这个简单的循环在所有的神经网络核心库中都有。虽然也有其他实现最优化的方法（比如LBFGS），但是到目前为止，梯度下降是对神经网络的损失函数最优化中最常用的方法。课程中，我们会在它的循环细节增加一些新的东西（比如更新的具体公式），但是核心思想不变，那就是我们一直跟着梯度走，直到结果不再变化。</p>
<p><strong>小批量数据梯度下降（**</strong>Mini-batch gradient descent<strong>**）</strong>：在大规模的应用中（比如ILSVRC挑战赛），训练数据可以达到百万级量级。如果像这样计算整个训练集，来获得仅仅一个参数的更新就太浪费了。一个常用的方法是计算训练集中的<strong>小批量（batches）</strong>数据。例如，在目前最高水平的卷积神经网络中，一个典型的小批量包含256个例子，而整个训练集是多少呢？一百二十万个。这个小批量数据就用来实现一个参数更新：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 普通的小批量数据梯度下降</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">  data_batch = sample_training_data(data, <span class="number">256</span>) <span class="comment"># 256个数据</span></span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad <span class="comment"># 参数更新</span></span><br></pre></td></tr></table></figure>
<p>这个方法之所以效果不错，是因为训练集中的数据都是相关的。要理解这一点，可以想象一个极端情况：在ILSVRC中的120万个图像是1000张不同图片的复制（每个类别1张图片，每张图片有1200张复制）。那么显然计算这1200张复制图像的梯度就应该是一样的。对比120万张图片的数据损失的均值与只计算1000张的子集的数据损失均值时，结果应该是一样的。实际情况中，数据集肯定不会包含重复图像，那么小批量数据的梯度就是对整个数据集梯度的一个近似。因此，在实践中通过计算小批量数据的梯度可以实现更快速地收敛，并以此来进行更频繁的参数更新。</p>
<p>小批量数据策略有个极端情况，那就是每个批量中只有1个数据样本，这种策略被称为<strong>随机梯度下降（Stochastic Gradient Descent 简称SGD）</strong>，有时候也被称为在线梯度下降。这种策略在实际情况中相对少见，因为向量化操作的代码一次计算100个数据 比100次计算1个数据要高效很多。即使SGD在技术上是指每次使用1个数据来计算梯度，你还是会听到人们使用SGD来指代小批量数据梯度下降（或者用MGD来指代小批量数据梯度下降，而BGD来指代则相对少见）。小批量数据的大小是一个超参数，但是一般并不需要通过交叉验证来调参。它一般由存储器的限制来决定的，或者干脆设置为同样大小，比如32，64，128等。之所以使用2的指数，是因为在实际中许多向量化操作实现的时候，如果输入数据量是2的倍数，那么运算更快。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/optimization/optimization_3.jpg?raw=true" width="300"></center>

<p>信息流的总结图例。数据集中的(x,y)是给定的。权重从一个随机数字开始，且可以改变。在前向传播时，评分函数计算出类别的分类评分并存储在向量<strong>f</strong>中。损失函数包含两个部分：数据损失和正则化损失。其中，数据损失计算的是分类评分f和实际标签y之间的差异，正则化损失只是一个关于权重的函数。在梯度下降过程中，我们计算权重的梯度（如果愿意的话，也可以计算数据上的梯度），然后使用它们来实现参数的更新。</p>
<p>—————————————————————————————————————————</p>
<p>在本节课中：</p>
<ul>
<li>将损失函数比作了一个<strong>高维度的最优化地形</strong>，并尝试到达它的最底部。最优化的工作过程可以看做一个蒙着眼睛的徒步者希望摸索着走到山的底部。在例子中，可见SVM的损失函数是分段线性的，并且是碗状的。</li>
<li>提出了迭代优化的思想，从一个随机的权重开始，然后一步步地让损失值变小，直到最小。</li>
<li>函数的<strong>梯度</strong>给出了该函数最陡峭的上升方向。介绍了利用有限的差值来近似计算梯度的方法，该方法实现简单但是效率较低（有限差值就是<em>h</em>，用来计算数值梯度）。</li>
<li>参数更新需要有技巧地设置<strong>步长</strong>。也叫学习率。如果步长太小，进度稳定但是缓慢，如果步长太大，进度快但是可能有风险。</li>
<li>讨论权衡了数值梯度法和分析梯度法。数值梯度法计算简单，但结果只是近似且耗费计算资源。分析梯度法计算准确迅速但是实现容易出错，而且需要对梯度公式进行推导的数学基本功。因此，在实际中使用分析梯度法，然后使用<strong>梯度检查</strong>来检查其实现正确与否，其本质就是将分析梯度法的结果与数值梯度法的计算结果对比。</li>
<li>介绍了<strong>梯度下降</strong>算法，它在循环中迭代地计算梯度并更新参数。</li>
</ul>
<p><strong>预告</strong>：这节课的核心内容是：理解并能计算损失函数关于权重的梯度，是设计、训练和理解神经网络的核心能力。下节中，将介绍如何使用链式法则来高效地计算梯度，也就是通常所说的<strong>反向传播（backpropagation）机制</strong>。该机制能够对包含卷积神经网络在内的几乎所有类型的神经网络的损失函数进行高效的最优化。</p>
<p><strong>最优化笔记全文翻译完</strong>。</p>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/optimization-1/" target="_blank" rel="noopener"><strong>Optimization Note</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/li-yi-ying-73" target="_blank" rel="noopener">李艺颖</a>和<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改</p>
<p>知乎地址：（上，下）</p>
<p><a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/hello-world/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-02 16:54:52" itemprop="dateModified" datetime="2018-03-02T16:54:52+08:00">2018-03-02</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Image_Classify/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Image_Classify/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：图像分类笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-26 08:53:48" itemprop="dateModified" datetime="2018-03-26T08:53:48+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：图像分类笔记"><a href="#CS231n课程笔记翻译：图像分类笔记" class="headerlink" title="CS231n课程笔记翻译：图像分类笔记"></a>CS231n课程笔记翻译：图像分类笔记</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>这是一篇介绍性教程，面向非计算机视觉领域的同学。教程将向同学们介绍图像分类问题和数据驱动方法。下面是<strong>内容列表</strong>：</p>
<ul>
<li><p>图像分类、数据驱动方法和流程</p>
</li>
<li><p>Nearest Neighbor分类器</p>
<ul>
<li>k-Nearest Neighbor </li>
</ul>
</li>
<li><p>验证集、交叉验证集和超参数调参</p>
</li>
<li><p>Nearest Neighbor的优劣</p>
</li>
<li><p>小结</p>
</li>
<li><p>小结：应用kNN实践</p>
</li>
<li><p>拓展阅读</p>
</li>
</ul>
<h2 id="图像分类"><a href="#图像分类" class="headerlink" title="图像分类"></a>图像分类</h2><p><strong>目标</strong>：这一节我们将介绍图像分类问题。所谓图像分类问题，就是已有固定的分类标签集合，然后对于输入的图像，从分类标签集合中找出一个分类标签，最后把分类标签分配给该输入图像。虽然看起来挺简单的，但这可是计算机视觉领域的核心问题之一，并且有着各种各样的实际应用。在后面的课程中，我们可以看到计算机视觉领域中很多看似不同的问题（比如物体检测和分割），都可以被归结为图像分类问题。</p>
<p><strong>例子</strong>：以下图为例，图像分类模型读取该图片，并生成该图片属于集合 {cat, dog, hat, mug}中各个标签的概率。需要注意的是，对于计算机来说，图像是一个由数字组成的巨大的3维数组。在这个例子中，猫的图像大小是宽248像素，高400像素，有3个颜色通道，分别是红、绿和蓝（简称RGB）。如此，该图像就包含了248X400X3=297600个数字，每个数字都是在范围0-255之间的整型，其中0表示全黑，255表示全白。我们的任务就是把这些上百万的数字变成一个简单的标签，比如“猫”。</p>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_0.png?raw=true" width="300"></center>

<p>图像分类的任务，就是对于一个给定的图像，预测它属于的那个分类标签（或者给出属于一系列不同标签的可能性）。图像是3维数组，数组元素是取值范围从0到255的整数。数组的尺寸是宽度x高度x3，其中这个3代表的是红、绿和蓝3个颜色通道。</p>
<p>—————————————————————————————————————————</p>
<p><strong>困难和挑战</strong>：对于人来说，识别出一个像“猫”一样视觉概念是简单至极的，然而从计算机视觉算法的角度来看就值得深思了。我们在下面列举了计算机视觉算法在图像识别方面遇到的一些困难，要记住图像是以3维数组来表示的，数组中的元素是亮度值。</p>
<ul>
<li><strong>视角变化（Viewpoint variation）</strong>：同一个物体，摄像机可以从多个角度来展现。</li>
<li><strong>大小变化（Scale variation）</strong>：物体可视的大小通常是会变化的（不仅是在图片中，在真实世界中大小也是变化的）。</li>
<li><strong>形变（Deformation）</strong>：很多东西的形状并非一成不变，会有很大变化。</li>
<li><strong>遮挡（Occlusion）</strong>：目标物体可能被挡住。有时候只有物体的一小部分（可以小到几个像素）是可见的。</li>
<li><strong>光照条件（Illumination conditions）</strong>：在像素层面上，光照的影响非常大。</li>
<li><strong>背景干扰（Background clutter）</strong>：物体可能混入背景之中，使之难以被辨认。</li>
<li><strong>类内差异（Intra-class variation）</strong>：一类物体的个体之间的外形差异很大，比如椅子。这一类物体有许多不同的对象，每个都有自己的外形。</li>
</ul>
<p>面对以上所有变化及其组合，好的图像分类模型能够在维持分类结论稳定的同时，保持对类间差异足够敏感。</p>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_1.jpg?raw=true" width="400"></center>—————————————————————————————————————————<br><br><strong>数据驱动方法</strong>：如何写一个图像分类的算法呢？这和写个排序算法可是大不一样。怎么写一个从图像中认出猫的算法？搞不清楚。因此，与其在代码中直接写明各类物体到底看起来是什么样的，倒不如说我们采取的方法和教小孩儿看图识物类似：给计算机很多数据，然后实现学习算法，让计算机学习到每个类的外形。这种方法，就是<em>数据驱动方法</em>。既然该方法的第一步就是收集已经做好分类标注的图片来作为训练集，那么下面就看看数据库到底长什么样：<br><br>—————————————————————————————————————————<br><br><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_2.jpg?raw=true" width="400"></center>

<p>一个有4个视觉分类的训练集。在实际中，我们可能有上千的分类，每个分类都有成千上万的图像。</p>
<p>—————————————————————————————————————————</p>
<p><strong>图像分类流程</strong>。在课程视频中已经学习过，<strong>图像分类</strong>就是输入一个元素为像素值的数组，然后给它分配一个分类标签。完整流程如下：</p>
<ul>
<li><strong>输入</strong>：输入是包含N个图像的集合，每个图像的标签是K种分类标签中的一种。这个集合称为<em>训练集。</em></li>
<li><strong>学习</strong>：这一步的任务是使用训练集来学习每个类到底长什么样。一般该步骤叫做<em>训练分类器</em>或者<em>学习一个模型</em>。</li>
<li><strong>评价</strong>：让分类器来预测它未曾见过的图像的分类标签，并以此来评价分类器的质量。我们会把分类器预测的标签和图像真正的分类标签对比。毫无疑问，分类器预测的分类标签和图像真正的分类标签如果一致，那就是好事，这样的情况越多越好。</li>
</ul>
<h2 id="Nearest-Neighbor分类器"><a href="#Nearest-Neighbor分类器" class="headerlink" title="Nearest Neighbor分类器"></a>Nearest Neighbor分类器</h2><p>作为课程介绍的第一个方法，我们来实现一个<strong>Nearest Neighbor分类器</strong>。虽然这个分类器和卷积神经网络没有任何关系，实际中也极少使用，但通过实现它，可以让读者对于解决图像分类问题的方法有个基本的认识。</p>
<p><strong>图像分类数据集：CIFAR-10。</strong>一个非常流行的图像分类数据集是<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ekriz/cifar.html" target="_blank" rel="noopener"><strong>CIFAR-10</strong></a>。这个数据集包含了60000张32X32的小图像。每张图像都有10种分类标签中的一种。这60000张图像被分为包含50000张图像的训练集和包含10000张图像的测试集。在下图中你可以看见10个类的10张随机图片。</p>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_3.jpg?raw=true" width="400"></center>

<p><strong>左边</strong>：从<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ekriz/cifar.html" target="_blank" rel="noopener"><strong>CIFAR-10</strong></a>数据库来的样本图像。<strong>右边</strong>：第一列是测试图像，然后第一列的每个测试图像右边是使用Nearest Neighbor算法，根据像素差异，从训练集中选出的10张最类似的图片。</p>
<p>—————————————————————————————————————————</p>
<p>假设现在我们有CIFAR-10的50000张图片（每种分类5000张）作为训练集，我们希望将余下的10000作为测试集并给他们打上标签。Nearest Neighbor算法将会拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。上面右边的图片就展示了这样的结果。请注意上面10个分类中，只有3个是准确的。比如第8行中，马头被分类为一个红色的跑车，原因在于红色跑车的黑色背景非常强烈，所以这匹马就被错误分类为跑车了。</p>
<p>那么具体如何比较两张图片呢？在本例中，就是比较32x32x3的像素块。最简单的方法就是逐个像素比较，最后将差异值全部加起来。换句话说，就是将两张图片先转化为两个向量<img src="http://www.zhihu.com/equation?tex=I_1" alt="I_1">和<img src="http://www.zhihu.com/equation?tex=I_2" alt="I_2">，然后计算他们的<strong>L1距离：</strong></p>
<p>这里的求和是针对所有的像素。下面是整个比较流程的图例：</p>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_4.jpg?raw=true" width="300"></center>

<p>以图片中的一个颜色通道为例来进行说明。两张图片使用L1距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么L1距离为0，但是如果两张图片很是不同，那L1值将会非常大。</p>
<p>—————————————————————————————————————————</p>
<p>下面，让我们看看如何用代码来实现这个分类器。首先，我们将CIFAR-10的数据加载到内存中，并分成4个数组：训练数据和标签，测试数据和标签。在下面的代码中，<strong>Xtr</strong>（大小是50000x32x32x3）存有训练集中所有的图像，<strong>Ytr</strong>是对应的长度为50000的1维数组，存有图像对应的分类标签（从0到9）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class="string">'data/cifar10/'</span>) <span class="comment"># a magic function we provide</span></span><br><span class="line"><span class="comment"># flatten out all images to be one-dimensional</span></span><br><span class="line">Xtr_rows = Xtr.reshape(Xtr.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xtr_rows becomes 50000 x 3072</span></span><br><span class="line">Xte_rows = Xte.reshape(Xte.shape[<span class="number">0</span>], <span class="number">32</span> * <span class="number">32</span> * <span class="number">3</span>) <span class="comment"># Xte_rows becomes 10000 x 3072</span></span><br></pre></td></tr></table></figure>
<p>现在我们得到所有的图像数据，并且把他们拉长成为行向量了。接下来展示如何训练并评价一个分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nn = NearestNeighbor() <span class="comment"># create a Nearest Neighbor classifier class</span></span><br><span class="line">nn.train(Xtr_rows, Ytr) <span class="comment"># train the classifier on the training images and labels</span></span><br><span class="line">Yte_predict = nn.predict(Xte_rows) <span class="comment"># predict labels on the test images</span></span><br><span class="line"><span class="comment"># and now print the classification accuracy, which is the average number</span></span><br><span class="line"><span class="comment"># of examples that are correctly predicted (i.e. label matches)</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'accuracy: %f'</span> % ( np.mean(Yte_predict == Yte) )</span><br></pre></td></tr></table></figure>
<p>作为评价标准，我们常常使用<strong>准确率</strong>，它描述了我们预测正确的得分。请注意以后我们实现的所有分类器都需要有这个API：<strong>train(X, y)</strong>函数。该函数使用训练集的数据和标签来进行训练。从其内部来看，类应该实现一些关于标签和标签如何被预测的模型。这里还有个<strong>predict(X)</strong>函数，它的作用是预测输入的新数据的分类标签。现在还没介绍分类器的实现，下面就是使用L1距离的Nearest Neighbor分类器的实现套路：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    <span class="string">""" X is N x D where each row is an example. Y is 1-dimension of size N """</span></span><br><span class="line">    <span class="comment"># the nearest neighbor classifier simply remembers all the training data</span></span><br><span class="line">    self.Xtr = X</span><br><span class="line">    self.ytr = y</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">""" X is N x D where each row is an example we wish to predict label for """</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># lets make sure that the output type matches the input type</span></span><br><span class="line">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over all test rows</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># find the nearest training image to the i'th test image</span></span><br><span class="line">      <span class="comment"># using the L1 distance (sum of absolute value differences)</span></span><br><span class="line">      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = <span class="number">1</span>)</span><br><span class="line">      min_index = np.argmin(distances) <span class="comment"># get the index with smallest distance</span></span><br><span class="line">      Ypred[i] = self.ytr[min_index] <span class="comment"># predict the label of the nearest example</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Ypred</span><br></pre></td></tr></table></figure>
<p>如果你用这段代码跑CIFAR-10，你会发现准确率能达到<strong>38.6%</strong>。这比随机猜测的10%要好，但是比人类识别的水平（<a href="http://link.zhihu.com/?target=http%3A//karpathy.github.io/2011/04/27/manually-classifying-cifar10/" target="_blank" rel="noopener"><strong>据研究推测是94%</strong></a>）和卷积神经网络能达到的95%还是差多了。点击查看基于CIFAR-10数据的<a href="http://link.zhihu.com/?target=http%3A//www.kaggle.com/c/cifar-10/leaderboard" target="_blank" rel="noopener"><strong>Kaggle算法竞赛排行榜</strong></a>。</p>
<p><strong>距离选择</strong>：计算向量间的距离有很多种方法，另一个常用的方法是<strong>L2距离</strong>，从几何学的角度，可以理解为它在计算两个向量间的欧式距离。L2距离的公式如下：</p>
<p>换句话说，我们依旧是在计算像素间的差值，只是先求其平方，然后把这些平方全部加起来，最后对这个和开方。在Numpy中，我们只需要替换上面代码中的1行代码就行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distances = np.sqrt(np.sum(np.square(self.Xtr - X[i,:]), axis = <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>注意在这里使用了<strong>np.sqrt</strong>，但是在实际中可能不用。因为求平方根函数是一个<em>单调函数</em>，它对不同距离的绝对值求平方根虽然改变了数值大小，但依然保持了不同距离大小的顺序。所以用不用它，都能够对像素差异的大小进行正确比较。如果你在CIFAR-10上面跑这个模型，正确率是<strong>35.4%</strong>，比刚才低了一点。</p>
<p><strong>L1和L2比较</strong>。比较这两个度量方式是挺有意思的。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在<a href="http://link.zhihu.com/?target=http%3A//planetmath.org/vectorpnorm" target="_blank" rel="noopener"><strong>p-norm</strong></a>常用的特殊形式。</p>
<h2 id="k-Nearest-Neighbor分类器"><a href="#k-Nearest-Neighbor分类器" class="headerlink" title="k-Nearest Neighbor分类器"></a>k-Nearest Neighbor分类器</h2><p>你可能注意到了，为什么只用最相似的1张图片的标签来作为测试图像的标签呢？这不是很奇怪吗！是的，使用<strong>k-Nearest Neighbor分类器</strong>就能做得更好。它的思想很简单：与其只找最相近的那1个图片的标签，我们找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。从直观感受上就可以看到，更高的k值可以让分类的效果更平滑，使得分类器对于异常值更有抵抗力。</p>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_5.jpg?raw=true" width="350"></center>

<p>上面示例展示了Nearest Neighbor分类器和5-Nearest Neighbor分类器的区别。例子使用了2维的点来表示，分成3类（红、蓝和绿）。不同颜色区域代表的是使用L2距离的分类器的<strong>决策边界</strong>。白色的区域是分类模糊的例子（即图像与两个以上的分类标签绑定）。需要注意的是，在NN分类器中，异常的数据点（比如：在蓝色区域中的绿点）制造出一个不正确预测的孤岛。5-NN分类器将这些不规则都平滑了，使得它针对测试数据的<strong>泛化（generalization）</strong>能力更好（例子中未展示）。注意，5-NN中也存在一些灰色区域，这些区域是因为近邻标签的最高票数相同导致的（比如：2个邻居是红色，2个邻居是蓝色，还有1个是绿色）。</p>
<p>—————————————————————————————————————————</p>
<p>在实际中，大多使用k-NN分类器。但是k值如何确定呢？接下来就讨论这个问题。</p>
<h2 id="用于超参数调优的验证集"><a href="#用于超参数调优的验证集" class="headerlink" title="用于超参数调优的验证集"></a>用于超参数调优的验证集</h2><p>k-NN分类器需要设定k值，那么选择哪个k值最合适的呢？我们可以选择不同的距离函数，比如L1范数和L2范数等，那么选哪个好？还有不少选择我们甚至连考虑都没有考虑到（比如：点积）。所有这些选择，被称为<strong>超参数（hyperparameter）</strong>。在基于数据进行学习的机器学习算法设计中，超参数是很常见的。一般说来，这些超参数具体怎么设置或取值并不是显而易见的。</p>
<p>你可能会建议尝试不同的值，看哪个值表现最好就选哪个。好主意！我们就是这么做的，但这样做的时候要非常细心。特别注意：<strong>决不能使用测试集来进行调优</strong>。当你在设计机器学习算法的时候，应该把测试集看做非常珍贵的资源，不到最后一步，绝不使用它。如果你使用测试集来调优，而且算法看起来效果不错，那么真正的危险在于：算法实际部署后，性能可能会远低于预期。这种情况，称之为算法对测试集<strong>过拟合</strong>。从另一个角度来说，如果使用测试集来调优，实际上就是把测试集当做训练集，由测试集训练出来的算法再跑测试集，自然性能看起来会很好。这其实是过于乐观了，实际部署起来效果就会差很多。所以，最终测试的时候再使用测试集，可以很好地近似度量你所设计的分类器的泛化性能（在接下来的课程中会有很多关于泛化性能的讨论）。</p>
<blockquote>
<p>测试数据集只使用一次，即在训练完成后评价最终的模型时使用。</p>
</blockquote>
<p>好在我们有不用测试集调优的方法。其思路是：从训练集中取出一部分数据用来调优，我们称之为<strong>验证集(validation set)</strong>。以CIFAR-10为例，我们可以用49000个图像作为训练集，用1000个图像作为验证集。验证集其实就是作为假的测试集来调优。下面就是代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># assume we have Xtr_rows, Ytr, Xte_rows, Yte as before</span></span><br><span class="line"><span class="comment"># recall Xtr_rows is 50,000 x 3072 matrix</span></span><br><span class="line">Xval_rows = Xtr_rows[:<span class="number">1000</span>, :] <span class="comment"># take first 1000 for validation</span></span><br><span class="line">Yval = Ytr[:<span class="number">1000</span>]</span><br><span class="line">Xtr_rows = Xtr_rows[<span class="number">1000</span>:, :] <span class="comment"># keep last 49,000 for train</span></span><br><span class="line">Ytr = Ytr[<span class="number">1000</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># find hyperparameters that work best on the validation set</span></span><br><span class="line">validation_accuracies = []</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># use a particular value of k and evaluation on validation data</span></span><br><span class="line">  nn = NearestNeighbor()</span><br><span class="line">  nn.train(Xtr_rows, Ytr)</span><br><span class="line">  <span class="comment"># here we assume a modified NearestNeighbor class that can take a k as input</span></span><br><span class="line">  Yval_predict = nn.predict(Xval_rows, k = k)</span><br><span class="line">  acc = np.mean(Yval_predict == Yval)</span><br><span class="line">  <span class="keyword">print</span> <span class="string">'accuracy: %f'</span> % (acc,)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># keep track of what works on the validation set</span></span><br><span class="line">  validation_accuracies.append((k, acc))</span><br></pre></td></tr></table></figure>
<p>程序结束后，我们会作图分析出哪个k值表现最好，然后用这个k值来跑真正的测试集，并作出对算法的评价。</p>
<blockquote>
<p>把训练集分成训练集和验证集。使用验证集来对所有超参数调优。最后只在测试集上跑一次并报告结果。</p>
</blockquote>
<p><strong>交叉验证</strong>。有时候，训练集数量较小（因此验证集的数量更小），人们会使用一种被称为<strong>交叉验证</strong>的方法，这种方法更加复杂些。还是用刚才的例子，如果是交叉验证集，我们就不是取1000个图像，而是将训练集平均分成5份，其中4份用来训练，1份用来验证。然后我们循环着取其中4份来训练，其中1份来验证，最后取所有5次验证结果的平均值作为算法验证结果。</p>
<p>—————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_6.png?raw=true" width="300"></center><br>这就是5份交叉验证对k值调优的例子。针对每个k值，得到5个准确率结果，取其平均值，然后对不同k值的平均表现画线连接。本例中，当k=7的时算法表现最好（对应图中的准确率峰值）。如果我们将训练集分成更多份数，直线一般会更加平滑（噪音更少）。</p>
<p>—————————————————————————————————————————</p>
<p><strong>实际应用</strong>。在实际情况下，人们不是很喜欢用交叉验证，主要是因为它会耗费较多的计算资源。一般直接把训练集按照50%-90%的比例分成训练集和验证集。但这也是根据具体情况来定的：如果超参数数量多，你可能就想用更大的验证集，而验证集的数量不够，那么最好还是用交叉验证吧。至于分成几份比较好，一般都是分成3、5和10份。</p>
<p>—————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_7.jpg?raw=true" width="300"></center><br>常用的数据分割模式。给出训练集和测试集后，训练集一般会被均分。这里是分成5份。前面4份用来训练，黄色那份用作验证集调优。如果采取交叉验证，那就各份轮流作为验证集。最后模型训练完毕，超参数都定好了，让模型跑一次（而且只跑一次）测试集，以此测试结果评价算法。</p>
<p>—————————————————————————————————————————</p>
<h2 id="Nearest-Neighbor分类器的优劣"><a href="#Nearest-Neighbor分类器的优劣" class="headerlink" title="Nearest Neighbor分类器的优劣"></a>Nearest Neighbor分类器的优劣</h2><p>现在对Nearest Neighbor分类器的优缺点进行思考。首先，Nearest Neighbor分类器易于理解，实现简单。其次，算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来。然而测试要花费大量时间计算，因为每个测试图像需要和所有存储的训练图像进行比较，这显然是一个缺点。在实际应用中，我们关注测试效率远远高于训练效率。其实，我们后续要学习的卷积神经网络在这个权衡上走到了另一个极端：虽然训练花费很多时间，但是一旦训练完成，对新的测试数据进行分类非常快。这样的模式就符合实际使用需求。</p>
<p>Nearest Neighbor分类器的计算复杂度研究是一个活跃的研究领域，若干<strong>Approximate Nearest Neighbor </strong>(ANN)算法和库的使用可以提升Nearest Neighbor分类器在数据上的计算速度（比如：<a href="http://link.zhihu.com/?target=http%3A//www.cs.ubc.ca/research/flann/" target="_blank" rel="noopener"><strong>FLANN</strong></a>）。这些算法可以在准确率和时空复杂度之间进行权衡，并通常依赖一个预处理/索引过程，这个过程中一般包含kd树的创建和k-means算法的运用。</p>
<p>Nearest Neighbor分类器在某些特定情况（比如数据维度较低）下，可能是不错的选择。但是在实际的图像分类工作中，很少使用。因为图像都是高维度数据（他们通常包含很多像素），而高维度向量之间的距离通常是反直觉的。下面的图片展示了基于像素的相似和基于感官的相似是有很大不同的：</p>
<p>—————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_8.png?raw=true" width="350"></center><br>在高维度数据上，基于像素的的距离和感官上的非常不同。上图中，右边3张图片和左边第1张原始图片的L2距离是一样的。很显然，基于像素比较的相似和感官上以及语义上的相似是不同的。</p>
<p>—————————————————————————————————————————</p>
<p>这里还有个视觉化证据，可以证明使用像素差异来比较图像是不够的。z这是一个叫做<a href="http://link.zhihu.com/?target=http%3A//lvdmaaten.github.io/tsne/" target="_blank" rel="noopener"><strong>t-SNE</strong></a>的可视化技术，它将CIFAR-10中的图片按照二维方式排布，这样能很好展示图片之间的像素差异值。在这张图片中，排列相邻的图片L2距离就小。</p>
<p>—————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/image_classification/image_classification_9.jpg?raw=true" width="400"></center><br>上图使用t-SNE的可视化技术将CIFAR-10的图片进行了二维排列。排列相近的图片L2距离小。可以看出，图片的排列是被背景主导而不是图片语义内容本身主导。</p>
<p>——————————————————————————————————————————</p>
<p>具体说来，这些图片的排布更像是一种颜色分布函数，或者说是基于背景的，而不是图片的语义主体。比如，狗的图片可能和青蛙的图片非常接近，这是因为两张图片都是白色背景。从理想效果上来说，我们肯定是希望同类的图片能够聚集在一起，而不被背景或其他不相关因素干扰。为了达到这个目的，我们不能止步于原始像素比较，得继续前进。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>简要说来：</p>
<ul>
<li>介绍了<strong>图像分类</strong>问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。</li>
<li>介绍了一个简单的图像分类器：<strong>最近邻分类器(Nearest Neighbor classifier)</strong>。分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。</li>
<li>选取超参数的正确方法是：将原始训练集分为训练集和<strong>验证集</strong>，我们在验证集上尝试不同的超参数，最后保留表现最好那个。</li>
<li>如果训练数据量不够，使用<strong>交叉验证</strong>方法，它能帮助我们在选取最优超参数的时候减少噪音。</li>
<li>一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。</li>
<li>最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。</li>
<li>最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。</li>
</ul>
<p>在接下来的课程中，我们将专注于解决这些问题和挑战，并最终能够得到超过90%准确率的解决方案。该方案能够在完成学习就丢掉训练集，并在一毫秒之内就完成一张图片的分类。</p>
<h2 id="小结：实际应用k-NN"><a href="#小结：实际应用k-NN" class="headerlink" title="小结：实际应用k-NN"></a>小结：实际应用k-NN</h2><p>如果你希望将k-NN分类器用到实处（最好别用到图像上，若是仅仅作为练手还可以接受），那么可以按照以下流程：</p>
<ol>
<li>预处理你的数据：对你数据中的特征进行归一化（normalize），让其具有零平均值（zero mean）和单位方差（unit variance）。在后面的小节我们会讨论这些细节。本小节不讨论，是因为图像中的像素都是同质的，不会表现出较大的差异分布，也就不需要标准化处理了。</li>
<li>如果数据是高维数据，考虑使用降维方法，比如PCA(<a href="http://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener"><strong>wiki ref</strong></a>, <a href="http://link.zhihu.com/?target=http%3A//cs229.stanford.edu/notes/cs229-notes10.pdf" target="_blank" rel="noopener"><strong>CS229ref</strong></a>, <a href="http://link.zhihu.com/?target=http%3A//www.bigdataexaminer.com/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/" target="_blank" rel="noopener"><strong>blog ref</strong></a>)或<a href="http://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/random_projection.html" target="_blank" rel="noopener"><strong>随机投影</strong></a>。</li>
<li>将数据随机分入训练集和验证集。按照一般规律，70%-90% 数据作为训练集。这个比例根据算法中有多少超参数，以及这些超参数对于算法的预期影响来决定。如果需要预测的超参数很多，那么就应该使用更大的验证集来有效地估计它们。如果担心验证集数量不够，那么就尝试交叉验证方法。如果计算资源足够，使用交叉验证总是更加安全的（份数越多，效果越好，也更耗费计算资源）。</li>
<li>在验证集上调优，尝试足够多的k值，尝试L1和L2两种范数计算方式。</li>
<li>如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor库（比如<a href="http://link.zhihu.com/?target=http%3A//www.cs.ubc.ca/research/flann/" target="_blank" rel="noopener"><strong>FLANN</strong></a>）来加速这个过程，其代价是降低一些准确率。</li>
<li>对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。在实践中，<strong>不要这样做</strong>。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。<strong>直接使用测试集来测试用最优参数设置好的最优模型</strong>，得到测试集数据的分类准确率，并以此作为你的kNN分类器在该数据上的性能表现。</li>
</ol>
<h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><p>下面是一些你可能感兴趣的拓展阅读链接：</p>
<ul>
<li><a href="http://link.zhihu.com/?target=http%3A//homes.cs.washington.edu/%257Epedrod/papers/cacm12.pdf" target="_blank" rel="noopener"><strong>A Few Useful Things to Know about Machine Learning</strong></a>，文中第6节与本节相关，但是整篇文章都强烈推荐。</li>
<li><a href="http://link.zhihu.com/?target=http%3A//people.csail.mit.edu/torralba/shortCourseRLOC/index.html" target="_blank" rel="noopener"><strong>Recognizing and Learning Object Categories</strong></a>，ICCV 2005上的一节关于物体分类的课程。</li>
</ul>
<p><strong>图像分类笔记全文翻译完毕</strong>。</p>
<blockquote>
<p>翻译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/classification" target="_blank" rel="noopener"><strong>image classification notes</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>进行翻译，<a href="https://www.zhihu.com/people/sqfan" target="_blank" rel="noopener">ShiqingFan</a>和<a href="https://www.zhihu.com/people/gong-zi-jia-57" target="_blank" rel="noopener">巩子嘉</a>进行校对修改。</p>
<p>知乎地址：<a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Linear_Classify/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Linear_Classify/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：线性分类笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-26 08:53:54" itemprop="dateModified" datetime="2018-03-26T08:53:54+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：线性分类笔记"><a href="#CS231n课程笔记翻译：线性分类笔记" class="headerlink" title="CS231n课程笔记翻译：线性分类笔记"></a>CS231n课程笔记翻译：线性分类笔记</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li><p>线性分类器简介</p>
</li>
<li><p>线性评分函数</p>
</li>
<li><p>阐明线性分类器</p>
</li>
<li><p>损失函数</p>
<ul>
<li>多类SVM</li>
</ul>
</li>
</ul>
<ul>
<li>Softmax分类器</li>
<li>SVM和Softmax的比较</li>
</ul>
<ul>
<li><p>基于Web的可交互线性分类器原型</p>
</li>
<li><p>小结</p>
</li>
</ul>
<h2 id="线性分类"><a href="#线性分类" class="headerlink" title="线性分类"></a>线性分类</h2><p>上一篇笔记介绍了图像分类问题。图像分类的任务，就是从已有的固定分类标签集合中选择一个并分配给一张图像。我们还介绍了k-Nearest Neighbor （k-NN）分类器，该分类器的基本思想是通过将测试图像与训练集带标签的图像进行比较，来给测试图像打上分类标签。k-Nearest Neighbor分类器存在以下不足：</p>
<ul>
<li>分类器必须<em>记住</em>所有训练数据并将其存储起来，以便于未来测试数据用于比较。这在存储空间上是低效的，数据集的大小很容易就以GB计。</li>
<li>对一个测试图像进行分类需要和所有训练图像作比较，算法计算资源耗费高。</li>
</ul>
<p><strong>概述</strong>：我们将要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：一个是<strong>评分函数（score function）</strong>，它是原始图像数据到类别分值的映射。另一个是<strong>损失函数（loss function）</strong>，它是用来量化预测分类标签的得分与真实标签之间一致性的。该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。</p>
<h2 id="从图像到标签分值的参数化映射"><a href="#从图像到标签分值的参数化映射" class="headerlink" title="从图像到标签分值的参数化映射"></a>从图像到标签分值的参数化映射</h2><p>​        该方法的第一部分就是定义一个评分函数，这个函数将图像的像素值映射为各个分类类别的得分，得分高低代表图像属于该类别的可能性高低。下面会利用一个具体例子来展示该方法。现在假设有一个包含很多图像的训练集$x_i \in R^D$，每个图像都有一个对应的分类标签$y_i$。这里$i=1,2….N$并且$y_i \in 1….K$。这就是说，我们有<strong>N</strong>个图像样例，每个图像的维度是<strong>D</strong>，共有<strong>K</strong>种不同的分类。</p>
<p>举例来说，在CIFAR-10中，我们有一个<strong>N</strong>=50000的训练集，每个图像有<strong>D</strong>=32x32x3=3072个像素，而<strong>K</strong>=10，这是因为图片被分为10个不同的类别（狗，猫，汽车等）。我们现在定义评分函数为：$f:R^D \rightarrow R^K$，该函数是原始图像像素到分类分值的映射。</p>
<p><strong>线性分类器</strong>：在本模型中，我们从最简单的概率函数开始，一个线性映射：</p>
<center>$$f(x_i,W,b)=Wx_i+b$$</center>

<p>在上面的公式中，假设每个图像数据都被拉长为一个长度为D的列向量，大小为[D x 1]。其中大小为[K x D]的矩阵<strong>W</strong>和大小为[K x 1]列向量<strong>b</strong>为该函数的<strong>参数（parameters）</strong>。还是以CIFAR-10为例，$x_i$就包含了第$i$个图像的所有像素信息，这些信息被拉成为一个[3072 x 1]的列向量，W大小为[10x3072]，b的大小为[10x1]。因此，3072个数字（原始像素数值）输入函数，函数输出10个数字（不同分类得到的分值）。参数W被称为权重（weights）。b被称为偏差向量（bias vector），这是因为它影响输出数值，但是并不和原始数据产生关联。在实际情况中，人们常常混用权重和参数这两个术语。</p>
<p>需要注意的几点：</p>
<ul>
<li>首先，一个单独的矩阵乘法$Wx_i$就高效地并行评估10个不同的分类器（每个分类器针对一个分类），其中每个类的分类器就是W的一个行向量。</li>
<li>注意我们认为输入数据$(x_i,y_i)$是给定且不可改变的，但参数<strong>W</strong>和<strong>b</strong>是可控制改变的。我们的目标就是通过设置这些参数，使得计算出来的分类分值情况和训练集中图像数据的真实类别标签相符。在接下来的课程中，我们将详细介绍如何做到这一点，但是目前只需要直观地让正确分类的分值比错误分类的分值高即可。</li>
<li>该方法的一个优势是训练数据是用来学习到参数<strong>W</strong>和<strong>b</strong>的，一旦训练完成，训练数据就可以丢弃，留下学习到的参数即可。这是因为一个测试图像可以简单地输入函数，并基于计算出的分类分值来进行分类。</li>
<li>最后，注意只需要做一个矩阵乘法和一个矩阵加法就能对一个测试数据分类，这比k-NN中将测试图像和所有训练数据做比较的方法快多了。</li>
</ul>
<blockquote>
<p><em>预告：卷积神经网络映射图像像素值到分类分值的方法和上面一样，但是映射$(f)$就要复杂多了，其包含的参数也更多。</em></p>
</blockquote>
<h2 id="理解线性分类器"><a href="#理解线性分类器" class="headerlink" title="理解线性分类器"></a>理解线性分类器</h2><p>线性分类器计算图像中3个颜色通道中所有像素的值与权重的矩阵乘，从而得到分类分值。根据我们对权重设置的值，对于图像中的某些位置的某些颜色，函数表现出喜好或者厌恶（根据每个权重的符号而定）。举个例子，可以想象“船”分类就是被大量的蓝色所包围（对应的就是水）。那么“船”分类器在蓝色通道上的权重就有很多的正权重（它们的出现提高了“船”分类的分值），而在绿色和红色通道上的权重为负的就比较多（它们的出现降低了“船”分类的分值）。</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_0.jpg?raw=true" width="300"></center>

<p>一个将图像映射到分类分值的例子。为了便于可视化，假设图像只有4个像素（都是黑白像素，这里不考虑RGB通道），有3个分类（红色代表猫，绿色代表狗，蓝色代表船，注意，这里的红、绿和蓝3种颜色仅代表分类，和RGB通道没有关系）。首先将图像像素拉伸为一个列向量，与W进行矩阵乘，然后得到各个分类的分值。需要注意的是，这个W一点也不好：猫分类的分值非常低。从上图来看，算法倒是觉得这个图像是一只狗。</p>
<p>————————————————————————————————————————</p>
<p><strong>将图像看做高维度的点</strong>：既然图像被伸展成为了一个高维度的列向量，那么我们可以把图像看做这个高维度空间中的一个点（即每张图像是3072维空间中的一个点）。整个数据集就是一个点的集合，每个点都带有1个分类标签。</p>
<p>既然定义每个分类类别的分值是权重和图像的矩阵乘，那么每个分类类别的分数就是这个空间中的一个线性函数的函数值。我们没办法可视化3072维空间中的线性函数，但假设把这些维度挤压到二维，那么就可以看看这些分类器在做什么了：</p>
<p>——————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_1.jpg?raw=true" width="350"></center>

<p>图像空间的示意图。其中每个图像是一个点，有3个分类器。以红色的汽车分类器为例，红线表示空间中汽车分类分数为0的点的集合，红色的箭头表示分值上升的方向。所有红线右边的点的分数值均为正，且线性升高。红线左边的点分值为负，且线性降低。</p>
<p>—————————————————————————————————————————</p>
<p>从上面可以看到，<strong>W</strong>的每一行都是一个分类类别的分类器。对于这些数字的几何解释是：如果改变其中一行的数字，会看见分类器在空间中对应的直线开始向着不同方向旋转。而偏差<strong>b</strong>，则允许分类器对应的直线平移。需要注意的是，如果没有偏差，无论权重如何，在$x_i=0$时分类分值始终为0。这样所有分类器的线都不得不穿过原点。</p>
<p><strong>将线性分类器看做模板匹配</strong>：关于权重<strong>W</strong>的另一个解释是<strong>它</strong>的每一行对应着一个分类的模板（有时候也叫作<em>原型</em>）。一张图像对应不同分类的得分，是通过使用内积（也叫<em>点积</em>）来比较图像和模板，然后找到和哪个模板最相似。从这个角度来看，线性分类器就是在利用学习到的模板，针对图像做模板匹配。从另一个角度来看，可以认为还是在高效地使用k-NN，不同的是我们没有使用所有的训练集的图像来比较，而是每个类别只用了一张图片（这张图片是我们学习到的，而不是训练集中的某一张），而且我们会使用（负）内积来计算向量间的距离，而不是使用L1或者L2距离。</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_2.jpg?raw=true" width="500"></center>

<p>将课程进度快进一点。这里展示的是以CIFAR-10为训练集，学习结束后的权重的例子。注意，船的模板如期望的那样有很多蓝色像素。如果图像是一艘船行驶在大海上，那么这个模板利用内积计算图像将给出很高的分数。</p>
<p>————————————————————————————————————————</p>
<p>可以看到马的模板看起来似乎是两个头的马，这是因为训练集中的马的图像中马头朝向各有左右造成的。线性分类器将这两种情况融合到一起了。类似的，汽车的模板看起来也是将几个不同的模型融合到了一个模板中，并以此来分辨不同方向不同颜色的汽车。这个模板上的车是红色的，这是因为CIFAR-10中训练集的车大多是红色的。线性分类器对于不同颜色的车的分类能力是很弱的，但是后面可以看到神经网络是可以完成这一任务的。神经网络可以在它的隐藏层中实现中间神经元来探测不同种类的车（比如绿色车头向左，蓝色车头向前等）。而下一层的神经元通过计算不同的汽车探测器的权重和，将这些合并为一个更精确的汽车分类分值。</p>
<p><strong>偏差和权重的合并技巧</strong>：在进一步学习前，要提一下这个经常使用的技巧。它能够将我们常用的参数$W$和$b$合二为一。回忆一下，分类评分函数定义为：</p>
<center>$$f(x_i,W,b)=Wx_i+b$$</center>

<p>分开处理这两个参数（权重参数$W$和偏差参数$b$）有点笨拙，一般常用的方法是把两个参数放到同一个矩阵中，同时$x_i$向量就要增加一个维度，这个维度的数值是常量1，这就是默认的偏差维度。这样新的公式就简化成下面这样：</p>
<center>$$f(x_i,W)=Wx_i$$</center>

<p>还是以CIFAR-10为例，那么$x_i$的大小就变成[3073x1]，而不是[3072x1]了，多出了包含常量1的1个维度）。$W$大小就是[10x3073]了。$W$中多出来的这一列对应的就是偏差值$b$，具体见下图：</p>
<p>————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_3.jpg?raw=true" width="350"></center>

<p>偏差技巧的示意图。左边是先做矩阵乘法然后做加法，右边是将所有输入向量的维度增加1个含常量1的维度，并且在权重矩阵中增加一个偏差列，最后做一个矩阵乘法即可。左右是等价的。通过右边这样做，我们就只需要学习一个权重矩阵，而不用去学习两个分别装着权重和偏差的矩阵了。</p>
<p>—————————————————————————————————————————</p>
<p><strong>图像数据预处理</strong>：在上面的例子中，所有图像都是使用的原始像素值（从0到255）。在机器学习中，对于输入的特征做归一化（normalization）处理是常见的套路。而在图像分类的例子中，图像上的每个像素可以看做一个特征。在实践中，对每个特征减去平均值来<strong>中心化</strong>数据是非常重要的。在这些图片的例子中，该步骤意味着根据训练集中所有的图像计算出一个平均图像值，然后每个图像都减去这个平均值，这样图像的像素值就大约分布在[-127, 127]之间了。下一个常见步骤是，让所有数值分布的区间变为[-1, 1]。<strong>零均值的中心化</strong>是很重要的，等我们理解了梯度下降后再来详细解释。</p>
<h2 id="损失函数-Loss-function"><a href="#损失函数-Loss-function" class="headerlink" title="损失函数 Loss function"></a>损失函数 Loss function</h2><p>在上一节定义了从图像像素值到所属类别的评分函数（score function），该函数的参数是权重矩阵$W$。在函数中，数据$(x_i,y_i)$是给定的，不能修改。但是我们可以调整权重矩阵这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p>
<p>回到之前那张猫的图像分类例子，它有针对“猫”，“狗”，“船”三个类别的分数。我们看到例子中权重值非常差，因为猫分类的得分非常低（-96.8），而狗（437.9）和船（61.95）比较高。我们将使用<strong>损失函数（Loss Function）</strong>（有时也叫<strong>代价函数Cost Function</strong>或<strong>目标函数Objective</strong>）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。</p>
<h3 id="多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss"><a href="#多类支持向量机损失-Multiclass-Support-Vector-Machine-Loss" class="headerlink" title="多类支持向量机损失 Multiclass Support Vector Machine Loss"></a>多类支持向量机损失 Multiclass Support Vector Machine Loss</h3><p>损失函数的具体形式多种多样。首先，介绍常用的多类支持向量机（SVM）损失函数。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值$\Delta$。我们可以把损失函数想象成一个人，这位SVM先生（或者女士）对于结果有自己的品位，如果某个结果能使得损失值更低，那么SVM就更加喜欢它。</p>
<p>让我们更精确一些。回忆一下，第i个数据中包含图像$x_i​$的像素和代表正确类别的标签$y_i​$。评分函数输入像素数据，然后通过公式$f(x_i,W)​$来计算不同分类类别的分值。这里我们将分值简写为$s​$。比如，针对第j个类别的得分就是第j个元素：$s_j=f(x_i,W)_j​$。针对第i个数据的多类SVM的损失函数定义如下：</p>
<center>$$L_i=\sum_{j\neq{y_i}}\max(0,s_j-s_{y_i}+\Delta)$$</center>

<p><strong>举例</strong>：用一个例子演示公式是如何计算的。假设有3个分类，并且得到了分值$s=[13,-7,11]$。其中第一个类别是正确类别，即$y_i=0$。同时假设$\Delta$是10（后面会详细介绍该超参数）。上面的公式是将所有不正确分类（$j\not=y_i$）加起来，所以我们得到两个部分：</p>
<p>可以看到第一个部分结果是0，这是因为[-7-13+10]得到的是负数，经过$max(0,-)$函数处理后得到0。这一对类别分数和标签的损失值是0，这是因为正确分类的得分13与错误分类的得分-7的差为20，高于边界值10。而SVM只关心差距至少要大于10，更大的差值还是算作损失值为0。第二个部分计算[11-13+10]得到8。虽然正确分类的得分比不正确分类的得分要高（13&gt;11），但是比10的边界值还是小了，分差只有2，这就是为什么损失值等于8。简而言之，SVM的损失函数想要正确分类类别$y_i$的分数比不正确类别分数高，而且至少要高$\Delta$。如果不满足这点，就开始计算损失值。</p>
<p>那么在这次的模型中，我们面对的是线性评分函数（$f(x_i,W)=Wx_i$），所以我们可以将损失函数的公式稍微改写一下：</p>
<center>$$L_i=\sum_{j\neq{y_i}}\max(0,w_j^Tx_i-w_{y_i}^Tx_i+\Delta)$$</center>

<p>其中$w_j$是权重的$W$第j行，被变形为列向量。然而，一旦开始考虑更复杂的评分函数$f$公式，这样做就不是必须的了。</p>
<p>在结束这一小节前，还必须提一下的属于是关于0的阀值：$max(0,-)$函数，它常被称为<strong>折叶损失（hinge loss）</strong>。有时候会听到人们使用平方折叶损失SVM（即L2-SVM），它使用的是$max(0,-)^2$，将更强烈（平方地而不是线性地）地惩罚过界的边界值。不使用平方是更标准的版本，但是在某些数据集中，平方折叶损失会工作得更好。可以通过交叉验证来决定到底使用哪个。</p>
<blockquote>
<p>我们对于预测训练集数据分类标签的情况总有一些不满意的，而损失函数就能将这些不满意的程度量化。</p>
</blockquote>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_4.jpg?raw=true" width="400"></center>

<p>多类SVM“想要”正确类别的分类分数比其他不正确分类类别的分数要高，而且至少高出delta的边界值。如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。</p>
<p>—————————————————————————————————————————</p>
<p><strong>正则化（Regularization）：</strong>上面损失函数有一个问题。假设有一个数据集和一个权重集<strong>W</strong>能够正确地分类每个数据（即所有的边界都满足，对于所有的i都有$L_i=0$）。问题在于这个<strong>W</strong>并不唯一：可能有很多相似的<strong>W</strong>都能正确地分类所有的数据。一个简单的例子：如果<strong>W</strong>能够正确分类所有数据，即对于每个数据，损失值都是0。那么当$\lambda&gt;1$时，任何数乘$\lambda W$都能使得损失值为0，因为这个变化将所有分值的大小都均等地扩大了，所以它们之间的绝对差值也扩大了。举个例子，如果一个正确分类的分值和举例它最近的错误分类的分值的差距是15，对<strong>W</strong>乘以2将使得差距变成30。</p>
<p>换句话说，我们希望能向某些特定的权重<strong>W</strong>添加一些偏好，对其他权重则不添加，以此来消除模糊性。这一点是能够实现的，方法是向损失函数增加一个<strong>正则化惩罚(regularization penalty)</strong> $R(W)$部分。最常用的正则化惩罚是L2范式，L2范式通过对所有参数进行逐元素的平方惩罚来抑制大数值的权重：</p>
<center>$$\sum_k\sum_l{W_{k,l}^2}$$</center>

<p>上面的表达式中，将$W$中所有元素平方后求和。注意正则化函数不是数据的函数，仅基于权重。包含正则化惩罚后，就能够给出完整的多类SVM损失函数了，它由两个部分组成：<strong>数据损失（data loss）</strong>，即所有样例的的平均损失$L_i$，以及<strong>正则化损失（regularization loss）</strong>。完整公式如下所示：</p>
<center>$$L=\displaystyle \underbrace{ \frac{1}{N}\sum_i L_i}<em>{data \  loss}+\underbrace{\lambda R(W)}</em>{regularization \ loss}$$</center>

<p>将其展开完整公式是：</p>
<center>$$L=\frac{1}{N}\sum_i \sum_{j\neq{y_i}}[max(0,f(x_i;W)_j-f(x_i;W)_{y_i}+\Delta)]+\lambda\sum_k\sum_l{W_{k,l}^2}$$</center>

<p>其中，$N$是训练集的数据量。现在正则化惩罚添加到了损失函数里面，并用超参数$\lambda$来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。</p>
<p>除了上述理由外，引入正则化惩罚还带来很多良好的性质，这些性质大多会在后续章节介绍。比如引入了L2惩罚后，SVM们就有了<strong>最大边界（max margin）</strong>这一良好性质。（如果感兴趣，可以查看<a href="http://link.zhihu.com/?target=http%3A//cs229.stanford.edu/notes/cs229-notes3.pdf" target="_blank" rel="noopener"><strong>CS229课程</strong></a>）。</p>
<p>其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。举个例子，假设输入向量$x=[1,1,1,1]$，两个权重向量$w_1=[1,0,0,0]$，$w_2=[0.25,0.25,0.25,0.25]$。那么$w^T_1x=w^T_2=1$,两个权重向量都得到同样的内积，但是$w_1$的L2惩罚是1.0，而$w_2$的L2惩罚是0.25。因此，根据L2惩罚来看，$w_2$更好，因为它的正则化损失更小。从直观上来看，这是因为$w_2$的权重值更小且更分散。既然L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免<em>过拟合</em>。</p>
<p>需要注意的是，和权重不同，偏差没有这样的效果，因为它们并不控制输入维度上的影响强度。因此通常只对权重$W$正则化，而不正则化偏差$b$。在实际操作中，可发现这一操作的影响可忽略不计。最后，因为正则化惩罚的存在，不可能在所有的例子中得到0的损失值，这是因为只有当$W=0$的特殊情况下，才能得到损失值为0。</p>
<p><strong>代码</strong>：下面是一个无正则化部分的损失函数的Python实现，有非向量化和半向量化两个形式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_i</span><span class="params">(x, y, W)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  unvectorized version. Compute the multiclass svm loss for a single example (x,y)</span></span><br><span class="line"><span class="string">  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)</span></span><br><span class="line"><span class="string">    with an appended bias dimension in the 3073-rd position (i.e. bias trick)</span></span><br><span class="line"><span class="string">  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)</span></span><br><span class="line"><span class="string">  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  delta = <span class="number">1.0</span> <span class="comment"># see notes about delta later in this section</span></span><br><span class="line">  scores = W.dot(x) <span class="comment"># scores becomes of size 10 x 1, the scores for each class</span></span><br><span class="line">  correct_class_score = scores[y]</span><br><span class="line">  D = W.shape[<span class="number">0</span>] <span class="comment"># number of classes, e.g. 10</span></span><br><span class="line">  loss_i = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> xrange(D): <span class="comment"># iterate over all wrong classes</span></span><br><span class="line">    <span class="keyword">if</span> j == y:</span><br><span class="line">      <span class="comment"># skip for the true class to only loop over incorrect classes</span></span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># accumulate loss for the i-th example</span></span><br><span class="line">    loss_i += max(<span class="number">0</span>, scores[j] - correct_class_score + delta)</span><br><span class="line">  <span class="keyword">return</span> loss_i</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_i_vectorized</span><span class="params">(x, y, W)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  A faster half-vectorized implementation. half-vectorized</span></span><br><span class="line"><span class="string">  refers to the fact that for a single example the implementation contains</span></span><br><span class="line"><span class="string">  no for loops, but there is still one loop over the examples (outside this function)</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  delta = <span class="number">1.0</span></span><br><span class="line">  scores = W.dot(x)</span><br><span class="line">  <span class="comment"># compute the margins for all classes in one vector operation</span></span><br><span class="line">  margins = np.maximum(<span class="number">0</span>, scores - scores[y] + delta)</span><br><span class="line">  <span class="comment"># on y-th position scores[y] - scores[y] canceled and gave delta. We want</span></span><br><span class="line">  <span class="comment"># to ignore the y-th position and only consider margin on max wrong class</span></span><br><span class="line">  margins[y] = <span class="number">0</span></span><br><span class="line">  loss_i = np.sum(margins)</span><br><span class="line">  <span class="keyword">return</span> loss_i</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L</span><span class="params">(X, y, W)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  fully-vectorized implementation :</span></span><br><span class="line"><span class="string">  - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)</span></span><br><span class="line"><span class="string">  - y is array of integers specifying correct class (e.g. 50,000-D array)</span></span><br><span class="line"><span class="string">  - W are weights (e.g. 10 x 3073)</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># evaluate loss over all examples in X without using any for loops</span></span><br><span class="line">  <span class="comment"># left as exercise to reader in the assignment</span></span><br></pre></td></tr></table></figure>
<p>在本小节的学习中，一定要记得SVM损失采取了一种特殊的方法，使得能够衡量对于训练数据预测分类和实际分类标签的一致性。还有，对训练集中数据做出准确分类预测和让损失值最小化这两件事是等价的。</p>
<blockquote>
<p>接下来要做的，就是找到能够使损失值最小化的权重了。</p>
</blockquote>
<h3 id="实际考虑"><a href="#实际考虑" class="headerlink" title="实际考虑"></a>实际考虑</h3><p><strong>设置Delta</strong>：你可能注意到上面的内容对超参数$\Delta$及其设置是一笔带过，那么它应该被设置成什么值？需要通过交叉验证来求得吗？现在看来，该超参数在绝大多数情况下设为$\Delta=1.0$都是安全的。超参数$\Delta$和$\lambda$看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。理解这一点的关键是要知道，权重$W$的大小对于分类分值有直接影响（当然对他们的差异也有直接影响）：当我们将$W$中值缩小，分类分值之间的差异也变小，反之亦然。因此，不同分类分值之间的边界的具体值（比如$\Delta=1$或$\Delta=100$）从某些角度来看是没意义的，因为权重自己就可以控制差异变大和缩小。也就是说，真正的权衡是我们允许权重能够变大到何种程度（通过正则化强度$\lambda$来控制）。</p>
<p><strong>与二元支持向量机（Binary Support Vector Machine）的关系</strong>：在学习本课程前，你可能对于二元支持向量机有些经验，它对于第i个数据的损失计算公式是：</p>
<p>其中，$C$是一个超参数，并且$y_i\in{-1,1}$。可以认为本章节介绍的SVM公式包含了上述公式，上述公式是多类支持向量机公式只有两个分类类别的特例。也就是说，如果我们要分类的类别只有两个，那么公式就化为二元SVM公式。这个公式中的$C$和多类SVM公式中的$\lambda$都控制着同样的权衡，而且它们之间的关系是$C\propto\frac{1}{\lambda}$</p>
<p><strong>备注：在初始形式中进行最优化</strong>。如果在本课程之前学习过SVM，那么对kernels，duals，SMO算法等将有所耳闻。在本课程（主要是神经网络相关）中，损失函数的最优化的始终在非限制初始形式下进行。很多这些损失函数从技术上来说是不可微的（比如当$x=y$时，$max(x,y)$函数就不可微分），但是在实际操作中并不存在问题，因为通常可以使用次梯度。</p>
<p><strong>备注：其他多类SVM公式</strong>。需要指出的是，本课中展示的多类SVM只是多种SVM公式中的一种。另一种常用的公式是<em>One-Vs-All</em>（OVA）SVM，它针对每个类和其他类训练一个独立的二元分类器。还有另一种更少用的叫做<em>All-Vs-All</em>（AVA）策略。我们的公式是按照<a href="http://link.zhihu.com/?target=https%3A//www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es1999-461.pdf" target="_blank" rel="noopener"><strong>Weston and Watkins 1999 (pdf)</strong></a>版本，比OVA性能更强（在构建有一个多类数据集的情况下，这个版本可以在损失值上取到0，而OVA就不行。感兴趣的话在论文中查阅细节）。最后一个需要知道的公式是Structured SVM，它将正确分类的分类分值和非正确分类中的最高分值的边界最大化。理解这些公式的差异超出了本课程的范围。本课程笔记介绍的版本可以在实践中安全使用，而被论证为最简单的OVA策略在实践中看起来也能工作的同样出色（在 Rikin等人2004年的论文<a href="http://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume5/rifkin04a/rifkin04a.pdf" target="_blank" rel="noopener"><strong>In Defense of One-Vs-All Classification (pdf)</strong></a>中可查）。</p>
<h2 id="Softmax分类器"><a href="#Softmax分类器" class="headerlink" title="Softmax分类器"></a>Softmax分类器</h2><p>SVM是最常用的两个分类器之一，而另一个就是<strong>Softmax分类器，</strong>它的损失函数与SVM的损失函数不同。对于学习过二元逻辑回归分类器的读者来说，Softmax分类器就可以理解为逻辑回归分类器面对多个分类的一般化归纳。SVM将输出$f(x_i,W)$作为每个分类的评分（因为无定标，所以难以直接解释）。与SVM不同，Softmax的输出（归一化的分类概率）更加直观，并且从概率上可以解释，这一点后文会讨论。在Softmax分类器中，函数映射$f(x_i;W)=Wx_i$保持不变，但将这些评分值视为每个分类的未归一化的对数概率，并且将<em>折叶损失（hinge loss）</em>替换为<strong>交叉熵损失（cross-entropy loss）</strong>。公式如下：</p>
<center>$\displaystyle Li=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})$ 或等价的 $L_i=-f_{y_i}+log(\sum_je^{f_j})$</center>

<p>在上式中，使用$f_j$来表示分类评分向量$f$中的第j个元素。和之前一样，整个数据集的损失值是数据集中所有样本数据的损失值$L_i$之和。其中函数$f_j(z)=\frac{e^{z_j}}{\sum_ke^{z_k}}$被称作<strong>softmax 函数</strong>：其输入值是一个向量，向量中元素为任意实数的评分值（$z$中的），函数对其进行压缩，输出一个向量，其中每个元素值在0到1之间，且所有元素之和为1。所以，包含softmax函数的完整交叉熵损失看起唬人，实际上还是比较容易理解的。</p>
<p><strong>信息理论视角</strong>：在“真实”分布$p$和估计分布$q$之间的<em>交叉熵</em>定义如下：</p>
<p>因此，Softmax分类器所做的就是最小化在估计分类概率（就是上面的$e^{f_{y_i}}/\sum_je^{f_j}$）和“真实”分布之间的交叉熵，在这个解释中，“真实”分布就是所有概率密度都分布在正确的类别上（比如：$p=[0,…1,…,0]$中在$y_i$的位置就有一个单独的1）。还有，既然交叉熵可以写成熵和相对熵（Kullback-Leibler divergence）$H(p,q)=H(p)+D_{KL}(p||q)$，并且delta函数$p$的熵是0，那么就能等价的看做是对两个分布之间的相对熵做最小化操作。换句话说，交叉熵损失函数“想要”预测分布的所有概率密度都在正确分类上。</p>
<blockquote>
<p> <strong>*译者注</strong>：Kullback-Leibler差异（Kullback-Leibler Divergence）也叫做相对熵（Relative Entropy），它衡量的是相同事件空间里的两个概率分布的差异情况。*</p>
</blockquote>
<p><strong>概率论解释</strong>：先看下面的公式：</p>
<p>可以解释为是给定图像数据$x_i$为参数，分配给正确分类标签$y_i$的归一化概率。为了理解这点，请回忆一下Softmax分类器将输出向量$f$中的评分值解释为没有归一化的对数概率。那么以这些数值做指数函数的幂就得到了没有归一化的概率，而除法操作则对数据进行了归一化处理，使得这些概率的和为1。从概率论的角度来理解，我们就是在最小化正确分类的负对数概率，这可以看做是在进行<em>最大似然估计</em>（MLE）。该解释的另一个好处是，损失函数中的正则化部分$R(W)$可以被看做是权重矩阵$W$的高斯先验，这里进行的是最大后验估计（MAP）而不是最大似然估计。提及这些解释只是为了让读者形成直观的印象，具体细节就超过本课程范围了。</p>
<p><strong>实操事项：数值稳定。</strong>编程实现softmax函数计算的时候，中间项$e^{f_{y_i}}$和$\sum_j e^{f_j}$因为存在指数函数，所以数值可能非常大。除以大数值可能导致数值计算的不稳定，所以学会使用归一化技巧非常重要。如果在分式的分子和分母都乘以一个常数$C$，并把它变换到求和之中，就能得到一个从数学上等价的公式：</p>
<p>$C$的值可自由选择，不会影响计算结果，通过使用这个技巧可以提高计算中的数值稳定性。通常将$C$设为$logC=-max_jf_j$。该技巧简单地说，就是应该将向量$f$中的数值进行平移，使得最大值为0。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">f = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>]) <span class="comment"># 例子中有3个分类，每个评分的数值都很大</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 不妙：数值问题，可能导致数值爆炸</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 那么将f中的值平移到最大值为0：</span></span><br><span class="line">f -= np.max(f) <span class="comment"># f becomes [-666, -333, 0]</span></span><br><span class="line">p = np.exp(f) / np.sum(np.exp(f)) <span class="comment"># 现在OK了，将给出正确结果</span></span><br></pre></td></tr></table></figure>
<p><strong>让人迷惑的命名规则</strong>：精确地说，SVM分类器使用的是<em>折叶损失（hinge loss）</em>，有时候又被称为<em>最大边界损失（max-margin loss）</em>。Softmax分类器使用的是<em>交叉熵损失（corss-entropy loss）</em>。Softmax分类器的命名是从<em>softmax函数</em>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p>
<h2 id="SVM和Softmax的比较"><a href="#SVM和Softmax的比较" class="headerlink" title="SVM和Softmax的比较"></a>SVM和Softmax的比较</h2><p>下图有助于区分这 Softmax和SVM这两种分类器：</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_5.png?raw=true" width="350"></center>

<p>————————————————————————————————————————</p>
<p>针对一个数据点，SVM和Softmax分类器的不同处理方式的例子。两个分类器都计算了同样的分值向量<strong>f</strong>（本节中是通过矩阵乘来实现）。不同之处在于对<strong>f</strong>中分值的解释：SVM分类器将它们看做是分类评分，它的损失函数鼓励正确的分类（本例中是蓝色的类别2）的分值比其他分类的分值高出至少一个边界值。Softmax分类器将这些数值看做是每个分类没有归一化的<strong>对数概率</strong>，鼓励正确分类的归一化的对数概率变高，其余的变低。SVM的最终的损失值是1.58，Softmax的最终的损失值是0.452，但要注意这两个数值没有可比性。只在给定同样数据，在同样的分类器的损失值计算中，它们才有意义。</p>
<p>————————————————————————————————————————</p>
<p><strong>Softmax分类器为每个分类提供了“可能性”</strong>：SVM的计算是无标定的，而且难以针对所有分类的评分值给出直观解释。Softmax分类器则不同，它允许我们计算出对于所有分类标签的可能性。举个例子，针对给出的图像，SVM分类器可能给你的是一个[12.5, 0.6, -23.0]对应分类“猫”，“狗”，“船”。而softmax分类器可以计算出这三个标签的”可能性“是[0.9, 0.09, 0.01]，这就让你能看出对于不同分类准确性的把握。为什么我们要在”可能性“上面打引号呢？这是因为可能性分布的集中或离散程度是由正则化参数λ直接决定的，λ是你能直接控制的一个输入参数。举个例子，假设3个分类的原始分数是[1, -2, 0]，那么softmax函数就会计算：</p>
<p>现在，如果正则化参数λ更大，那么权重W就会被惩罚的更多，然后他的权重数值就会更小。这样算出来的分数也会更小，假设小了一半吧[0.5, -1, 0]，那么softmax函数的计算就是：</p>
<p>现在看起来，概率的分布就更加分散了。还有，随着正则化参数λ不断增强，权重数值会越来越小，最后输出的概率会接近于均匀分布。这就是说，softmax分类器算出来的概率最好是看成一种对于分类正确性的自信。和SVM一样，数字间相互比较得出的大小顺序是可以解释的，但其绝对值则难以直观解释<strong>。</strong></p>
<p><strong>在实际使用中，SVM和Softmax经常是相似的</strong>：通常说来，两种分类器的表现差别很小，不同的人对于哪个分类器更好有不同的看法。相对于Softmax分类器，SVM更加“局部目标化（local objective）”，这既可以看做是一个特性，也可以看做是一个劣势。考虑一个评分是[10, -2, 3]的数据，其中第一个分类是正确的。那么一个SVM（$\Delta =1$）会看到正确分类相较于不正确分类，已经得到了比边界值还要高的分数，它就会认为损失值是0。SVM对于数字个体的细节是不关心的：如果分数是[10, -100, -100]或者[10, 9, 9]，对于SVM来说没设么不同，只要满足超过边界值等于1，那么损失值就等于0。</p>
<p>对于softmax分类器，情况则不同。对于[10, 9, 9]来说，计算出的损失值就远远高于[10, -100, -100]的。换句话来说，softmax分类器对于分数是永远不会满意的：正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小。但是，SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。</p>
<h2 id="交互式的网页Demo"><a href="#交互式的网页Demo" class="headerlink" title="交互式的网页Demo"></a>交互式的网页Demo</h2><p>————————————————————————————————————————</p>
<p><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/linear_classification/linear_classification_6.jpg?raw=true" width="400"></center>我们实现了一个交互式的网页原型，来帮助读者直观地理解线性分类器。原型将损失函数进行可视化，画面表现的是对于2维数据的3种类别的分类。原型在课程进度上稍微超前，展现了最优化的内容，最优化将在下一节课讨论。</p>
<p>————————————————————————————————————————</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>总结如下：</p>
<ul>
<li>定义了从图像像素映射到不同类别的分类评分的评分函数。在本节中，评分函数是一个基于权重<strong>W</strong>和偏差<strong>b</strong>的线性函数。</li>
<li>与kNN分类器不同，<strong>参数方法</strong>的优势在于一旦通过训练学习到了参数，就可以将训练数据丢弃了。同时该方法对于新的测试数据的预测非常快，因为只需要与权重<strong>W</strong>进行一个矩阵乘法运算。</li>
<li>介绍了偏差技巧，让我们能够将偏差向量和权重矩阵合二为一，然后就可以只跟踪一个矩阵。</li>
<li>定义了损失函数（介绍了SVM和Softmax线性分类器最常用的2个损失函数）。损失函数能够衡量给出的参数集与训练集数据真实类别情况之间的一致性。在损失函数的定义中可以看到，对训练集数据做出良好预测与得到一个足够低的损失值这两件事是等价的。</li>
</ul>
<p>现在我们知道了如何基于参数，将数据集中的图像映射成为分类的评分，也知道了两种不同的损失函数，它们都能用来衡量算法分类预测的质量。但是，如何高效地得到能够使损失值最小的参数呢？这个求得最优参数的过程被称为最优化，将在下节课中进行介绍。</p>
<h2 id="拓展阅读"><a href="#拓展阅读" class="headerlink" title="拓展阅读"></a>拓展阅读</h2><p>下面的内容读者可根据兴趣选择性阅读。</p>
<ul>
<li><a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1306.0239" target="_blank" rel="noopener"><strong>Deep Learning using Linear Support Vector Machines</strong></a>一文的作者是Tang Charlie，论文写于2013年，展示了一些L2SVM比Softmax表现更出色的结果。</li>
</ul>
<p><strong>线性分类笔记全文翻译完毕</strong>。</p>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/linear-classify/" target="_blank" rel="noopener"><strong>Linear Classification Note</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改</p>
<p>知乎地址：（上，中，下）</p>
<p><a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit</a></p>
<p> <a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/02/11/CS231n/CS231n_Neural_Network2/"/>

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Heroinlin"/>
      <meta itemprop="description" content=""/>
      <meta itemprop="image" content="/images/leaf.jpg"/>
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Heroinlin's Blog"/>
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/02/11/CS231n/CS231n_Neural_Network2/" class="post-title-link" itemprop="url">CS231n课程笔记翻译：神经网络笔记2</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-02-11 11:11:11" itemprop="dateCreated datePublished" datetime="2018-02-11T11:11:11+08:00">2018-02-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-03-26 08:53:34" itemprop="dateModified" datetime="2018-03-26T08:53:34+08:00">2018-03-26</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/CS231n课程笔记翻译/" itemprop="url" rel="index"><span itemprop="name">CS231n课程笔记翻译</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="CS231n课程笔记翻译：神经网络笔记-2"><a href="#CS231n课程笔记翻译：神经网络笔记-2" class="headerlink" title="CS231n课程笔记翻译：神经网络笔记 2"></a>CS231n课程笔记翻译：神经网络笔记 2</h1><h2 id="原文如下"><a href="#原文如下" class="headerlink" title="原文如下"></a>原文如下</h2><p>内容列表：</p>
<ul>
<li>设置数据和模型<ul>
<li>数据预处理</li>
<li>权重初始化</li>
<li>批量归一化（Batch Normalization）</li>
<li>正则化（L2/L1/Maxnorm/Dropout）</li>
</ul>
</li>
<li>损失函数</li>
<li>小结</li>
</ul>
<h2 id="设置数据和模型"><a href="#设置数据和模型" class="headerlink" title="设置数据和模型"></a>设置数据和模型</h2><p>在上一节中介绍了神经元的模型，它在计算内积后进行非线性激活函数计算，神经网络将这些神经元组织成各个层。这些做法共同定义了<strong>评分函数（score function）</strong> 的新形式，该形式是从前面线性分类章节中的简单线性映射发展而来的。具体来说，神经网络就是进行了一系列的线性映射与非线性激活函数交织的运算。本节将讨论更多的算法设计选项，比如数据预处理，权重初始化和损失函数。</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>关于数据预处理我们有3个常用的符号，数据矩阵<strong>X</strong>，假设其尺寸是 <strong>[N x D]</strong> （<strong>N</strong> 是数据样本的数量，<strong>D</strong> 是数据的维度）。</p>
<p><strong>均值减法（Mean subtraction）</strong> 是预处理最常用的形式。它对数据中每个独立<em>特征</em>减去平均值，从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。在numpy中，该操作可以通过代码 <strong>X -= np.mean(X, axis=0)</strong> 实现。而对于图像，更常用的是对所有像素都减去一个值，可以用 <strong>X -= np.mean(X)</strong> 实现，也可以在3个颜色通道上分别操作。</p>
<p><strong>归一化（Normalization）</strong> 是指将数据的所有维度都归一化，使其数值范围都近似相等。有两种常用方法可以实现归一化。第一种是先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为 <strong>X /= np.std(X, axis=0)</strong> 。第二种方法是对每个维度都做归一化，使得每个维度的最大和最小值是1和-1。这个预处理操作只有在确信不同的输入特征有不同的数值范围（或计量单位）时才有意义，但要注意预处理操作的重要性几乎等同于学习算法本身。在图像处理中，由于像素的数值范围几乎是一致的（都在0-255之间），所以进行这个额外的预处理步骤并不是很必要。</p>
<p>——————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_10.jpg?raw=true" width="400"></center><br>一般数据预处理流程： <strong>左边</strong> :原始的2维输入数据。 <strong>中间</strong> :在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。 <strong>右边</strong> : 每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。<br><br>——————————————————————————————————————————<br><br><strong>PCA和白化（Whitening）</strong> 是另一种预处理形式。在这种处理中，先对数据进行零中心化处理，然后计算协方差矩阵，它展示了数据中的相关性结构。<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设输入数据矩阵X的尺寸为[N x D]</span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">0</span>) <span class="comment"># 对数据进行零中心化(重要)</span></span><br><span class="line">cov = np.dot(X.T, X) / X.shape[<span class="number">0</span>] <span class="comment"># 得到数据的协方差矩阵</span></span><br></pre></td></tr></table></figure><br><br>数据协方差矩阵的第(i, j)个元素是数据第i个和第j个维度的<em>协方差</em>。具体来说，该矩阵的对角线上的元素是方差。还有，协方差矩阵是对称和<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Positive-definite_matrix%23Negative-definite.2C_semidefinite_and_indefinite_matrices" target="_blank" rel="noopener"><strong>半正定</strong></a>的。我们可以对数据协方差矩阵进行SVD（奇异值分解）运算。<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">U,S,V = np.linalg.svd(cov)</span><br></pre></td></tr></table></figure><br><br>U的列是特征向量，S是装有奇异值的1维数组（因为cov是对称且半正定的，所以S中元素是特征值的平方）。为了去除数据相关性，将已经零中心化处理过的原始数据投影到特征基准上：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xrot = np.dot(X,U) <span class="comment"># 对数据去相关性</span></span><br></pre></td></tr></table></figure><br><br>注意U的列是标准正交向量的集合（范式为1，列之间标准正交），所以可以把它们看做标准正交基向量。因此，投影对应x中的数据的一个旋转，旋转产生的结果就是新的特征向量。如果计算<strong>Xrot</strong>的协方差矩阵，将会看到它是对角对称的。<strong>np.linalg.svd</strong>的一个良好性质是在它的返回值<strong>U</strong>中，特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有<strong>方差</strong>的维度。 这个操作也被称为主成分分析（ <a href="http://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener"><strong>Principal Component Analysis</strong></a> 简称PCA）降维：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xrot_reduced = np.dot(X, U[:,:<span class="number">100</span>]) <span class="comment"># Xrot_reduced 变成 [N x 100]</span></span><br></pre></td></tr></table></figure><br><br>经过上面的操作，将原始的数据集的大小由[N x D]降到了[N x 100]，留下了数据中包含最大<strong>方差</strong>的100个维度。通常使用PCA降维过的数据训练线性分类器和神经网络会达到非常好的性能效果，同时还能节省时间和存储器空间。<br><br>最后一个在实践中会看见的变换是<strong>白化（whitening）</strong>。白化操作的输入是特征基准上的数据，然后对每个维度除以其特征值来对数值范围进行归一化。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个均值为零，且协方差相等的矩阵。该操作的代码如下：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据进行白化操作:</span></span><br><span class="line"><span class="comment"># 除以特征值 </span></span><br><span class="line">Xwhite = Xrot / np.sqrt(S + <span class="number">1e-5</span>)</span><br></pre></td></tr></table></figure><br><br><em>警告：夸大的噪声</em>。注意分母中添加了1e-5（或一个更小的常量）来防止分母为0。该变换的一个缺陷是在变换的过程中可能会夸大数据中的噪声，这是因为它将所有维度都拉伸到相同的数值范围，这些维度中也包含了那些只有极少差异性(方差小)而大多是噪声的维度。在实际操作中，这个问题可以用更强的平滑来解决（例如：采用比1e-5更大的值）。<br><br>——————————————————————————————————————————<br><br><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_11.jpg?raw=true" width="400"></center><br>PCA/白化。 <strong>左边</strong> 是二维的原始数据。 <strong>中间</strong> : 经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。 <strong>右边</strong> : 每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。<br><br>——————————————————————————————————————————<br><br>我们可以使用CIFAR-10数据将这些变化可视化出来。CIFAR-10训练集的大小是50000x3072，其中每张图片都可以拉伸为3072维的行向量。我们可以计算[3072 x 3072]的协方差矩阵然后进行奇异值分解（比较耗费计算性能），那么经过计算的特征向量看起来是什么样子呢？<br><br>—————————————————————————————————————————<br><br><center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_12.jpg?raw=true" width="400"></center><br><strong>最左</strong>: 一个用于演示的集合，含49张图片。<strong>左二</strong>: 3072个特征值向量中的前144个。靠前面的特征向量解释了数据中大部分的方差，可以看见它们与图像中较低的频率相关。<strong>第三张</strong> 是49张经过了PCA降维处理的图片，展示了144个特征向量。这就是说，展示原始图像是每个图像用3072维的向量，向量中的元素是图片上某个位置的像素在某个颜色通道中的亮度值。而现在每张图片只使用了一个144维的向量，其中每个元素表示了特征向量对于组成这张图片的贡献度。为了让图片能够正常显示，需要将144维度重新变成基于像素基准的3072个数值。因为U是一个旋转，可以通过乘以U.transpose()[:144,:]来实现，然后将得到的3072个数值可视化。可以看见图像变得有点模糊了，这正好说明前面的特征向量获取了较低的频率。然而，大多数信息还是保留了下来。<strong>最右</strong>: 将“白化”后的数据进行显示。其中144个维度中的方差都被压缩到了相同的数值范围。然后144个白化后的数值通过乘以U.transpose()[:144,:]转换到图像像素基准上。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。<br><br>——————————————————————————————————————————<br><br><strong>实践操作:</strong>  在这个笔记中提到PCA和白化主要是为了介绍的完整性，实际上在卷积神经网络中并不会采用这些变换。然而对数据进行零中心化操作还是非常重要的，对每个像素进行归一化也很常见。<br><br><strong>常见错误:</strong> 进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。<strong>应该怎么做呢？应该先分成训练/验证/测试集，只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。</strong><br><br><strong>译者注：此处确为初学者常见错误，请务必注意！</strong><br><br>### 权重初始化<br><br>我们已经看到如何构建一个神经网络的结构并对数据进行预处理，但是在开始训练网络之前，还需要初始化网络的参数。<br><br><strong>错误：全零初始化</strong>  让我们从应该避免的错误开始。在训练完毕后，虽然不知道网络中每个权重的最终值应该是多少，但如果数据经过了恰当的归一化的话，就可以假设所有权重数值中大约一半为正数，一半为负数。这样，一个听起来蛮合理的想法就是把这些权重的初始值都设为0吧，因为在期望上来说0是最合理的猜测。这个做法错误的！因为如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值，神经元之间就失去了不对称性的源头。<br><br><strong>小随机数初始化</strong> 因此，权重初始值要非常接近0又不能等于0。解决方法就是将权重初始化为很小的数值，以此来<em>打破对称性</em>。其思路是：如果神经元刚开始的时候是随机且不相等的，那么它们将计算出不同的更新，并将自身变成整个网络的不同部分。小随机数权重初始化的实现方法是： <strong>W = 0.01 * np.random.randn(D,H)</strong> 。其中 <strong>randn</strong> 函数是基于零均值和标准差的一个高斯分布（ <strong>译者注：国内教程一般习惯称均值参数为期望  $\mu$  </strong> ）来生成随机数的。根据这个式子，每个神经元的权重向量都被初始化为一个随机向量，而这些随机向量又服从一个多变量高斯分布，这样在输入空间中，所有的神经元的指向是随机的。也可以使用均匀分布生成的随机数，但是从实践结果来看，对于算法的结果影响极小。<br><br><strong>*警告</strong>   并不是小数值一定会得到好的结果。例如，一个神经网络的层中的权重值很小，那么在反向传播的时候就会计算出非常小的梯度（因为梯度与权重值是成比例的）。这就会很大程度上减小反向传播中的“梯度信号”，在深度网络中，就会出现问题。<br><br><strong>使用1/sqrt(n)校准方差</strong>  上面做法存在一个问题，随着输入数据量的增长，随机初始化的神经元的输出数据的分布中的方差也在增大。我们可以除以输入数据量的平方根来调整其数值范围，这样神经元输出的方差就归一化到1了。也就是说，建议将神经元的权重向量初始化为： <strong>w = np.random.randn(n) / sqrt(n)。</strong> 其中<strong>n</strong>是输入数据的数量。这样就保证了网络中所有神经元起始时有近似同样的输出分布。实践经验证明，这样做可以提高收敛的速度。<br><br>上述结论的推导过程如下：假设权重$w$和输入$x$之间的内积为$s=\sum^n_iw_ix_i$，这是还没有进行非线性激活函数运算之前的原始数值。我们可以检查$s$的方差：<br><br><center>$$\begin{align}\text{Var}(s) &amp;= \text{Var}(\sum_i^n w_ix_i) \&amp;= \sum_i^n \text{Var}(w_ix_i) \&amp;= \sum_i^n [E(w_i)]^2\text{Var}(x_i) + E[(x_i)]^2\text{Var}(w_i) + \text{Var}(x_i)\text{Var}(w_i) \&amp;= \sum_i^n \text{Var}(x_i)\text{Var}(w_i) \&amp;= \left( n \text{Var}(w) \right) \text{Var}(x)\end{align} $$</center>

<p>在前两步，使用了<a href="http://link.zhihu.com/?target=http%3A//en.wikipedia.org/wiki/Variance" target="_blank" rel="noopener"><strong>方差的性质</strong></a>。在第三步，因为假设输入和权重的平均值都是0，所以$E[x_i]=E[w_i]=0$。注意这并不是一般化情况，比如在ReLU单元中均值就为正。在最后一步，我们假设所有的$w_i,x_i$都服从同样的分布。从这个推导过程我们可以看见，如果想要$s$有和输入$x$一样的方差，那么在初始化的时候必须保证每个权重$w$的方差是$1/n$。又因为对于一个随机变量$X$和标量$a$，有$Var(aX)=a^2Var(X)$，这就说明可以基于一个标准高斯分布，然后乘以$a=\sqrt{1/n}$，使其方差为$1/n$)，于是得出：<strong>w = np.random.randn(n) / sqrt(n)</strong>。</p>
<p>Glorot等在论文<a href="http://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener"><strong>Understanding the difficulty of training deep feedforward neural networks</strong></a>中作出了类似的分析。在论文中，作者推荐初始化公式为$ ( \text{Var}(w) = 2/(n_{in} + n_{out}) ) $，其中$(n_{in}, n_{out})$是在前一层和后一层中单元的个数。这是基于妥协和对反向传播中梯度的分析得出的结论。该主题下最新的一篇论文是：<a href="http://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852" target="_blank" rel="noopener"><strong>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</strong></a>，作者是He等人。文中给出了一种针对ReLU神经元的特殊初始化，并给出结论：网络中神经元的方差应该是$2.0/n$。代码为<strong>w = np.random.randn(n) * sqrt(2.0/n)</strong>。这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。</p>
<p><strong>稀疏初始化（Sparse initialization）</strong> 另一个处理非标定方差的方法是将所有权重矩阵设为0，但是为了打破对称性，每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）。一个比较典型的连接数目是10个。</p>
<p><strong>偏置（biases）的初始化</strong>  通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。</p>
<p><strong>实践</strong>  当前的推荐是使用ReLU激活函数，并且使用 <strong>w = np.random.randn(n) * sqrt(2.0/n)</strong> 来进行权重初始化，关于这一点，<a href="http://link.zhihu.com/?target=http%3A//arxiv-web3.library.cornell.edu/abs/1502.01852" target="_blank" rel="noopener"><strong>这篇文章</strong></a>有讨论。</p>
<p><strong>批量归一化（Batch Normalization）</strong> <a href="http://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.03167" target="_blank" rel="noopener"><strong>批量归一化</strong></a>是loffe和Szegedy最近才提出的方法，该方法减轻了如何合理初始化神经网络这个棘手问题带来的头痛：），其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。因为归一化是一个简单可求导的操作，所以上述思路是可行的。在实现层面，应用这个技巧通常意味着全连接层（或者是卷积层，后续会讲）与激活函数之间添加一个BatchNorm层。对于这个技巧本节不会展开讲，因为上面的参考文献中已经讲得很清楚了，需要知道的是在神经网络中使用批量归一化已经变得非常常见。在实践中，使用了批量归一化的网络对于不好的初始值有更强的鲁棒性。最后一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！</p>
<h3 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 Regularization"></a>正则化 Regularization</h3><p>有不少方法是通过控制神经网络的容量来防止其过拟合的：</p>
<p><strong>L2正则化</strong> 可能是最常用的正则化方法了。可以通过惩罚目标函数中所有参数的平方将其实现。即对于网络中的每个权重$w$，向目标函数中增加一个$\frac{1}{2}\lambda w^2$，其中$\lambda$是正则化强度。前面这个$\frac{1}{2}$很常见，是因为加上$\frac{1}{2}$后，该式子关于$w$梯度就是$\lambda w$而不是$2\lambda w$了。L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。在线性分类章节中讨论过，由于输入和权重之间的乘法操作，这样就有了一个优良的特性：使网络更倾向于使用所有输入特征，而不是严重依赖输入特征中某些小部分特征。最后需要注意在梯度下降和参数更新的时候，使用L2正则化意味着所有的权重都以<strong>w += -lambda * W</strong>向着0线性下降。</p>
<p><strong>L1正则化</strong>  是另一个相对常用的正则化方法。对于每个$w$我们都向目标函数增加一个$\lambda|w|$。L1和L2正则化也可以进行组合：$\lambda_1|w|+\lambda_2w^2$，这也被称作<a href="http://link.zhihu.com/?target=http%3A//web.stanford.edu/%257Ehastie/Papers/B67.2%2520%25282005%2529%2520301-320%2520Zou%2520%26%2520Hastie.pdf" target="_blank" rel="noopener"><strong>Elastic net regularizaton</strong></a>。L1正则化有一个有趣的性质，它会让权重向量在最优化的过程中变得稀疏（即非常接近0）。也就是说，使用L1正则化的神经元最后使用的是它们最重要的输入数据的稀疏子集，同时对于噪音输入则几乎是不变的了。相较L1正则化，L2正则化中的权重向量大多是分散的小数字。在实践中，如果不是特别关注某些明确的特征选择，一般说来L2正则化都会比L1正则化效果好。</p>
<p><strong>最大范式约束（Max norm constraints）</strong> 另一种形式的正则化是给每个神经元中权重向量的量级设定上限，并使用投影梯度下降来确保这一约束。在实践中，与之对应的是参数更新方式不变，然后要求神经元中的权重向量$\overrightarrow{w}$必须满足$||\overrightarrow{w}||_2&lt;c$这一条件，一般$c$值为3或者4。有研究者发文称在使用这种正则化方法时效果更好。这种正则化还有一个良好的性质，即使在学习率设置过高的时候，网络中也不会出现数值“爆炸”，这是因为它的参数更新始终是被限制着的。</p>
<p><strong>随机失活（Dropout）</strong> 是一个简单又极其有效的正则化方法。该方法由Srivastava在论文<a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%257Ersalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener"><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</strong></a>中提出的，与L1正则化，L2正则化和最大范式约束等方法互为补充。在训练的时候，随机失活的实现方法是让神经元以超参数$p$的概率被激活或者被设置为0。</p>
<p>—————————————————————————————————————————</p>
<center><img src="https://github.com/heroinlin/picture_of_markdown/blob/master/cs231n/neural_nets/neural_nets_13.jpg?raw=true" width="400"></center><br>图片来源自 <a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%7Ersalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener"><strong>论文</strong></a>  ，展示其核心思路。在训练过程中，随机失活可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。<br><br>—————————————————————————————————————————<br><br>一个3层神经网络的普通版随机失活可以用下面代码实现：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" 普通版随机失活: 不推荐实现 (看下面笔记) """</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="string">""" X中是输入数据 """</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = np.random.rand(*H1.shape) &lt; p <span class="comment"># 第一个随机失活遮罩</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = np.random.rand(*H2.shape) &lt; p <span class="comment"># 第二个随机失活遮罩</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2) * p <span class="comment"># 注意：激活数据要乘以p</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure><br><br>在上面的代码中， <strong>train_step</strong> 函数在第一个隐层和第二个隐层上进行了两次随机失活。在输入层上面进行随机失活也是可以的，为此需要为输入数据 <strong>X创建</strong> 一个二值的遮罩。反向传播保持不变，但是肯定需要将遮罩 <strong>U1</strong> 和 <strong>U2</strong> 加入进去。<br><br>注意：在 <strong>predict</strong> 函数中不进行随机失活，但是对于两个隐层的输出都要乘以$p$，调整其数值范围。这一点非常重要，因为在测试时所有的神经元都能看见它们的输入，因此我们想要神经元的输出与训练时的预期输出是一致的。以$p=0.5$为例，在测试时神经元必须把它们的输出减半，这是因为在训练的时候它们的输出只有一半。为了理解这点，先假设有一个神经元$x$的输出，那么进行随机失活的时候，该神经元的输出就是$px+(1-p)0$)，这是有$1-p$的概率神经元的输出为0。在测试时神经元总是激活的，就必须调整$x\to px$来保持同样的预期输出。在测试时会在所有可能的二值遮罩（也就是数量庞大的所有子网络）中迭代并计算它们的协作预测，进行这种减弱的操作也可以认为是与之相关的。<br><br>上述操作不好的性质是必须在测试时对激活数据要按照$p$进行数值范围调整。既然测试性能如此关键，实际更倾向使用 <strong>反向随机失活（inverted dropout）</strong> ，它是在训练时就进行数值范围调整，从而让前向传播在测试时保持不变。这样做还有一个好处，无论你决定是否使用随机失活，预测方法的代码可以保持不变。反向随机失活的代码如下：<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" </span></span><br><span class="line"><span class="string">反向随机失活: 推荐实现方式.</span></span><br><span class="line"><span class="string">在训练的时候drop和调整数值范围，测试时不做任何事.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> <span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  U1 = (np.random.rand(*H1.shape) &lt; p) / p <span class="comment"># 第一个随机失活遮罩. 注意/p!</span></span><br><span class="line">  H1 *= U1 <span class="comment"># drop!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  U2 = (np.random.rand(*H2.shape) &lt; p) / p <span class="comment"># 第二个随机失活遮罩. 注意/p!</span></span><br><span class="line">  H2 *= U2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1) <span class="comment"># 不用数值范围调整了</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure><br><br>在随机失活发布后，很快有大量研究为什么它的实践效果如此之好，以及它和其他正则化方法之间的关系。如果你感兴趣，可以看看这些文献：<br><br>- <a href="http://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/%257Ersalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener"><strong>Dropout paper</strong></a> by Srivastava et al. 2014.<br>- <a href="http://link.zhihu.com/?target=http%3A//papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf" target="_blank" rel="noopener"><strong>Dropout Training as Adaptive Regularization</strong></a>：“我们认为：在使用费希尔信息矩阵（<a href="http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Fisher_information_metric" target="_blank" rel="noopener"><strong>fisher information matrix</strong></a>）的对角逆矩阵的期望对特征进行数值范围调整后，再进行L2正则化这一操作，与随机失活正则化是一阶相等的。”<br><br><strong>前向传播中的噪音</strong> 在更一般化的分类上，随机失活属于网络在前向传播中有随机行为的方法。测试时，通过<em>分析法</em>（在使用随机失活的本例中就是乘以$p$）或<em>数值法</em>（例如通过抽样出很多子网络，随机选择不同子网络进行前向传播，最后对它们取平均）将噪音边缘化。在这个方向上的另一个研究是<a href="http://link.zhihu.com/?target=http%3A//cs.nyu.edu/%257Ewanli/dropc/" target="_blank" rel="noopener"><strong>DropConnect</strong></a>，它在前向传播的时候，一系列权重被随机设置为0。提前说一下，卷积神经网络同样会吸取这类方法的优点，比如随机汇合（stochastic pooling），分级汇合（fractional pooling），数据增长（data augmentation）。我们在后面会详细介绍。<br><br><strong>偏置正则化</strong> 在线性分类器的章节中介绍过，对于偏置参数的正则化并不常见，因为它们在矩阵乘法中和输入数据并不产生互动，所以并不需要控制其在数据维度上的效果。然而在实际应用中（使用了合理数据预处理的情况下），对偏置进行正则化也很少会导致算法性能变差。这可能是因为相较于权重参数，偏置参数实在太少，所以分类器需要它们来获得一个很好的数据损失，那么还是能够承受的。<br><br><strong>每层正则化</strong> 对于不同的层进行不同强度的正则化很少见（可能除了输出层以外），关于这个思路的相关文献也很少。<br><br><strong>实践</strong> : 通过交叉验证获得一个全局使用的L2正则化强度是比较常见的。在使用L2正则化的同时在所有层后面使用随机失活也很常见。$p$值一般默认设为0.5，也可能在验证集上调参。<br><br>## 损失函数<br><br>我们已经讨论过损失函数的正则化损失部分，它可以看做是对模型复杂程度的某种惩罚。损失函数的第二个部分是<em>数据损失</em>，它是一个有监督学习问题，用于衡量分类算法的预测结果（即分类评分）和真实标签结果之间的一致性。数据损失是对所有样本的数据损失求平均。也就是说，$L=\frac{1}{N}\sum_iL_i$中，$N$是训练集数据的样本数。让我们把神经网络中输出层的激活函数简写为$f=f(x_i;W)$，在实际中你可能需要解决以下几类问题：<br><br><strong>分类问题</strong> 是我们一直讨论的。在该问题中，假设有一个装满样本的数据集，每个样本都有一个唯一的正确标签（是固定分类标签之一）。在这类问题中，一个最常见的损失函数就是SVM（是Weston Watkins 公式）：<br><br><center>$$L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)$$</center>

<p>之前简要提起过，有些学者的论文中指出平方折叶损失（即使用$max(0,f_j-f_{y_i}+1)^2$）算法的结果会更好。第二个常用的损失函数是Softmax分类器，它使用交叉熵损失：</p>
<center>$$L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)$$</center>

<p><strong>问题：类别数目巨大</strong> 当标签集非常庞大（例如字典中的所有英语单词，或者ImageNet中的22000种分类），就需要使用 <em>分层Softmax（ <strong>Hierarchical Softmax</strong> ）</em> 了（<a href="http://link.zhihu.com/?target=http%3A//arxiv.org/pdf/1310.4546.pdf" target="_blank" rel="noopener"><strong>参考文献</strong></a>）。分层softmax将标签分解成一个树。每个标签都表示成这个树上的一个路径，这个树的每个节点处都训练一个Softmax分类器来在左和右分枝之间做决策。树的结构对于算法的最终结果影响很大，而且一般需要具体问题具体分析。</p>
<p><strong>属性（Attribute）分类</strong> 上面两个损失公式的前提，都是假设每个样本只有一个正确的标签$y_i$。但是如果$y_i$是一个二值向量，每个样本可能有，也可能没有某个属性，而且属性之间并不相互排斥呢？比如在Instagram上的图片，就可以看成是被一个巨大的标签集合中的某个子集打上标签，一张图片上可能有多个标签。在这种情况下，一个明智的方法是为每个属性创建一个独立的二分类的分类器。例如，针对每个分类的二分类器会采用下面的公式：</p>
<center>$$L_i = \sum_j \max(0, 1 - y_{ij} f_j)$$</center>

<p>上式中，求和是对所有分类$j$，$y_{ij}$的值为1或者-1，具体根据第i个样本是否被第j个属性打标签而定，当该类别被正确预测并展示的时候，分值向量$f_j$为正，其余情况为负。可以发现，当一个正样本的得分小于+1，或者一个负样本得分大于-1的时候，算法就会累计损失值。</p>
<p>另一种方法是对每种属性训练一个独立的逻辑回归分类器。二分类的逻辑回归分类器只有两个分类（0，1），其中对于分类1的概率计算为：</p>
<center>$$P(y = 1 \mid x; w, b) = \frac{1}{1 + e^{-(w^Tx +b)}} = \sigma (w^Tx + b)$$</center>

<p>因为类别0和类别1的概率和为1，所以类别0的概率为：$\displaystyle P(y=0|x;w,b)=1-P(y=1|x;w,b)$。这样，如果$\sigma(w^Tx+b)&gt;0.5$或者$w^Tx+b&gt;0$，那么样本就要被分类成为正样本（y=1）。然后损失函数最大化这个对数似然函数，问题可以简化为：</p>
<center>$$L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))$$</center>

<p>上式中，假设标签$y_{ij}$非0即1，$\sigma(.)$就是sigmoid函数。上面的公式看起来吓人，但是$f$的梯度实际上非常简单：$\displaystyle \frac{\partial L_i}{\partial f_j}=y_{ij}-\sigma(f_j)$（你可以自己求导来验证）。</p>
<p><strong>回归问题</strong> 是预测实数的值的问题，比如预测房价，预测图片中某个东西的长度等。对于这种问题，通常是计算预测值和真实值之间的损失。然后用L2平方范式或L1范式度量差异。对于某个样本，L2范式计算如下：</p>
<center>$$L_i = \Vert f - y_i \Vert_2^2$$</center>

<p>之所以在目标函数中要进行平方，是因为梯度算起来更加简单。因为平方是一个单调运算，所以不用改变最优参数。L1范式则是要将每个维度上的绝对值加起来：</p>
<center>$$L_i = \Vert f - y_i \Vert_1 = \sum_j \mid f_j - (y_i)_j \mid$$</center>

<p>在上式中，如果有多个数量被预测了，就要对预测的所有维度的预测求和，即$\sum_j$。观察第i个样本的第j维，用$\delta_{ij}$表示预测值与真实值之间的差异。关于该维度的梯度（也就是$\partial L_i/\partial f_j$）能够轻松地通过被求导为L2范式的$\delta_{ij}$或$sign(\delta_{ij})$。这就是说，评分值的梯度要么与误差中的差值直接成比例，要么是固定的并从差值中继承sign。</p>
<p><em>注意</em>：L2损失比起较为稳定的Softmax损失来，其最优化过程要困难很多。直观而言，它需要网络具备一个特别的性质，即对于每个输入（和增量）都要输出一个确切的正确值。而在Softmax中就不是这样，每个评分的准确值并不是那么重要：只有当它们量级适当的时候，才有意义。还有，L2损失鲁棒性不好，因为异常值可以导致很大的梯度。所以在面对一个回归问题时，先考虑将输出变成二值化是否真的不够用。例如，如果对一个产品的星级进行预测，使用5个独立的分类器来对1-5星进行打分的效果一般比使用一个回归损失要好很多。分类还有一个额外优点，就是能给出关于回归的输出的分布，而不是一个简单的毫无把握的输出值。如果确信分类不适用，那么使用L2损失吧，但是一定要谨慎：L2非常脆弱，在网络中使用随机失活（尤其是在L2损失层的上一层）不是好主意。</p>
<blockquote>
<p>当面对一个回归任务，首先考虑是不是必须这样。一般而言，尽量把你的输出变成二分类，然后对它们进行分类，从而变成一个分类问题。</p>
</blockquote>
<p><strong>结构化预测（structured prediction）</strong> 结构化损失是指标签可以是任意的结构，例如图表、树或者其他复杂物体的情况。通常这种情况还会假设结构空间非常巨大，不容易进行遍历。结构化SVM背后的基本思想就是在正确的结构$y_i$和得分最高的非正确结构之间画出一个边界。解决这类问题，并不是像解决一个简单无限制的最优化问题那样使用梯度下降就可以了，而是需要设计一些特殊的解决方案，这样可以有效利用对于结构空间的特殊简化假设。我们简要地提一下这个问题，但是详细内容就超出本课程范围。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>小结如下：</p>
<ul>
<li>推荐的预处理操作是对数据的每个特征都进行零中心化，然后将其数值范围都归一化到[-1,1]范围之内。</li>
<li>使用标准差为$\sqrt{2/n}$的高斯分布来初始化权重，其中<img src="http://www.zhihu.com/equation?tex=n" alt="n">是输入的神经元数。例如用numpy可以写作：<strong>w = np.random.randn(n) * sqrt(2.0/n)</strong>。</li>
<li>使用L2正则化和随机失活的倒置版本。</li>
<li>使用批量归一化。</li>
<li>讨论了在实践中可能要面对的不同任务，以及每个任务对应的常用损失函数。</li>
</ul>
<p>现在，我们预处理了数据，初始化了模型。在下一节中，我们将讨论算法的学习过程及其运作特性。</p>
<blockquote>
<p>译自斯坦福CS231n课程笔记<a href="http://link.zhihu.com/?target=http%3A//cs231n.github.io/neural-networks-2/" target="_blank" rel="noopener"><strong>Neural Nets notes 2</strong></a>，课程教师<a href="http://link.zhihu.com/?target=http%3A//cs.stanford.edu/people/karpathy/" target="_blank" rel="noopener"><strong>Andrej Karpathy</strong></a>授权翻译。本篇教程由<a href="https://www.zhihu.com/people/du-ke" target="_blank" rel="noopener">杜客</a>翻译完成，<a href="https://www.zhihu.com/people/kun-kun-97-81" target="_blank" rel="noopener">堃堃</a>进行校对修改</p>
<p>知乎地址：<a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit</a></p>
</blockquote>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/leaf.jpg"
                alt="Heroinlin"/>
            
              <p class="site-author-name" itemprop="name">Heroinlin</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">35</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">11</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">59</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/heroinlin" title="GitHub &rarr; https://github.com/heroinlin" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="/heroinlj@gmail.com" title="E-Mail &rarr; heroinlj@gmail.com"><i class="fa fa-fw fa-gmail"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://twitter.com/Heroin" title="Twitter &rarr; https://twitter.com/Heroin" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Heroinlin</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.6.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.1.2</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  















  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script src="/lib/fancybox/source/jquery.fancybox.pack.js"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/affix.js?v=7.1.2"></script>

  <script src="/js/schemes/pisces.js?v=7.1.2"></script>



  

  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  

  

  

  



  




  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
